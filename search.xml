<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Cpp知识点梳理</title>
      <link href="2022/10/27/cpp-zhi-shi-dian-shu-li/"/>
      <url>2022/10/27/cpp-zhi-shi-dian-shu-li/</url>
      
        <content type="html"><![CDATA[<h2 id="变量和基本类型"><a href="#变量和基本类型" class="headerlink" title="变量和基本类型"></a>变量和基本类型</h2><h3 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h3><p>变量提供一个具名的、可供程序操作的存储空间。C++中每一个变量都有其数据类型，数据类型决定着变量所占内存空间的大小和布局方式、该空间能存储的值的范围，以及变量能参与的运算。对C++程序员来说，“变量”和“对象”一般可以互换使用。</p><p><strong>初始化</strong></p><p>当变量在创建时会获得一个特定的值，即被初始化，变量的初始化不是赋值，初始化的含义时创建变量时赋予其一个初始值，而赋值的含义是把对象的当前值擦除，以一个新值来代替。</p><p>C++初始化问题复杂性的一个体现就是其定义了初始化的好几种不同形式</p><h3 id="算术类型"><a href="#算术类型" class="headerlink" title="算术类型"></a>算术类型</h3><p>C++定义了一套包括算术类型和空类型在内的基本数据类型。其中算术类型包含了字符型、整型、浮点型和布尔型。空类型不对应任何具体的值，仅用于一些特殊的场合。</p><ul><li><p>字符型：C++提供了集中字符类型，多数支持国际化。其中最基本的字符类型是<code>char</code>，一个<code>char</code>的空间应确保可以存放<em>机器基本字符集</em>（没有严格定义，例如ASCII码也是一种机器基本字符集）中任意字符对应的数字值。因此一个<code>char</code>的大小和一个机器字节(<code>8 bit</code>)一样。</p><p>其他字符类型用于扩展字符集，如<code>wchar_t</code>、<code>char16_t</code>、<code>char32_t</code>。其中<code>wchar_t</code>类型用于确保可以存放机器最大扩展字符集中的任意一个字符，类型<code>char16_t</code>和<code>char32_t</code>则为<code>Unicode</code>字符集服务。</p></li><li><p>整型：用于表示不同尺寸的整型数，其中C++11新定义了<code>long long</code>类型</p></li><li><p>浮点型：表示单精度、双精度和扩展精度值</p></li><li><p>布尔型：表示真（<code>true</code>）和假（<code>false</code>）</p></li></ul><p><em>Tips：</em>除去布尔型和字符型之外，整型还可以分为带符号的（<code>signed</code>）和无符号的（<code>unsigned</code>）。字符型可以被分为三种：<code>char</code>、<code>unsigned char</code>和<code>signed char</code>，但是字符的表现形式只有两种：带符号的和无符号的，类型<code>char</code>实际上会表现为上述两种形式中的一种，具体哪种由编译器决定。</p><p><strong>选择类型的准则：</strong></p><ul><li>当明确知晓数值不可能为负时，选择无符号类型。</li><li>当使用<code>int</code>执行整数运算时，如果数值超过了<code>int</code>的表示范围，选用<code>long long</code>，因为<code>short</code>常常显得太小，而<code>long</code>一般和<code>int</code>有着一样的尺寸。</li><li>在算术表达中，不要使用<code>char</code>或者<code>bool</code>只有在存放字符或者布尔型时才使用它们。因为<code>char</code>在一些机器上是有符号的，在另外一些机器上是无符号的，因此特别容易出问题，如果需要使用一个不大的整数，务必明确指定它的类型<code>signed char</code>或<code>unsigned char</code></li><li>执行浮点数运算时，使用<code>double</code>，因为<code>float</code>通常精度不够，并且<code>double</code>与<code>float</code>计算的代价相差无几，在某些机器上<code>double</code>甚至比<code>float</code>要快。一般不使用<code>long double</code>。</li></ul><h3 id="类型转换"><a href="#类型转换" class="headerlink" title="类型转换"></a>类型转换</h3><p>当我们把一种算术类型的值赋给另外一种类型时，类型所能表示的值的范围决定了转换的过程：</p><ul><li>非布尔型赋给布尔型时，<code>0-&gt;false</code>，<code>other-&gt;true</code></li><li>布尔型赋给非布尔型时，<code>false-&gt;0</code>，<code>true-&gt;1</code></li><li>浮点型赋给整型时，进行近似处理，截断小数部分</li><li>整型赋给浮点型时，小数部分记为0；如果整型所占空间超过了浮点型的容量，精度可能有损失</li><li>当赋给一个无符号类型超出它表示范围的值时，结果是初始值对无符号类型表示数值总数取模后的余数</li><li>当赋给一个带符号类型超出它表示范围的值时，结果是未定义的，此时程序可能继续工作，可能崩溃，也可能生成垃圾数据。</li></ul><p>当在程序的某处使用了一种算术类型的值，而其实所需的是另一种类型的值时，编译器同样会执行上述的类型转换。</p><p>当表达式里既有带符号类型又有无符号类型，当带符号类型取值为负时会出现异常结果，这是因为带符号数会自动地转换成无符号数。</p><h3 id="复合类型"><a href="#复合类型" class="headerlink" title="复合类型"></a>复合类型</h3><h2 id="字符串、向量和数组"><a href="#字符串、向量和数组" class="headerlink" title="字符串、向量和数组"></a>字符串、向量和数组</h2><h2 id="表达式"><a href="#表达式" class="headerlink" title="表达式"></a>表达式</h2><h2 id="语句"><a href="#语句" class="headerlink" title="语句"></a>语句</h2><h2 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h2><h2 id="类"><a href="#类" class="headerlink" title="类"></a>类</h2><h2 id="标准库"><a href="#标准库" class="headerlink" title="标准库"></a>标准库</h2><h2 id="模板与泛型编程"><a href="#模板与泛型编程" class="headerlink" title="模板与泛型编程"></a>模板与泛型编程</h2>]]></content>
      
      
      <categories>
          
          <category> 重学Cpp语言 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程语言 </tag>
            
            <tag> CPP语言 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>线性表经典笔试题解</title>
      <link href="2022/10/12/xian-xing-biao-jing-dian-bi-shi-ti-jie/"/>
      <url>2022/10/12/xian-xing-biao-jing-dian-bi-shi-ti-jie/</url>
      
        <content type="html"><![CDATA[<h2 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h2><h3 id="Remove-Duplicates-from-Sorted-Array"><a href="#Remove-Duplicates-from-Sorted-Array" class="headerlink" title="Remove Duplicates from Sorted Array"></a>Remove Duplicates from Sorted Array</h3><p><strong>描述</strong></p><p>Given a sorted array, remove the duplicates in place such that each element appear only once and return the new length. </p><p>Do not allocate extra space for another array, you must do this in place with constant memory. </p><p>For example, Given input array A = [1,1,2], Your function should return length = 2, and A is now [1,2].</p><p>给定已排序的数组，删除重复的元素，使得每个元素只出现一次，并且返回新数组的长度。</p><p>不要为另一个数组分配额外的空间，您必须使用常量内存来执行此操作。</p><p>例如，给定输入数组 A = [1,1,2]，您的函数应该返回长度 = 2，A 现在是 [1,2]</p><p><strong>分析</strong></p><p>无</p><p><strong>代码1</strong></p>]]></content>
      
      
      <categories>
          
          <category> 算法与数据结构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
            <tag> 数据结构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>无人驾驶——硬件平台篇</title>
      <link href="2022/10/10/wu-ren-jia-shi-ying-jian-ping-tai-pian/"/>
      <url>2022/10/10/wu-ren-jia-shi-ying-jian-ping-tai-pian/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 无人驾驶 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 无人驾驶 </tag>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>自主人工智能的机器教学(一) 工业自主人工智能</title>
      <link href="2022/10/09/zi-zhu-ren-gong-zhi-neng-de-ji-qi-jiao-xue-yi-gong-ye-zi-zhu-ren-gong-zhi-neng/"/>
      <url>2022/10/09/zi-zhu-ren-gong-zhi-neng-de-ji-qi-jiao-xue-yi-gong-ye-zi-zhu-ren-gong-zhi-neng/</url>
      
        <content type="html"><![CDATA[<h2 id="自主人工智能的真实案例"><a href="#自主人工智能的真实案例" class="headerlink" title="自主人工智能的真实案例"></a>自主人工智能的真实案例</h2><p>自主人工智能是AI驱动的自动化，通过实时感知和响应来优化设备与流程。</p>]]></content>
      
      
      <categories>
          
          <category> 华盛顿大学自动驾驶课程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 无人驾驶 </tag>
            
            <tag> 算法 </tag>
            
            <tag> Coursera </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>无人驾驶——行为预测篇</title>
      <link href="2022/10/06/wu-ren-jia-shi-xing-wei-yu-ce-pian/"/>
      <url>2022/10/06/wu-ren-jia-shi-xing-wei-yu-ce-pian/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 无人驾驶 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 无人驾驶 </tag>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>无人驾驶——系统安全篇</title>
      <link href="2022/10/02/wu-ren-jia-shi-xi-tong-an-quan-pian/"/>
      <url>2022/10/02/wu-ren-jia-shi-xi-tong-an-quan-pian/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 无人驾驶 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 无人驾驶 </tag>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>无人驾驶——数据通信协议篇</title>
      <link href="2022/09/24/wu-ren-jia-shi-shu-ju-tong-xin-xie-yi-pian/"/>
      <url>2022/09/24/wu-ren-jia-shi-shu-ju-tong-xin-xie-yi-pian/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 无人驾驶 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 无人驾驶 </tag>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>无人驾驶——深度学习篇（Transformer）</title>
      <link href="2022/09/12/wu-ren-jia-shi-shen-du-xue-xi-pian-transformer/"/>
      <url>2022/09/12/wu-ren-jia-shi-shen-du-xue-xi-pian-transformer/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 无人驾驶 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 无人驾驶 </tag>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>无人驾驶——深度学习篇（CNN）</title>
      <link href="2022/08/25/wu-ren-jia-shi-shen-du-xue-xi-pian-cnn/"/>
      <url>2022/08/25/wu-ren-jia-shi-shen-du-xue-xi-pian-cnn/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 无人驾驶 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 无人驾驶 </tag>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>链表问题集锦</title>
      <link href="2022/08/15/lian-biao-wen-ti-ji-jin/"/>
      <url>2022/08/15/lian-biao-wen-ti-ji-jin/</url>
      
        <content type="html"><![CDATA[<h2 id="反转链表"><a href="#反转链表" class="headerlink" title="反转链表"></a>反转链表</h2><h3 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h3><p>给定一个单链表的头结点 <code>pHead</code> (该头节点是有值的，比如在下图，它的 <code>val</code> 是 1)，长度为 n，反转该链表后，返回新链表的表头。</p><p><strong>数据范围：</strong> $0\le n\le10000$</p><p><strong>要求：</strong>空间复杂度 $O(1)$ ，时间复杂度 $O(n)$ 。</p><p>如当输入链表 {1,2,3} 时，经反转后，原链表变为 {3,2,1}，所以对应的输出为 {3,2,1}。</p><p>以上转换过程如下图所示：</p><p><img src="https://uploadfiles.nowcoder.com/images/20211014/423483716_1634206291971/4A47A0DB6E60853DEDFCFDF08A5CA249" alt="img" style="zoom: 33%;"></p><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><h4 id="递归实现"><a href="#递归实现" class="headerlink" title="递归实现"></a>递归实现</h4><pre class=" language-lang-cpp"><code class="language-lang-cpp">/*struct ListNode {    int val;    struct ListNode *next;    ListNode(int x) :            val(x), next(NULL) {    }};*/#include\<stack\>#include\<cstdio\>class Solution {public:    ListNode* ReverseList(ListNode* pHead) {        if(pHead==NULL || pHead->next==NULL){            return pHead;        }        ListNode* ans = ReverseList(pHead->next);        pHead->next->next=pHead;        pHead->next=NULL;        return ans;    }};</code></pre><h4 id="栈实现"><a href="#栈实现" class="headerlink" title="栈实现"></a>栈实现</h4><pre class=" language-lang-cpp"><code class="language-lang-cpp">/*struct ListNode {    int val;    struct ListNode *next;    ListNode(int x) :            val(x), next(NULL) {    }};*/#include\<stack\>#include\<cstdio\>class Solution {public:    ListNode* ReverseList(ListNode* pHead) {        ListNode * p = pHead;        ListNode * tempNode = NULL;        if(pHead==NULL || pHead->next==NULL){            return pHead;        }        stack <ListNode *> num_stack;        while(p){            num_stack.push(p);            p = p->next;        }        ListNode * node = num_stack.top();        ListNode * head = node;        num_stack.pop();        while(!num_stack.empty()){            tempNode = num_stack.top();            num_stack.pop();            node->next = tempNode;            node = node->next;        }        node->next=NULL;        return head;    }}</code></pre><h4 id="双链表实现"><a href="#双链表实现" class="headerlink" title="双链表实现"></a>双链表实现</h4><pre class=" language-lang-cpp"><code class="language-lang-cpp">/*struct ListNode {    int val;    struct ListNode *next;    ListNode(int x) :            val(x), next(NULL) {    }};*/#include\<stack\>#include\<cstdio\>class Solution {public:    ListNode* ReverseList(ListNode* pHead) {        ListNode * newHead = NULL;        while(pHead){            ListNode *temp = pHead->next;            pHead->next = newHead;            newHead = pHead;            pHead = temp;        }        return newHead;    }};</code></pre><h2 id="链表内指定区间反转"><a href="#链表内指定区间反转" class="headerlink" title="链表内指定区间反转"></a>链表内指定区间反转</h2><h3 id="描述-1"><a href="#描述-1" class="headerlink" title="描述"></a>描述</h3><p>将一个节点数为 size 链表 m 位置到 n 位置之间的区间反转，要求时间复杂度 $O(n)$，空间复杂度 $O(1)$。<br>例如：<br>给出的链表为 $1\to 2 \to 3 \to 4 \to 5 \to NULL, m=2,n=4$<br>返回 $1\to 4\to 3\to 2\to 5\to NULL$.</p><p><strong>数据范围：</strong> 链表长度 $0 &lt; size \le 10000$，$0 &lt; m \le n \le size$，链表中每个节点的值满足 $|val| \le 1000$</p><p><strong>要求：</strong>时间复杂度 $O(n)$ ，空间复杂度 $O(n)$</p><p><strong>进阶：</strong>时间复杂度 $O(n)$，空间复杂度 $O(1)$</p><h3 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a>代码实现</h3><h4 id="头插法迭代"><a href="#头插法迭代" class="headerlink" title="头插法迭代"></a>头插法迭代</h4><pre class=" language-lang-cpp"><code class="language-lang-cpp">/** * struct ListNode { *    int val; *    struct ListNode *next; * }; */class Solution {public:    /**     *      * @param head ListNode类      * @param m int整型      * @param n int整型      * @return ListNode类     */    ListNode* reverseBetween(ListNode* head, int m, int n) {        // write code here        ListNode * res = new ListNode(-1);        res->next = head;        ListNode * pre = res;        ListNode * cur = head;        for(int i = 1; i < m; i++){            pre = cur;            cur = cur->next;        }        for(int i = m; i < n; i++){            ListNode *temp = cur->next;            cur->next = temp->next;            temp->next = pre->next;            pre->next = temp;        }        return res->next;    }}</code></pre><h4 id="递归实现-1"><a href="#递归实现-1" class="headerlink" title="递归实现"></a>递归实现</h4><pre class=" language-lang-cpp"><code class="language-lang-cpp">class Solution {public:    ListNode* temp = NULL;    ListNode* reverse(ListNode* head, int n){        //只颠倒第一个节点，后续不管        if(n == 1){             temp = head->next;            return head;        }        //进入子问题        ListNode* node = reverse(head->next, n - 1);         //反转        head->next->next = head;        //每个子问题反转后的尾拼接第n个位置后的节点        head->next = temp;        return node;    }    ListNode* reverseBetween(ListNode* head, int m, int n) {        //从第一个节点开始        if(m == 1)             return reverse(head, n);        //缩减子问题        ListNode* node = reverseBetween(head->next, m - 1, n - 1);         //拼接已翻转        head->next = node;        return head;    }};</code></pre><h2 id="链表中的节点每k个一组翻转"><a href="#链表中的节点每k个一组翻转" class="headerlink" title="链表中的节点每k个一组翻转"></a>链表中的节点每k个一组翻转</h2><h3 id="描述-2"><a href="#描述-2" class="headerlink" title="描述"></a>描述</h3><p>将给出的链表中的节点每 k 个一组翻转，返回翻转后的链表。如果链表中的节点数不是 k 的倍数，将最后剩下的节点保持原样。你不能更改节点中的值，只能更改节点本身。</p><p>数据范围： $0 \le n \le 2000 $ ，$1 \le k \le 2000$ ，链表中每个元素都满足 $0 \le val \le 1000$<br>要求空间复杂度 $O(1)$，时间复杂度 $O(n)$</p><p>例如：</p><p>给定的链表是 $1\to2\to3\to4\to5$</p><p>对于 $k = 2$, 你应该返回 $2\to 1\to 4\to 3\to 5$</p><p>对于 $k = 3$, 你应该返回 $3\to2 \to1 \to 4\to 5$</p><h3 id="代码实现-2"><a href="#代码实现-2" class="headerlink" title="代码实现"></a>代码实现</h3><h4 id="递归实现-2"><a href="#递归实现-2" class="headerlink" title="递归实现"></a>递归实现</h4><pre class=" language-lang-cpp"><code class="language-lang-cpp">/** * struct ListNode { *    int val; *    struct ListNode *next; * }; */class Solution {public:    /**     *      * @param head ListNode类      * @param k int整型      * @return ListNode类     */    ListNode* reverseKGroup(ListNode* head, int k) {        ListNode *tail=head;        for(int i = 0; i < k; i++){            if(tail==NULL)                return head;            tail = tail->next;           }        ListNode *pre=NULL,*cur=head;        while(cur!=tail){            ListNode *temp = cur->next;            cur->next = pre;            pre = cur;            cur = temp;        }        head->next = reverseKGroup(tail, k);        return pre;    }}</code></pre><h4 id=""><a href="#" class="headerlink" title=" "></a> </h4>]]></content>
      
      
      <categories>
          
          <category> 算法与数据结构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
            <tag> 数据结构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>无人驾驶——强化学习篇</title>
      <link href="2022/08/12/wu-ren-jia-shi-qiang-hua-xue-xi-pian/"/>
      <url>2022/08/12/wu-ren-jia-shi-qiang-hua-xue-xi-pian/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 无人驾驶 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 无人驾驶 </tag>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>无人驾驶——模拟器技术篇</title>
      <link href="2022/07/21/wu-ren-jia-shi-mo-ni-qi-ji-zhu-pian/"/>
      <url>2022/07/21/wu-ren-jia-shi-mo-ni-qi-ji-zhu-pian/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 无人驾驶 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 无人驾驶 </tag>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>无人驾驶——操作系统篇</title>
      <link href="2022/07/02/wu-ren-jia-shi-cao-zuo-xi-tong-pian/"/>
      <url>2022/07/02/wu-ren-jia-shi-cao-zuo-xi-tong-pian/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 无人驾驶 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 无人驾驶 </tag>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>无人驾驶——高精地图篇</title>
      <link href="2022/06/21/wu-ren-jia-shi-gao-jing-di-tu-pian/"/>
      <url>2022/06/21/wu-ren-jia-shi-gao-jing-di-tu-pian/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 无人驾驶 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 无人驾驶 </tag>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>无人驾驶——决策、规划与控制篇</title>
      <link href="2022/05/28/wu-ren-jia-shi-jue-ce-gui-hua-yu-kong-zhi-pian/"/>
      <url>2022/05/28/wu-ren-jia-shi-jue-ce-gui-hua-yu-kong-zhi-pian/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 无人驾驶 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 无人驾驶 </tag>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>无人驾驶——感知系统篇</title>
      <link href="2022/05/21/wu-ren-jia-shi-gan-zhi-xi-tong-pian/"/>
      <url>2022/05/21/wu-ren-jia-shi-gan-zhi-xi-tong-pian/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 无人驾驶 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 无人驾驶 </tag>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>无人驾驶——定位篇（GPS与IMU）</title>
      <link href="2022/05/03/wu-ren-jia-shi-ding-wei-pian-gps-imu/"/>
      <url>2022/05/03/wu-ren-jia-shi-ding-wei-pian-gps-imu/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 无人驾驶 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 无人驾驶 </tag>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>无人驾驶——激光雷达篇</title>
      <link href="2022/05/01/wu-ren-jia-shi-xi-lie-ji-guang-lei-da-pian/"/>
      <url>2022/05/01/wu-ren-jia-shi-xi-lie-ji-guang-lei-da-pian/</url>
      
        <content type="html"><![CDATA[<h2 id="激光雷达技术简介"><a href="#激光雷达技术简介" class="headerlink" title="激光雷达技术简介"></a>激光雷达技术简介</h2><p>无人驾驶技术是多项技术的集成，包括传感器、定位与深度学习、高精地图、路径规划、障碍物检测与规避、机械控制、系统集成与优化、能耗与散热管理等。无人车系统的感知端由不同的传感器组成，其中包括GPS（用于定位）、激光雷达（用于定位及障碍物检测）、RGB照相机（用于深度学习的物体识别），以及定位辅助。</p><p>在传感器采集到信息后，我们就进入了感知阶段，主要工作是定位与物体识别。在这个阶段，可以用数学的方法，比如卡尔曼滤波与粒子滤波等算法，对各种传感器信息进行融合，并得出当前最大概率的位置。如果使用激光雷达为主要的定位传感器，则可以将激光雷达扫描回来的信息跟已知的高精地图做对比，从而得出当前的车辆位置。如果当前没有地图甚至可以将当前的激光雷达扫描与之前的激光雷达扫描用ICP(Iterative Closest Point，迭代最近点)算法做对比，从而推算出当前的车辆位置。在得出基于激光雷达的位置预测后，可以用数学的方法与其他传感器信息进行融合，推算出更精准的位置信息。</p><p>最后我们进入规划与控制阶段。在这个阶段，我们根据位置信息及识别出的图像信息（比如红绿灯），实时调节车辆的行车规划，并把行车规划转化成控制信号去操控车辆。全局的路径规划可以用A-Star类似的算法实现，本地的路径规划可以用DWA等算法实现。</p><h2 id="激光雷达基础知识"><a href="#激光雷达基础知识" class="headerlink" title="激光雷达基础知识"></a>激光雷达基础知识</h2><h3 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h3><p>激光雷达是一种光学遥感技术，它向目标物体发射一束激光，根据接收——反射的时间间隔确定目标物体的实际距离。然后根据距离及激光发射的角度，通过简单的几何变化推导出物体的位置信息。由于激光的传播受外界影响小，激光雷达能够检测的距离一般可以达到100m以上。与传统雷达使用无线电波相比，激光雷达使用激光射线，商用激光雷达使用的激光射线波长一般在600nm~1000nm，远远低于传统雷达使用的波长。因此激光雷达在测量物体距离和表面形状上可达到更高的精准度，一般精准度可以达到厘米级。</p><p>激光雷达系统一般可以分为三个部分：第一部分是激光发射器，发射出波长为600~1000nm的激光射线；第二部分是扫描与光学部件，主要用于收集反射点距离与该点反射的时间和水平角度（Azimuth）；第三部分是感光部件，主要检测返回光的强度。因此，我们检测到的每一个点都包括了空间坐标信息$(x,y,z)$及光强度$i$。光强度与物体的光反射度直接相关，所以从检测到的光强度我们也可以对检测到的物体有初步判断。</p><p><img src="https://uptolimit.top/img/v2-e70df4e7bdcdf138024df47fbc0f1698_b.jpg" alt="img" style="zoom:80%;"></p><h3 id="激光雷达点云"><a href="#激光雷达点云" class="headerlink" title="激光雷达点云"></a>激光雷达点云</h3><p>无人车所使用的激光雷达并不是静止不动的。在无人车的行驶过程中，激光雷达同时以一定的角速度匀速转动，在这个过程中不断地发出激光并收集反射点的信息，以便得到全方位的环境信息。激光雷达在收集反射距离的过程中会同时记录该点发生的时间和水平角度，并且，每个激光发射器都有其编号和固定的垂直角度，根据这个数据就可以计算出所有反射点的坐标。激光雷达每旋转一周，收集到的所有反射点坐标的集合就形成了点云（Point Cloud）。</p><p>如图所示，激光雷达通过激光反射可以测出和物体的距离（distance），因为激光的垂直角度是固定的，记作$a$，这里我们可以直接求出$z$轴坐标为$sin(a)\cdot distance$。由$cos(a)\cdot distance$可以得到$distance$在$XY$平面的投影，记作$xy_dist$。激光雷达在记录反射点距离的同时也会记录当前激光雷达转动的水平角度$b$，这样根据简单的几何转换就可以得到该点的$x$、$y$坐标，分别为$cos(b)\cdot xy_dist$和$sin(b)\cdot xy_dist$。</p><p><img src="https://uptolimit.top/img/image-20221020234250470.png" alt="image-20221020234250470" style="zoom:80%;"></p><h2 id="激光雷达的应用"><a href="#激光雷达的应用" class="headerlink" title="激光雷达的应用"></a>激光雷达的应用</h2><h3 id="高精地图的绘制"><a href="#高精地图的绘制" class="headerlink" title="高精地图的绘制"></a>高精地图的绘制</h3><p>高精地图不同于我们日常使用的导航地图。高精地图是由众多的点云拼接而成的，主要用于无人车的精确定位。高精地图的绘制也是通过激光雷达完成的。安装激光雷达的地图数据采集车在想要绘制高精地图的路线上多次反复行驶，并收集点云数据。后期经过人工标注，首先将过滤一些点云图中的错误信息，例如路上行驶的汽车和行人反射所形成的点，然后对多次收集到的点云进行对齐拼接，形成最终的高精地图。</p><h3 id="基于点云的定位"><a href="#基于点云的定位" class="headerlink" title="基于点云的定位"></a>基于点云的定位</h3><p>激光雷达会在车辆行驶过程中不断地收集点云来了解周围的环境。我们可以很自然地想到利用这些观察到的环境信息帮助我们定位。可以把这个问题简化为 ：已知$t_0$时刻的GPS信息，$t_0$时刻的点云信息，以及无人车$t_1$时刻可能在三个位置$P_1$、$P_2$、$P_3$。求$t_1$时刻车在这三个点的概率。根据贝叶斯法则，无人车的定位问题可以简化为下面这个概率公式：</p><script type="math/tex; mode=display">P(X_t)\approx P(Z_t|X_t)\cdot{\bar{P(X_t)}}</script><p>其中$P(Z_t|X_t)$表示给定当前位置，观测到点云信息的概率分布。其计算方式一般分为局部估计和全局估计。局部估计较简单的做法就是通过当前时刻点云和上一时刻点云的匹配，借助几何上的推导，估计出无人车在当前位置的可能性。全局估计就是利用当前时刻的点云和上面提到过的高精地图做匹配，从而得到当前车相对地图上某一位置的可能性。在实际应用中一般会将两种定位方法结合使用。第二项$\bar{P(X_t)}$表示对当前位置的预测的概率分布，这里可以简单地用GPS给出地位置信息作为预测。通过计算P1,P2,P3这三个点的后验概率，可以估算出无人车在哪一个位置的可能性最高。通过对两个概率分布相乘，可以很大程度上提高无人车定位的准确度。</p><p><img src="https://uptolimit.top/img/image-20221028151054107.png" alt="image-20221028151054107" style="zoom:67%;"></p><h3 id="障碍物检测"><a href="#障碍物检测" class="headerlink" title="障碍物检测"></a>障碍物检测</h3><p>在机器视觉中，一个比较难解决的问题就是判断物体的远近，基于单一摄像头抓取的2D图像无法得到准确的距离信息，而基于多摄像头生成深度图的方法需要很大的计算量，不能很好的满足无人车在实时性上的要求。另一个棘手问题是光学摄像头受光照条件的影响巨大，物体的识别准确度很不稳定。</p><p>利用激光雷达生成的点云可以很大程度上解决上述两个问题，借助激光雷达本身的特性，可以反射障碍物的远近、高低甚至表面形状，做出较准确的估计，从而大大提高障碍物检测的准确度，而且其算法的复杂度低于基于摄像头的视觉算法，因此更能满足无人车的实时性要求。</p><h2 id="激光雷达面临的挑战"><a href="#激光雷达面临的挑战" class="headerlink" title="激光雷达面临的挑战"></a>激光雷达面临的挑战</h2><h3 id="技术挑战：空气中的悬浮物"><a href="#技术挑战：空气中的悬浮物" class="headerlink" title="技术挑战：空气中的悬浮物"></a>技术挑战：空气中的悬浮物</h3><p>激光雷达的精度会受到天气的影响，由于空气中的悬浮物会对光速产生影响。外部环境（大雾及雨天）也会影响激光雷达的精度。</p><h3 id="计算性能挑战：计算量大"><a href="#计算性能挑战：计算量大" class="headerlink" title="计算性能挑战：计算量大"></a>计算性能挑战：计算量大</h3><p>即使是16线的激光雷达，每秒要处理的点也达到了30万个。如此大量的数据处理是无人车定位算法和障碍物检测算法的实时性需要面临的一大挑战。例如之前所说的激光雷达给出的原始数据只是反射物体的距离信息，需要对所有产生的点进行几何变换，将其转换为位置坐标，其中至少涉及4次浮点运算和三次三角函数运算，而且点云在后期的处理中还有大量坐标系转转等更多复杂的运算，这些都对计算资源（CPU、GPU和FPGA）提出了很大的需求。</p><div class="table-container"><table><thead><tr><th>型号</th><th>Channel数量</th><th>每秒产生的点数</th></tr></thead><tbody><tr><td>Velodyne HDL-64E</td><td>64 Channels</td><td>2200000</td></tr><tr><td>Velodyne HDL-32E</td><td>32 Channels</td><td>700000</td></tr><tr><td>Velodyne HDL-16E</td><td>16 Channels</td><td>300000</td></tr></tbody></table></div><h3 id="成本挑战：造价昂贵"><a href="#成本挑战：造价昂贵" class="headerlink" title="成本挑战：造价昂贵"></a>成本挑战：造价昂贵</h3><p>上面提到的Velodyne VLP-16激光雷达官网税前售价为7999美元，而Velodyne HDL-64E激光雷达售价也在10万美元以上。这些成本要加在本来就没有过高利润的汽车价格中，无疑会大大阻碍无人车的商业化。</p><h2 id="激光雷达的点云特性"><a href="#激光雷达的点云特性" class="headerlink" title="激光雷达的点云特性"></a>激光雷达的点云特性</h2><p>无人驾驶对激光雷达提出了新的需求，核心指标是分辨率和探测距离，以及传感器的几何大小和成本，等等，点云分辨率和探测距离对激光雷达的应用效果有着决定性影响</p><h3 id="多线扫描低分辨率激光雷达"><a href="#多线扫描低分辨率激光雷达" class="headerlink" title="多线扫描低分辨率激光雷达"></a>多线扫描低分辨率激光雷达</h3><p>以Velodyne为代表的多线扫描低分辨率雷达在自动驾驶系统中被广泛采用。这类激光雷达的典型架构如图所示。由多组光路延纵向排列构成，每一组光路有独立的发射接收光路，以覆盖纵向视场内的多个离散角度。整个系统绕纵轴360°旋转，覆盖横向视场的整个范围。纵向视场角内的线数通常在16线到64线之间。</p><p><img src="https://uptolimit.top/img/e4b2df88-2be9-47ba-8657-1314c1953807.jpg" alt="e4b2df88-2be9-47ba-8657-1314c1953807" style="zoom:80%;"></p><h3 id="高分辨率近距离激光雷达"><a href="#高分辨率近距离激光雷达" class="headerlink" title="高分辨率近距离激光雷达"></a>高分辨率近距离激光雷达</h3><h3 id="高分辨率图像级长距激光雷达"><a href="#高分辨率图像级长距激光雷达" class="headerlink" title="高分辨率图像级长距激光雷达"></a>高分辨率图像级长距激光雷达</h3><h2 id="总结与展望"><a href="#总结与展望" class="headerlink" title="总结与展望"></a>总结与展望</h2><p>尽管无人驾驶技术渐趋成熟，但激光雷达始终是一个绕不过去的技术。纯视觉与GPS/IMU的定位及避障方案价格虽然低，却不成熟，很难应用到室外场景中。同时，激光雷达的价格居高不下，消费者很难承受动辄几十万美元定价的无人车。因此，当务之急就是快速把系统成本特别是激光雷达的成本大幅降低。其中一个交由希望的方法是使用较低价的激光雷达，虽然会损失一些精度，但可以用其他低价的传感器与激光雷达做信息混合，较精准的推算出车辆的位置。简而言之就是用更好的算法弥补硬件传感器的不足。</p>]]></content>
      
      
      <categories>
          
          <category> 无人驾驶 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 无人驾驶 </tag>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>无人驾驶系列—— 概述</title>
      <link href="2022/04/26/wu-ren-jia-shi-xi-lie-gai-shu/"/>
      <url>2022/04/26/wu-ren-jia-shi-xi-lie-gai-shu/</url>
      
        <content type="html"><![CDATA[<h2 id="无人驾驶行业概述"><a href="#无人驾驶行业概述" class="headerlink" title="无人驾驶行业概述"></a>无人驾驶行业概述</h2><h3 id="什么是无人驾驶"><a href="#什么是无人驾驶" class="headerlink" title="什么是无人驾驶"></a>什么是无人驾驶</h3><p><code>A self-driving car, also known as an autonomous car, driver-less car, or robotic car (robo-car),is a car incorporating vehicular automation, that is, a ground vehicle that is capable of sensing its environment and moving safely with little or no human input.</code><br>能够感知周围的环境，用很少的人的参与来执行驾驶行为。<br>自动驾驶：某些情况下可以加入人的参与与监督。<br>无人驾驶：完全不依赖于人的行为，是为更高级的自动驾驶</p><h4 id="无人驾驶能解决什么问题"><a href="#无人驾驶能解决什么问题" class="headerlink" title="无人驾驶能解决什么问题"></a>无人驾驶能解决什么问题</h4><ol><li>提升交通效率</li></ol><ul><li>无人驾驶汽车严格遵守交通规则</li><li>道路通行更顺畅、拥堵减少、交通事故大大降低</li><li>隐形路口问题，通行效率提高45%</li><li>减少通勤时间</li></ul><ol><li>车辆利用率提升</li></ol><ul><li>缓解停车位紧张</li><li>无需停车位，点对点交通运输</li><li>共享交通方式将最大化普及</li><li>费用降低</li></ul><ol><li>交通事故问题</li></ol><ul><li>每年全球超过100万人死于车祸</li><li>93%的事故是人为造成的：醉驾、疲劳驾驶、分心驾驶</li><li>人类需要每次从零开始学习，总有新手司机上路，错误一遍遍重演</li><li>无人驾驶能够互联互通，比人更高效</li></ul><h3 id="无人驾驶发展历程"><a href="#无人驾驶发展历程" class="headerlink" title="无人驾驶发展历程"></a>无人驾驶发展历程</h3><h4 id="无人驾驶分级"><a href="#无人驾驶分级" class="headerlink" title="无人驾驶分级"></a>无人驾驶分级</h4><p>自动驾驶分为6个级别<br><img src="http://uptolimit.top/img/32583119239284.png" alt="32583119239284"><br>SAE:美国汽车工程师协会<br>L0:无自动化，全部人工<br>L1:巡航定速（Adaptive Cruise Control,ACC）,巡航装置可以纵向控制车辆，可以加速减速<br>L2: 车道保持辅助（Lane Keeping)系统可以纵向控制也可以横向控制汽车，但车是辅助，人才是主导<br>L2.5: 可以提供简单路况下的变道能力；<br>L2与L3之间有一个鸿沟：权责问题<br>L3：在L2基础上提供编导能力，在某一时段内车是责任主体<br>L4: 相当于全无人驾驶，大部分时间由车来做主导<br>L5: 驾驶能力上几乎与人类没有关系，车辆上没有接管设备</p><h4 id="L4级别无人驾驶"><a href="#L4级别无人驾驶" class="headerlink" title="L4级别无人驾驶"></a>L4级别无人驾驶</h4><p>实现思路：</p><ul><li>V2X:Vehicle to Everything（车路协同）<ul><li>V2V(车辆)</li><li>V2I(公共设施)</li><li>V2P(行人)</li></ul></li><li>边缘计算<ul><li>RSU(路侧单元)</li><li>OBU(车载单元)</li></ul></li><li>5G通信能力<ul><li>LTE-V协议：专门针对车间通讯的协议，可兼容4G-5G</li></ul></li><li>路侧智能：强大的感知能力（百度ACE计划）</li><li>主车智能：近些年深度学习填补上了最后一块软件难题</li><li>感知能力：高度复杂冗余的传感器</li><li>决策能力：大数据下的智能决策</li><li>高精地图：丰富的地图信息数据</li><li>定位：精准的位置获取能力</li><li>权责问题如何处理？<ul><li>RSS模型（Responsibility-Sensitive Safety）责任敏感安全模型</li><li>为自动驾驶汽车与人类的责任概念提供具体可衡量的参数，并通过对所有记录在案的交通事故所涉及的行为进行分析和统计，为自动驾驶汽车界定了一个可计量的“安全状态”<h2 id="无人驾驶技术路径"><a href="#无人驾驶技术路径" class="headerlink" title="无人驾驶技术路径"></a>无人驾驶技术路径</h2><h2 id="无人驾驶技术概述"><a href="#无人驾驶技术概述" class="headerlink" title="无人驾驶技术概述"></a>无人驾驶技术概述</h2><h3 id="L4自动驾驶系统架构"><a href="#L4自动驾驶系统架构" class="headerlink" title="L4自动驾驶系统架构"></a>L4自动驾驶系统架构</h3><img src="http://uptolimit.top/img/499763119227151.png" alt="499763119227151" style="zoom:67%;"></li></ul></li></ul><h3 id="自动驾驶硬件概述"><a href="#自动驾驶硬件概述" class="headerlink" title="自动驾驶硬件概述"></a>自动驾驶硬件概述</h3><p><img src="http://uptolimit.top/img/131033419247317.png" alt="131033419247317" style="zoom:67%;"></p><h4 id="感知传感器"><a href="#感知传感器" class="headerlink" title="感知传感器"></a>感知传感器</h4><ul><li>摄像头：广泛用于物体识别和物体追踪场景，比如车道线检测、交通灯识别等，一般无人驾驶车都安装环视多枚摄像头</li><li>激光雷达：用于障碍物位置识别、绘制地图、辅助定位等，准确率非常高，很多方案中将激光雷达作为主传感器使用</li><li>毫米波雷达：阴雨天、雾霾天能够辅助感知获取物体的位置和速度，观测距离远但误检较多</li><li>超声波：近处高敏感度传感器，常用于作为安全冗余设备检测车辆的碰撞安全问题</li></ul><h4 id="定位系统传感器"><a href="#定位系统传感器" class="headerlink" title="定位系统传感器"></a>定位系统传感器</h4><ul><li>IMU：实时测量自身的姿态，200Hz或更高，包含了三个单轴的加速度计和三个单轴的陀螺仪，加速度计检测物体在载体坐标系统独立三轴的加速度信号，而陀螺仪检测载体相对于导航坐标系的角速度信号</li><li>GNSS：也是大家常听到的GPS，无人车一般使用RTK（载波相位差分技术）技术来进行定位、频率相对较低10Hz左右</li></ul><h4 id="车载计算单元-IPC"><a href="#车载计算单元-IPC" class="headerlink" title="车载计算单元(IPC)"></a>车载计算单元(IPC)</h4><p><img src="http://uptolimit.top/img/131295619239986.png" alt="131295619239986" style="zoom:67%;"></p><ul><li>高效连接计算单元内部各计算设备，连接外部传感器的信息输入和存储</li><li>冗余设计，以防止单点故障</li><li>需要考虑整体的车规、电磁干扰和振动方面的设计以及ISO-26262标准的要求</li><li>ISO-26262：一个硬件达到了ASIL D级别的要求，那么它的故障率是10FIT，即10亿个小时里面出一次故障，汽车行业在安全方面可以做到的极限</li></ul><h4 id="车辆线控系统"><a href="#车辆线控系统" class="headerlink" title="车辆线控系统"></a>车辆线控系统</h4><ul><li>自动驾驶线控系统：汽车的控制是由一些简单命令完成的，而不是由物理操作完成。这一部分相当于人的手和脚</li><li>传统汽车的这些控制由液压系统和真空助力泵协助完成，自动驾驶汽车的线控主要用电控化的零部件来完成，如电子液压制动系统</li></ul><h3 id="自动驾驶软件概述"><a href="#自动驾驶软件概述" class="headerlink" title="自动驾驶软件概述"></a>自动驾驶软件概述</h3><p><img src="http://uptolimit.top/img/38171620236541.png" alt="38171620236541" style="zoom:67%;"><br><img src="http://uptolimit.top/img/299481820232295.png" alt="299481820232295" style="zoom:67%;"></p><h4 id="操作系统OS"><a href="#操作系统OS" class="headerlink" title="操作系统OS"></a>操作系统OS</h4><ul><li>RTOS：实时操作系统<ul><li>QNX：类Unix系统，具有强实时性，符合车规级的实时操作系统</li><li>RT Linux：Linux内核补丁，通过软实时进行监控</li></ul></li><li>FrameWork：<ul><li>ROS（机器人操作系统）</li><li>YARP、Microsoft Robotics、MOOS、Cybertron<h4 id="高精地图-HD-Map（High-Dimensional）"><a href="#高精地图-HD-Map（High-Dimensional）" class="headerlink" title="高精地图 HD Map（High Dimensional）"></a>高精地图 HD Map（High Dimensional）</h4></li></ul></li><li>不同于导航地图，最大的特点就是高纬度和高精度</li><li>道路网的精确三维表征，如交叉路口布局和路标位置</li><li>地图语义信息，如道路的速度限制、左转车道开始的位置</li><li>导航地图只能达到米级精度，高精地图需要达到厘米级精度</li><li>高精地图坐标系：WGS84、墨卡托坐标系</li><li>高精地图提供其它Level4模块的数据支持</li><li>提供了很多准确的静态物体的信息</li><li>定位可以用于计算相对位置</li><li>帮助传感器缩小检测范围，缩小ROI区域</li><li>计算道路导航信息</li><li>帮助车辆识别车道的确切中心线 <h4 id="定位Localization"><a href="#定位Localization" class="headerlink" title="定位Localization"></a>定位Localization</h4></li><li>无人车最重要的一步就是知道自己在哪</li><li>INS：Inertial Navigation System惯性导航系统</li><li>IMU：获取自身状态（加速度和角速度）后通过状态矩阵递推下一时刻位置</li><li>但是如果没有校正信息的话，这种状态递推会随着时间不断累计误差，导致最终位置发散</li><li>RTK：载波相位差分系统（在GPS中加入了一个基站）<ul><li>GNSS</li><li>RTK多入了一个静止基站，同样收到定位卫星的信号，无人车与RTK相隔不太远的情况下，对二者之间的干扰信号用差分抹平</li><li>RTK通过较低的更新频率提供相对准确的位置信息，INS则以较高的频率提供和准确性较差的姿态信息。通过使用卡尔曼滤波整合两类数据获取其各自优势，合并提供出高准确性的实时信息</li></ul></li><li><p>几何定位</p><ul><li>激光雷达、摄像头、高精地图</li><li>利用激光雷达或者图像信息，可以通过物体匹配来对汽车进行定位。将检测的数据与预先存在的高精地图之间匹配，通过这种比较可获知汽车在高精地图上的全球位置和行驶方向</li><li>迭代最近点（ICP）、直方图滤波（Histogram Filter）<h4 id="感知-Perception"><a href="#感知-Perception" class="headerlink" title="感知 Perception"></a>感知 Perception</h4><img src="http://uptolimit.top/img/398003321250175.png" alt="398003321250175" style="zoom:67%;"></li></ul></li><li><p>四大基础任务：</p><ul><li>找出物体在环境中的位置</li><li>明确对象是什么，比如人、红绿灯等</li><li>随时间的持续观察移动物体并保持一致</li><li>图像中的每个像素与语义类别进行匹配，如道路、汽车、天空，边界清晰</li></ul></li><li>方式方法<ul><li>图像、点云、雷达反射值数据：</li><li>监督学习、半监督学习、强化学习</li><li>R-CNN、YOLO、SSD</li><li>计算融合问题：<ul><li>前融合、后融合 <h4 id="预测-Prediction"><a href="#预测-Prediction" class="headerlink" title="预测 Prediction"></a>预测 Prediction</h4></li></ul></li></ul></li><li>实时性和准确性</li><li>基于状态进行预测<ul><li>卡尔曼滤波</li><li>Particle滤波</li></ul></li><li>基于车道序列进行预测<ul><li>通过机器学习模型化简为分类问题</li></ul></li><li>行人预测：无人车需要非常重视安全问题，其中人的安全最为重要，而行人的意图变化却是最难预测的也是约束最少的，另外对于不同的障碍物也要有不同的理解，比如人和狗</li></ul><h4 id="决策-Planning"><a href="#决策-Planning" class="headerlink" title="决策 Planning"></a>决策 Planning</h4><ul><li>导航线路规划和精细轨迹表述</li><li>数学问题转换：将物理世界的地图转换为数学上的图表达</li><li>最优路径搜索：由于其他软件模块已经将不确定性进行了最大程度的消除，而最终决策规划模块又是对稳定性要求极高的模块，因此可以通过数学上的最优路径求解出确定解，遍历最优解是非常耗时的。</li><li>需要考虑车辆的体感和安全性<br><img src="http://uptolimit.top/img/301935921247779.png" alt="301935921247779" style="zoom:50%;"><h4 id="控制-Control"><a href="#控制-Control" class="headerlink" title="控制 Control"></a>控制 Control</h4></li><li><p>输入信息：目标轨迹、车辆状态；输出：方向盘、油门</p><ul><li>实现对无人车的控制，我们需要知道踩刹车和减速的关系、踩油门和加速的关系等，当无人车拿到一些控制学参数后，通过电脑对无人车进行控制</li><li>控制是对整个驾驶最后的保障，因此需要在任何情况下对准确性、稳定性和时效性要求都非常高，需要通过对车辆模型精细化描述进行严格的数学表达</li><li>传统的控制算法PID可以满足车辆控制要求，但是考虑到体感和一些极限情况，控制算法优化也是目前无人车的一个持续探讨的问题，如LQR、MPC等<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><h3 id="我在哪"><a href="#我在哪" class="headerlink" title="我在哪"></a>我在哪</h3><img src="http://uptolimit.top/img/560650622245281.png" alt="560650622245281" style="zoom:67%;"></li></ul><h3 id="我周围有什么"><a href="#我周围有什么" class="headerlink" title="我周围有什么"></a>我周围有什么</h3><p><img src="http://uptolimit.top/img/324370722248962.png" alt="324370722248962" style="zoom:67%;"></p><h3 id="他们到哪儿去"><a href="#他们到哪儿去" class="headerlink" title="他们到哪儿去"></a>他们到哪儿去</h3><p><img src="http://uptolimit.top/img/324370722248962.png" alt="324370722248962" style="zoom:67%;"></p><h3 id="我该怎么走"><a href="#我该怎么走" class="headerlink" title="我该怎么走"></a>我该怎么走</h3><p><img src="http://uptolimit.top/img/525040722244098.png" alt="525040722244098" style="zoom:67%;"></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 无人驾驶 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 无人驾驶 </tag>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>背包九讲</title>
      <link href="2021/10/30/bei-bao-jiu-jiang/"/>
      <url>2021/10/30/bei-bao-jiu-jiang/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 蓝桥杯 </tag>
            
            <tag> 背包问题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Visual studio与C++</title>
      <link href="2021/09/16/visual-studio-yu-c/"/>
      <url>2021/09/16/visual-studio-yu-c/</url>
      
        <content type="html"><![CDATA[<h2 id="Visual-Studio-的Debug"><a href="#Visual-Studio-的Debug" class="headerlink" title="Visual Studio 的Debug"></a>Visual Studio 的Debug</h2><h3 id="Build过程中的项目和编译错误"><a href="#Build过程中的项目和编译错误" class="headerlink" title="Build过程中的项目和编译错误"></a>Build过程中的项目和编译错误</h3><ol><li><p>Build Solution</p><p>Solution的两种基本配置类型：Debug和Release</p><p>|                            Debug                             |                    Release                     |<br>| :—————————————————————————————: | :——————————————————————: |<br>| 生成的文件未经优化，大而慢，内含开发人员添加的调试信息，不会被投入市场使用 | 生成的文件经过优化，小而快，可以被投入市场使用 |</p></li></ol><h3 id="Run过程中的动态错误"><a href="#Run过程中的动态错误" class="headerlink" title="Run过程中的动态错误"></a>Run过程中的动态错误</h3>]]></content>
      
      
      <categories>
          
          <category> 开发工具笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 工具 </tag>
            
            <tag> Trciks </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C++笔记（杂项）</title>
      <link href="2021/09/07/c-bi-ji-za-xiang/"/>
      <url>2021/09/07/c-bi-ji-za-xiang/</url>
      
        <content type="html"><![CDATA[<h2 id="变量作用域"><a href="#变量作用域" class="headerlink" title="变量作用域"></a>变量作用域</h2><p>我们可以简单的理解一个变量的<strong>作用域（scope）</strong>一般可以被界定为变量声明语句之后、包裹了它的声明语句的最内一层<code>{}</code>之内。</p><h2 id="递归问题"><a href="#递归问题" class="headerlink" title="递归问题"></a>递归问题</h2><h3 id="头递归"><a href="#头递归" class="headerlink" title="头递归"></a>头递归</h3><pre class=" language-lang-c++"><code class="language-lang-c++">int fac(int n) {    if (n == 1) {        return 1;    }    return fac(n - 1) * n;}</code></pre><p>在一般条件满足时（如这里的<code>n==1</code>）满足时，返回一个确定的值，而在其他情况下，返回一个包含本身函数的递归调用的设计，称为<strong>头递归</strong>（head recursion），在头递归的实现中，我们在进行下一层的调用前，没有进行计算。在下一层返回后，我们才完成了这一层的计算。</p><h3 id="尾递归"><a href="#尾递归" class="headerlink" title="尾递归"></a>尾递归</h3><pre class=" language-lang-cpp"><code class="language-lang-cpp">int fac(int n, int product){    if(n == 0){        return product;    }    product = product * n;    return fac(n - 1, product);}</code></pre><p>在<strong>尾递归</strong>的实现中，每一次函数的递归调用都会将一个阶段性的结果传递到下一个被调用的函数中，当最终的一般条件（如这里的<code>n==0</code>）满足时，把最终结果直接返回。我们在进行下一层的调用前，会先进行计算，在最终一般条件满足时，会将计算的结果逐层直接返回。</p><p>学习递归的函数调用更多是学习这种与数学归纳法有异曲同工之妙的思想。在遇到问题时，很多情况下，我们可以把待解问题逐层归约为更简单的问题，直到这个问题变为一个显而易见的一般性问题。这种思想在程序设计中就是递归。</p><h3 id="递归求解汉诺塔问题"><a href="#递归求解汉诺塔问题" class="headerlink" title="递归求解汉诺塔问题"></a>递归求解汉诺塔问题</h3><ol><li>1 个盘子：直接移动， “N==1” 是递归终结条件。</li><li>N 个盘子：把移动 N 个盘子的问题转化为移动 N-1 盘子的问题。<br>(1) 把 A 上面的 N-1 个盘子移动 B（借助 C）；<br>(2) 把第 N 个盘子一道 C；<br>(3) 把 B 上的 N-1 个盘子移到 C（借助 A）</li></ol><pre class=" language-lang-cpp"><code class="language-lang-cpp">#include<iostream>using std::cin;using std::cout;using std::endl;void hanoi(int n, char left, char middle, char right) {    if (n == 1) {        cout << left << "-->" << right << endl;    }    else {        hanoi(n - 1, left, right, middle);        cout << left << "-->" << right << endl;        hanoi(n - 1, middle, left, right);    }}int main() {    int n;    cin >> n;    hanoi(n, 'A', 'B', 'C');    return 0;}</code></pre><h2 id="参数传递"><a href="#参数传递" class="headerlink" title="参数传递"></a>参数传递</h2><h3 id="引用传递"><a href="#引用传递" class="headerlink" title="引用传递"></a>引用传递</h3><p><code>&amp;</code>符号既可以表示引用，也可以表示取地址运算符。<code>&amp;</code>符号出现在变量声明中表示声明一个引用，而出现在某个具体的表达式中则表示取地址运算符。</p><pre class=" language-lang-cpp"><code class="language-lang-cpp">int a = 3;int &ra = a; //定义引用int *p = &a;  //取地址</code></pre><h3 id="默认参数"><a href="#默认参数" class="headerlink" title="默认参数"></a>默认参数</h3><pre class=" language-lang-cpp"><code class="language-lang-cpp">int add(int x = 5, int y = 6){ //声明默认形参    return x+y;}int main(){    add(10, 20); //用实参初始化形参，实现10+20    add(10);//x初始化为10，y使用默认值6    add(); //x和y都使用默认值，即5+6}</code></pre><p>需要注意的是，有默认值的形参必须放在形参列表的最后，也就是说。在有默认值的形参右边。不能出现没有默认值的形参。如下所示</p><pre class=" language-lang-cpp"><code class="language-lang-cpp">int add(int x, int y=5, int z=6);</code></pre><p>而下面的用法是错误的</p><pre class=" language-lang-cpp"><code class="language-lang-cpp">int add(int x=1, int y=5, int z);int add(int x, int y=5, int z);</code></pre><h3 id="参数类型为函数"><a href="#参数类型为函数" class="headerlink" title="参数类型为函数"></a>参数类型为函数</h3><p>函数不像变量一样可以进行值传递，我们将函数作为参数传递时，需要传递它的地址。</p><p><img src="https://gitee.com/Gugoole/PicBed/raw/master/e01c23d19ebab4e5988cd48c8e2a2a87f3ca9eb3.png" alt="e01c23d19ebab4e5988cd48c8e2a2a87f3ca9eb3" style="zoom:80%;"></p><pre class=" language-lang-cpp"><code class="language-lang-cpp">int g(float (*f)(int), int a){    return (*f)(a);}</code></pre><p>在调用函数<code>g</code>，并且将函数地址作为参数时，直接将函数名传入就可以了</p><pre class=" language-lang-cpp"><code class="language-lang-cpp">float sqrt_minus_one(int x){    return sqrt(x) - 1;}</code></pre><pre class=" language-lang-cpp"><code class="language-lang-cpp">g(sqrt_minus_one, number)</code></pre><h2 id="函数重载"><a href="#函数重载" class="headerlink" title="函数重载"></a>函数重载</h2><p>编译器根据<strong>实参与形参的类型以及个数</strong>的最佳匹配，自动确定要调用的函数，这就是函数的重载。</p><p>简而言之，函数要可以重载，两个名字相同的函数必须具有不同的形参——这里的“不同”，指的是形参的个数或者形参的类型，这两者只要有一方面不同，就可以认为是不同的形参。</p><p>函数是否可以重载，与函数的返回值，以及形参的名称无关，即如果函数名称和形参类型都相同的话，不管返回值是否相同，都会被编译器判定为语法错误（函数重复定义）。</p><pre class=" language-lang-cpp"><code class="language-lang-cpp">int add(int x, int y);float add(float x, float y); // 形参类型不同int add(int x, int y);int add (int x, int y, int z); //形参个数不同</code></pre><p>对于有默认形参的函数重载时，需要避免二义性：</p><pre class=" language-lang-cpp"><code class="language-lang-cpp">void fun(int length, int width=2, int height=3);void fun(int length);</code></pre><h2 id="函数调用栈"><a href="#函数调用栈" class="headerlink" title="函数调用栈"></a>函数调用栈</h2><h3 id="全局变量与局部变量"><a href="#全局变量与局部变量" class="headerlink" title="全局变量与局部变量"></a>全局变量与局部变量</h3><p>对于全局变量来说。在编译器编译产生的目标代码中可以使用一个唯一确定的地址进行定位。</p><h3 id="调用栈的实质"><a href="#调用栈的实质" class="headerlink" title="调用栈的实质"></a>调用栈的实质</h3><p>调用栈的实质是系统分配的一段连续区域的内存空间——调用栈中的数据，在存储的时候按照函数的调用，分成一个一个的“小方格”，成为<strong>栈帧</strong>，每一个栈帧都跟一次函数调用一一对应。栈帧中存储了一系列跟函数本身有关的信息，包括这次函数调用中的<strong>形参值</strong>、<strong>局部变量值</strong>、<strong>一些控制信息</strong>和<strong>一些临时数据</strong>。</p><p>每一次函数调用时，都会有一个栈帧被压入调用栈，调用完后相应的栈帧就会被弹出。</p><p>一个函数在执行过程中，能访问的除了栈外的全局变量外，只有它自己的栈帧。</p><h2 id="类与面向对象"><a href="#类与面向对象" class="headerlink" title="类与面向对象"></a>类与面向对象</h2><h3 id="构造函数的特点"><a href="#构造函数的特点" class="headerlink" title="构造函数的特点"></a>构造函数的特点</h3><ol><li>如果参数列表为空，则不为数据成员赋初值</li><li>如果类内定义了成员的初始值，则使用定义的初始值</li><li>如果没有定义类内初始值，则以默认方式初始化</li><li>基本类型的数据默认初始化的值是不确定的</li></ol><h3 id="拷贝构造函数"><a href="#拷贝构造函数" class="headerlink" title="拷贝构造函数"></a>拷贝构造函数</h3><p>它的作用就是用一个已有的对象来执行一个新的对象的构造。拷贝构造函数具有一般构造函数的所有特性——它的<strong>形参是本类的一个对象的引用</strong>，作用是用一个已经存在的对象（即为函数的参数）来初始化一个新的对象，拷贝构造函数使用<strong>引用传参</strong></p><h4 id="拷贝构造函数会在什么时候被调用？"><a href="#拷贝构造函数会在什么时候被调用？" class="headerlink" title="拷贝构造函数会在什么时候被调用？"></a>拷贝构造函数会在什么时候被调用？</h4><ol><li><p>当用类的一个对象去初始化该类的另一个对象的时候：</p><pre class=" language-lang-cpp"><code class="language-lang-cpp">Point a(1, 2);Point b(a);//用对象a初始化对象b，拷贝构造函数被调用Point c=b;//用对象b给对象c赋值，拷贝构造函数被调用</code></pre></li><li><p>当函数的形参是类的对象，调用函数时进行形实结合的时候：</p><pre class=" language-lang-cpp"><code class="language-lang-cpp">void f(Point p){    //code here}int main(){    Point a(1, 2);    f(a);    return 0;}</code></pre></li><li><p>当函数的返回值是类的对象，函数执行完成返回调用者的时候：</p><pre class=" language-lang-cpp"><code class="language-lang-cpp">/*函数调用完成，这里的临时对象a会被销毁，函数返回的是通过拷贝构造函数，在主调函数中用a重新构造的对象。*/Point g(){    Point a(1, 2);    return a;}</code></pre></li></ol><h4 id="初始化列表"><a href="#初始化列表" class="headerlink" title="初始化列表"></a>初始化列表</h4><p>大多数情况下，使用初始化列表，或者是在构造函数中赋值，两种方法的效果是差不多的——而初始化列表在效率上相对于在构造函数中赋值，会存在一定的优势，如果函数的成员变量是基本数据类型的话，这种优势不会十分明显，但是如果函数的成员变量是较为复杂的自定义数据类型的对象的话，那么初始化列表在调用开销的节约方面，优势会非常显著。</p><p>在某些情况下，比如需要初始化的数据成员是对象（继承时调用基类构造函数），需要初始化<code>const</code>修饰的类成员，或者需要初始化引用成员数据，都必须使用初始化列表来进行初始化。</p><h3 id="类的前向引用声明"><a href="#类的前向引用声明" class="headerlink" title="类的前向引用声明"></a>类的前向引用声明</h3><p>指的是引用未定义的类之前，首先把这个类的名字告诉编译器，这样编译器遇到标识符的时候，就知道那是一个类名，不会引发编译错误。<strong>需要注意的是</strong>，尽管使用了前向引用声明，但是在提供了一个完整的类定义之前，不能定义该类的对象，也不能在成员函数中使用该对象。</p><pre class=" language-lang-cpp"><code class="language-lang-cpp">class Elder;class Toad{    Elder h;//错误，类的定义不完整};class Elder{    Toad h;};</code></pre><p>这个时候我们可以使用前向引用声明类的引用或者指针，这样就不会发生编译错误了。</p><pre class=" language-lang-cpp"><code class="language-lang-cpp">class Elder;class Toad{    Elder &h;};class Elder{    Toad &h;};</code></pre><p><strong>注意</strong>，即便我们声明了引用，我们仍然不能在类内的方法定义中直接使用i当以不完善的类的方法：</p><pre class=" language-lang-cpp"><code class="language-lang-cpp">class Elder;class Toad{    private:        Elder &h;    public:        void setWeather(){                h.sunny();//不合法，因为此时h尚未定义完善        }}</code></pre><h3 id="类的深拷贝与浅拷贝"><a href="#类的深拷贝与浅拷贝" class="headerlink" title="类的深拷贝与浅拷贝"></a>类的深拷贝与浅拷贝</h3><p>当一个对象被建立的时候，系统会通过我们定义的构造函数新建一个对象，如果我们没有定义拷贝构造函数，系统默认的拷贝构造函数在调用的时候，就会建立一个完全相同的对象（<strong>地址也是相同的</strong>），也即意味着新的对象赋给的指针指向的实际上是被拷贝的对象所建立的堆对象，即我们所说的“<strong>浅拷贝</strong>”。相对于此，我们更希望拷贝构造的对象应该有自己的堆对象，并且堆对象内部存储的数据跟被拷贝的对象完全一样，因此我们需要自己定义拷贝构造函数。</p><h2 id="数组、多维数组与字符串"><a href="#数组、多维数组与字符串" class="headerlink" title="数组、多维数组与字符串"></a>数组、多维数组与字符串</h2><h3 id="二维数组的旋转"><a href="#二维数组的旋转" class="headerlink" title="二维数组的旋转"></a>二维数组的旋转</h3><h4 id="坐标变换的对应关系"><a href="#坐标变换的对应关系" class="headerlink" title="坐标变换的对应关系"></a>坐标变换的对应关系</h4><pre class=" language-lang-cpp"><code class="language-lang-cpp"># m行n列矩阵旋转90°for (int i = 0; i < m; i++) {    for (int j = 0; j < n; j++) {        j && cout << " ";        cout << a[m-1-j][i];    }    cout << endl;}</code></pre><pre class=" language-lang-cpp"><code class="language-lang-cpp"># m行n列矩阵旋转180°for (int i = 0; i < m; i++) {    for (int j = 0; j < n; j++) {        j && cout << " ";        cout << a[m-1-j][n-1-i];    }    cout << endl;}</code></pre><pre class=" language-lang-cpp"><code class="language-lang-cpp"># m行n列矩阵旋转270°for (int i = 0; i < m; i++) {    for (int j = 0; j < n; j++) {        j && cout << " ";        cout << a[j][n-1-i];    }    cout << endl;}</code></pre><h3 id="矩阵乘法"><a href="#矩阵乘法" class="headerlink" title="矩阵乘法"></a>矩阵乘法</h3><pre class=" language-lang-cpp"><code class="language-lang-cpp">#include <iostream>using namespace std;int main() {    int matrix_a[10][10];    int matrix_b[10][10];    int m;    int n;    int res = 0;    int matrix_res[10][10];    cin >> m >> n;    for (int i = 0; i < m; i++) {        for (int j = 0; j < n; j++) {            cin >> matrix_a[i][j];        }    }    for (int i = 0; i < n; i++) {        for (int j = 0; j < m; j++) {            cin >> matrix_b[i][j];        }    }    //核心代码    for (int i = 0; i < m; i++) {        for (int j = 0; j < m; j++) {            res = 0;            for (int k = 0; k < n; k++) {                res += matrix_a[i][k] * matrix_b[k][j];            }            matrix_res[i][j] = res;        }    }    for (int i = 0; i < m; i++) {        for (int j = 0; j < m; j++) {            j&& cout << " ";            cout << matrix_res[i][j];        }        cout << endl;    }    return 0;}</code></pre><h2 id="简单算法入门"><a href="#简单算法入门" class="headerlink" title="简单算法入门"></a>简单算法入门</h2><h3 id="筛法列举质数"><a href="#筛法列举质数" class="headerlink" title="筛法列举质数"></a>筛法列举质数</h3><p>根据基本数论，如果$n$为合数，$c$为$n$的最小正因数，则有</p><script type="math/tex; mode=display">1\le c\le\sqrt n\Rightarrow 1\le c^2\le n</script><p>因此我们确信，只要找到了$c$，就可以确定$n$是合数，并将$n$进行标记。</p><pre class=" language-lang-cpp"><code class="language-lang-cpp">#include <cstdio>int main() {    int n = 15;    int mark[16] = {        1, 1, 0, 0,        0, 0, 0, 0,        0, 0, 0, 0,        0, 0, 0, 0    };    int c;    int j;    for (c = 2; c * c <= n; c++) {        if (mark[c]!=1){            for(j=2; j <= n/c; j++){                mark[c*j]=1;            }        }    }    for ( c = 2; c<=n;c++){        if(mark[c]!=1){            printf("%d\n",c);        }    }    return 0;}</code></pre><h3 id="指定范围的质数"><a href="#指定范围的质数" class="headerlink" title="指定范围的质数"></a>指定范围的质数</h3><pre class=" language-lang-cpp"><code class="language-lang-cpp">#include <cstdio>#include <cstring>#include <iostream>using namespace std;int n = 1000000;int mark[1000001];int main() {    int c;    int n, m;    int i,j;    cin >> n >> m;    memset(mark, 0, sizeof(mark));    mark[0] = 1;    mark[1] = 1;    for (c = 2; c * c <= n; c++) {        if (mark[c] != 1) {            for (j = 2; j <= n / c; j++) {                mark[c * j] = 1;            }        }    }    for (i = m; i < n; i++) {        if (mark[i] != 1) {            cout << i;            break;        }    }    for (int k = i+1 ; k <= n; k++) {        if (mark[k] != 1) {            cout << endl;            cout << k ;        }    }    return 0;}</code></pre><h3 id="二分查找"><a href="#二分查找" class="headerlink" title="二分查找"></a>二分查找</h3><pre class=" language-lang-cpp"><code class="language-lang-cpp">#include<stdio.h>#define MAX 1000000int arr[MAX +5] = {0};//定义两个全局数组 int sch[MAX + 5] = {0};int binary_search (int *arr, int n, int k) { //二分查找     int tail = n - 1, head = 0, mid;    while(head <= tail) {        mid = (tail + head) >> 1;        if(arr[mid] == k) return mid;        else if(arr[mid] > k) tail = mid - 1;        else head =  mid + 1;     }    return -1;}int main() {    int N, K;    scanf("%d %d", &N, &K); //    for(int i = 0; i < N; i++) { //循环输入数组         scanf("%d", &arr[i]);    }    for(int j = 0; j < K; j++) { //循环输入 待查找的数         scanf("%d", &sch[j]);    }    for(int j = 0; j < K; j++) { //循环调用二分查找函数并输出找的索引号         int index = binary_search(arr, N, sch[j]);        j && printf(" ");        printf("%d", index + 1);    }        return 0;}</code></pre><h3 id="冒泡排序"><a href="#冒泡排序" class="headerlink" title="冒泡排序"></a>冒泡排序</h3><pre class=" language-lang-cpp"><code class="language-lang-cpp">#include <cstdio>#include <iostream>#include <algorithm>using namespace std;int main() {    int n = 10;    int m;    int numbers[10];    int i;    int j;    fill(numbers, numbers + 10, 0);    // 读入给定的数字    for (i = 0; i < n; i++) {        scanf_s("%d", &numbers[i]);    }    for (i = 0; i < n - 1; i++) {        for (j = 0; j < n-1-i; j++) {            if (min(numbers[j+1], numbers[j])) {                swap(numbers[j+1], numbers[j]);            }        }    }    for (i = 0; i < n; i++) {        i&& cout << " ";        cout << numbers[i];    }    return 0;}</code></pre><h3 id="选择排序"><a href="#选择排序" class="headerlink" title="选择排序"></a>选择排序</h3><pre class=" language-lang-cpp"><code class="language-lang-cpp">#include <cstdio>#include <iostream>#include <algorithm>#define min(a, b) a &lt b?1:0using namespace std;int main() {    int n = 10;    int m;    int numbers[10];    int i;    fill(numbers, numbers + 10, 0);    // 读入给定的数字    for (i = 0; i < n; i++) {        scanf_s("%d", &numbers[i]);    }    for (i = 0; i < n; i++) {        for (int j = i+1; j < n; j++){            if (min(numbers[i], numbers[j])) {                swap(numbers[i], numbers[j]);            }        }    }    for (i = 0; i < n; i++) {        i&& cout << " ";        cout << numbers[i];    }    return 0;}</code></pre><h2 id="指针运算与类型转化"><a href="#指针运算与类型转化" class="headerlink" title="指针运算与类型转化"></a>指针运算与类型转化</h2><h3 id="隐式类型转化"><a href="#隐式类型转化" class="headerlink" title="隐式类型转化"></a>隐式类型转化</h3><p><img src="https://res.jisuanke.com/img/upload/20160706/6baa8d67540351c7839ecc9a203535d3b47a47ee.png" alt="隐式类型转换" style="zoom:80%;"></p><h2 id="函数指针与函数式编程"><a href="#函数指针与函数式编程" class="headerlink" title="函数指针与函数式编程"></a>函数指针与函数式编程</h2><h3 id="函数指针"><a href="#函数指针" class="headerlink" title="函数指针"></a>函数指针</h3><p>使用函数地址作为函数参数，就是将一个函数指针变量作为地址传给另一个函数。函数名在表示函数代码起始地址的同时，也包括函数的返回值类型，以及参数的个数、类型、排列次序等信息。因此，在通过函数名调用函数的时候，编译器就可以自动检查实参与形参是否相符，用函数的返回值参与其他运算是，能够自动进行类型一致性检查。</p><p>这种将一个函数作为参数交给另一个函数的编程范式叫做函数式编程，典型的函数式编程语言有LISP,Scala等</p><pre class=" language-lang-cpp"><code class="language-lang-cpp">typedef int (* DoubleIntFunction)(double);//声明函数指针DoubleIntFunction funcPtr;//使用函数指针void printFloat(float data){    cout << "the data to print is " << data << endl;}void (* func_ptr) (float); func_ptr = printFloat;//给函数指针赋值func_ptr(3.01);//等价于直接调用printFloath</code></pre><h3 id="对象指针"><a href="#对象指针" class="headerlink" title="对象指针"></a>对象指针</h3><p>跟基本类型的变量一样，每一个对象再初始化之后，都会在内存中占据一定的空间，所以我们可以通过地址来访问一个对象。尽管对象同时包含了数据和函数两种成员，但是<strong>对象所占据的内存空间只用于存放数据成员</strong>，函数成员并不在每一个对象的存储副本中。</p><pre class=" language-lang-cpp"><code class="language-lang-cpp">Line * line_ptr; //声明Line类的对象指针变量line_ptrLine l1; //声明Line类实例对象l1line_ptr = &l1; //取l1的地址赋值给line_ptr</code></pre><p>C++为每一个类的非静态成员函数都提供了一个隐含的指针<code>this</code>，它是成员函数隐藏的一个形参，当我们在成员函数中操作对象的数据成员的时候，实际上就是在使用<code>this</code>指针</p><pre class=" language-lang-cpp"><code class="language-lang-cpp">int getLength(){    return length;//等价于 return this->length;}</code></pre><h2 id="动态内存分配"><a href="#动态内存分配" class="headerlink" title="动态内存分配"></a>动态内存分配</h2><p>动态内存分配可以保证程序在运行的过程中，可以按照实际需要申请适量的内存，等到使用结束之后将其释放，这种在程序运行的过程中申请和释放的存储单元也称为<strong>堆对象</strong>，而动态内存分配所调用的内存空间则称为<strong>堆内存</strong></p><ul><li>使用<code>new</code>运算符可以在堆内存上创建一个新的堆对象，<code>new</code>运算符返回指针该对象的指针</li></ul><h3 id="new与malloc的区别"><a href="#new与malloc的区别" class="headerlink" title="new与malloc的区别"></a><code>new</code>与<code>malloc</code>的区别</h3><p>C语言中动态申请内存使用<code>malloc</code>函数，我们需要引入<code>malloc.h</code>或者<code>stdlib.h</code>文件</p><pre class=" language-lang-cpp"><code class="language-lang-cpp">int *p;p = (int *) malloc(sizeof(int))</code></pre><p><code>malloc</code>返回默认返回<code>void *</code>的空指针类型，在<code>malloc</code>之前添加<code>(int *)</code>会将这片内存空间的起始地址标记为整数型的地址，使之与整数型的指针变量相匹配。</p><p>如果我们想要在堆上建立一个容量为<code>n</code>的数组的话，可以这么写：</p><pre class=" language-lang-cpp"><code class="language-lang-cpp">p = (int *) malloc(n * sizeof(int))；//相当于p = new int [n];</code></pre><p>使用完后，我们需要使用<code>free</code>函数来释放它</p><pre class=" language-lang-cpp"><code class="language-lang-cpp">free(p);//相当于delete[] p;</code></pre><p>这里需要说明的是，<code>new</code>运算符和<code>malloc</code>函数，并不只是有语法层面的不同——两者的功能是完全不一样的。<code>malloc</code>只是分配一块指定大小的堆内存，而<code>new</code>则不同，它同时包括两个步骤：</p><ul><li>调用<code>malloc</code>分配一片堆内存</li><li>遍历变量或者对象（如果要建立对象的话，这一步<code>new</code>运算符会调用类的构造函数来完成）</li></ul><p>这里我们我们引入一个新的、重载的<code>new</code>运算符——“就地构造”（placement new）。它的作用是在指定的内存地址上构造对象。语法形式如下：</p><pre class=" language-lang-cpp"><code class="language-lang-cpp">new(address) Object();</code></pre><pre class=" language-lang-cpp"><code class="language-lang-cpp">Class *p = new Class();</code></pre><p>等价于</p><pre class=" language-lang-cpp"><code class="language-lang-cpp">Class *p = (Class *)malloc(sizeof(Class));new(p) Class();</code></pre><p>如果一些堆上的对象需要反复构造和销毁的话，可以使用<code>malloc</code>函数配合placement new，这样可以节省分配内存的开销。</p><h2 id="C-简易内存模型"><a href="#C-简易内存模型" class="headerlink" title="C++简易内存模型"></a>C++简易内存模型</h2><p>在这个模型中我们将一个C++程序在运行时候所占据的内存空间分为四个部分：</p><ul><li>栈区(Stack)</li><li>堆区(Heap)</li><li>全局区/静态区(Global/Static)</li><li>常量区(Constant)</li></ul><p><strong>Tips：</strong>这里的栈区与堆区的概念与数据结构中的“栈”与“堆”不是一个概念，不要弄混了！！！</p><p>在C++程序的内存空间中，我们在代码中声明的局部变量，以及函数的形式参数，都保存在栈区中——这部分内存在程序运行的时候会自动分配，而在不需要的时候也会自动释放，并不需要程序员自己取手动维护。</p><p>而我们使用<code>new</code>运算符，或者C语言中的<code>malloc</code>函数进行动态内存分配之后，系统为我们划分的内存空间，来自于<strong>堆区</strong>。因为程序并不知道这些动态分配的内存到什么时候会派不上用场，所以程序并不会堆这部分已经分配的内存做任何处理。</p><p>因此，如果我们不再需要一个堆对象的话，我们就必须手动使用<code>delete</code>运算符或者C语言中的<code>free</code>函数来删除这个堆对象，释放对应的内存空间，否则就会造成内存泄漏。</p><p>对于全局变量与静态变量，其保存在内存中的全局/静态区，即全局变量和静态变量是存储在同一块内存空间中的。我们可以简单地认为，程序启动之后，全局/静态变量就保存在这里，而程序结束之后，对应的内存空间就会自动释放。</p><p>最后是常量区，实际上程序中并<strong>不存在</strong>这样一个区域，为了方便理解，我们可以简单的认为，所有的常量（以<code>const</code>开头）都存放在一起，这些区域<strong>不可释放</strong>。</p><p><img src="https://gitee.com/Gugoole/PicBed/raw/master/image-20210922173258374.png" alt="image-20210922173258374" style="zoom:80%;"></p><h2 id="数据的共享与保护"><a href="#数据的共享与保护" class="headerlink" title="数据的共享与保护"></a>数据的共享与保护</h2><h3 id="常量指针与指针常量"><a href="#常量指针与指针常量" class="headerlink" title="常量指针与指针常量"></a>常量指针与指针常量</h3><ul><li><p>常量指针是指指针指向的内容是常量，定义方式有两种：</p><pre class=" language-lang-cpp"><code class="language-lang-cpp">const int * n;int const * n;</code></pre><p>常量指针说的是不能通过这个指针改变变量的值，但是还是可以通过引用来改变变量的值</p><pre class=" language-lang-cpp"><code class="language-lang-cpp">int a = 5;const int * = &a;a = 6;</code></pre><p>常量指针指向的值不能改变，但是并不意味着指针本身不能改变，常量指针可以指向其他的地址。</p><pre class=" language-lang-cpp"><code class="language-lang-cpp">int a = 5;int b = 6;const int* n = &a;n = &b;</code></pre></li><li><p>指针常量是指这个指针本身是个常量，不能再指向其他的地址：</p><pre class=" language-lang-cpp"><code class="language-lang-cpp">int * const n = address;</code></pre><p>需要注意的是，指针常量指向的地址不能改变，但是你可以通过指针常量取改变指针所指向的变量的值：</p><pre class=" language-lang-cpp"><code class="language-lang-cpp">int a = 5;int * const n = &a;*n = 8;</code></pre></li></ul><p>区分常量指针和指针常量的关键就在于<strong><code>*</code>的位置</strong>，我们以<code>*</code>为分界线，如果<code>const</code>在<code>*</code>的左边，则为常量指针，如果<code>const</code>在<code>*</code>右边则为指针常量。我们也可以这样考虑，如果我们将<code>*</code>读作指针，将<code>const</code>读作常量的话，那么正好与之相符，<code>int * const n</code>是指针常量，<code>int const * n</code>是常量指针；</p><h3 id="常引用"><a href="#常引用" class="headerlink" title="常引用"></a>常引用</h3><p>我们知道，函数的参数传递既可以是值传递，也可以是引用传递，如果在引用前面加上<code>const</code>关键字，那就是常引用，例如有构造函数如下</p><pre class=" language-lang-cpp"><code class="language-lang-cpp">class ( const  class & other)</code></pre><p>如果通过一个常引用来访问一个对象，那就意味着我们不能通过这个引用来修改对象。为什么要使用常引用呢？</p><p>我们知道引用传递的时候，对于一个拷贝构造函数来说，其参数就是另一个对象，在形实结合过程中，会构造一个临时变量。如果我们的类比较大， 那个这个构造临时变量的过程就会产生非常大的额外开销。所以我们需要传递常引用，一方面不再额外构造临时对象，另一方面常引用不允许我们通过它来修改对象的内容，对于拷贝构造函数来说，可以保证数据的安全性。</p>]]></content>
      
      
      <categories>
          
          <category> 重学C++语言 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程语言 </tag>
            
            <tag> C++语言 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Numpy基础知识</title>
      <link href="2021/05/12/numpy-ji-chu-zhi-shi/"/>
      <url>2021/05/12/numpy-ji-chu-zhi-shi/</url>
      
        <content type="html"><![CDATA[<h2 id="Numpy数组"><a href="#Numpy数组" class="headerlink" title="Numpy数组"></a>Numpy数组</h2><h2 id="创建数组"><a href="#创建数组" class="headerlink" title="创建数组"></a>创建数组</h2><h2 id="文件IO"><a href="#文件IO" class="headerlink" title="文件IO"></a>文件IO</h2><h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2><h2 id="数组的属性"><a href="#数组的属性" class="headerlink" title="数组的属性"></a>数组的属性</h2><h2 id="数学计算"><a href="#数学计算" class="headerlink" title="数学计算"></a>数学计算</h2>]]></content>
      
      
      <categories>
          
          <category> Numpy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> 数据分析 </tag>
            
            <tag> Numpy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>快速搭建SUMO仿真实例</title>
      <link href="2021/05/11/kuai-su-da-jian-sumo-fang-zhen-shi-li/"/>
      <url>2021/05/11/kuai-su-da-jian-sumo-fang-zhen-shi-li/</url>
      
        <content type="html"><![CDATA[<h2 id="安装SUMO"><a href="#安装SUMO" class="headerlink" title="安装SUMO"></a>安装SUMO</h2><p>首先在本地PC上安装好SUMO仿真软件，下载<a href="https://www.eclipse.org/sumo/">地址</a>，本文使用的版本是<strong>SUMO 1.9.1 FOR WINDOWS 64BITS</strong>。安装好后，在安装根目录下创建一个新的文件夹命名为map。</p><p><img src="https://cdn.jsdelivr.net/gh/UPTOLIMIT/PicBed//img/image-20210511233435589.png" alt="image-20210511233435589"></p><h2 id="安装Python环境"><a href="#安装Python环境" class="headerlink" title="安装Python环境"></a>安装Python环境</h2><p>生成配置文件的过程需要调用Python执行Python脚本，因此需要支持Python环境。</p><h2 id="获取目标区域地图"><a href="#获取目标区域地图" class="headerlink" title="获取目标区域地图"></a>获取目标区域地图</h2><p>从<a href="https://www.openstreetmap.org/">openstreetmap</a>获取需要的地图文件，并做相应处理</p><p><img src="https://cdn.jsdelivr.net/gh/UPTOLIMIT/PicBed//img/image-20210511231526505.png" alt="image-20210511231526505"></p><p>点击==导出==，左侧会出现手动选择工具栏和许可协议，再次点击许可协议下方的==导出==按钮，会获取到map.osm文件，文件中不仅包含路网信息也包含大量的别的模块例如建筑和河流，这些模块可以用 ==polyconvert== 工具添加到 sumo-gui 配置文件中。在C盘生成的Sumo文件夹下创建新的map文件夹，将map.osm文件放进去。</p><h2 id="配置SUMO仿真环境"><a href="#配置SUMO仿真环境" class="headerlink" title="配置SUMO仿真环境"></a>配置SUMO仿真环境</h2><ul><li><p>从SUMO安装目录中打开Sumo\doc\userdoc\Networks\Import\OpenStreetMap.html文件</p><p><img src="https://cdn.jsdelivr.net/gh/UPTOLIMIT/PicBed//img/image-20210511233353767.png" alt="image-20210511233353767"></p></li><li><p>点击左侧标记模块<code>Importing additional Polygons (Buildings, Water, etc.)</code>，复制如下代码块</p><p><img src="https://cdn.jsdelivr.net/gh/UPTOLIMIT/PicBed//img/image-20210511233906065.png" alt="image-20210511233906065"></p></li><li><p>打开本地文本编辑器，本文中使用的是vscode，新建文件，粘贴入复制的代码，将文件保存到刚才新建的<strong>map文件夹</strong>中，命名为<strong>typemap</strong>，文件类型为<strong>xml</strong>。</p><p><img src="https://cdn.jsdelivr.net/gh/UPTOLIMIT/PicBed//img/image-20210511234416504.png" alt="image-20210511234416504"></p></li><li><p>打开<code>sumo/bin/start-command-line.bat</code>，cd到map目录下，输入如下命令:</p><p><code>netconvert --osm-files map.osm -o map.net.xml</code></p><p><img src="https://cdn.jsdelivr.net/gh/UPTOLIMIT/PicBed//img/image-20210511234752678.png" alt="image-20210511234752678"></p><p><code>polyconvert --net-file map.net.xml --osm-files map.osm --type-file typemap.xml -o map.poly.xml</code></p><p><img src="https://cdn.jsdelivr.net/gh/UPTOLIMIT/PicBed//img/image-20210511234942710.png" alt="image-20210511234942710"></p><p>这两步完成后，我们获得<strong>map.net</strong>和<strong>map.poly</strong>两个文件</p><p><img src="https://cdn.jsdelivr.net/gh/UPTOLIMIT/PicBed//img/image-20210511235134988.png" alt="image-20210511235134988"></p></li><li><p>在当前目录下使用命令行调用python执行sumo的randomTrips.py文件，带参数选项的命令如下：</p><p><code>python "C:\Program Files (x86)\Eclipse\Sumo\tools\randomTrips.py" -n map.net.xml -r map.rou.xml -e 100 -l</code></p><ul><li><code>-n</code>表示输入的类型是<code>net</code></li><li><code>-r</code>表示输入的类型是<code>route</code></li><li><code>-e 100 -l</code>是随机工具配置</li></ul><p>执行成功后，我们获得<strong>map.net.xml</strong>与<strong>map.rou.xml</strong>两个文件。</p><p><img src="https://cdn.jsdelivr.net/gh/UPTOLIMIT/PicBed//img/image-20210511235621154.png" alt="image-20210511235621154"></p></li><li><p>从sumo的example文件夹中拷贝一份test.sumocfg文件，修改其中的input和output部分，然后保存为map.sumocfg文件，存到map目录下。我选用的是<code>C:\Program Files (x86)\Eclipse\Sumo\tools\contributed\saga\example\most.test.sumocfg</code>。</p><p><img src="https://cdn.jsdelivr.net/gh/UPTOLIMIT/PicBed//img/image-20210512000617458.png" alt="image-20210512000617458"></p></li></ul><h2 id="运行map仿真例子"><a href="#运行map仿真例子" class="headerlink" title="运行map仿真例子"></a>运行map仿真例子</h2><ul><li><p>在map目录下打开cmd，运行如下命令：</p><p><code>sumo-gui map.sumocfg</code></p><p><img src="https://cdn.jsdelivr.net/gh/UPTOLIMIT/PicBed//img/image-20210512001020488.png" alt="image-20210512001020488"></p></li><li><p>在打开的sumoGUI程序中，根据个人需求设置延迟时间和车流，本文设置的是200，车流为原始数据的4倍。</p><p><img src="https://cdn.jsdelivr.net/gh/UPTOLIMIT/PicBed//img/image-20210512001326078.png" alt="image-20210512001326078"></p></li><li><p>点击运行，一个简单的sumo仿真实例就此搭建完毕</p><p><img src="https://cdn.jsdelivr.net/gh/UPTOLIMIT/PicBed//img/image-20210512001614501.png" alt="image-20210512001614501"></p><p><img src="https://cdn.jsdelivr.net/gh/UPTOLIMIT/PicBed//img/image-20210512001416640.png" alt="image-20210512001416640"></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> SUMO </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 车联网 </tag>
            
            <tag> 仿真 </tag>
            
            <tag> SUMO </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python基础知识</title>
      <link href="2021/05/11/python-ji-chu-zhi-shi/"/>
      <url>2021/05/11/python-ji-chu-zhi-shi/</url>
      
        <content type="html"><![CDATA[<h2 id="变量与字符串"><a href="#变量与字符串" class="headerlink" title="变量与字符串"></a>变量与字符串</h2><pre class=" language-lang-python"><code class="language-lang-python"># 打印字符串print("Hello World!")# 变量声明msg = "Hello World"print(msg)# 字符串拼接first_name = "albert"last_name = "einstein"full_name = first_name + ' ' + last_nameprint(full_name)</code></pre><h2 id="输入输出"><a href="#输入输出" class="headerlink" title="输入输出"></a>输入输出</h2><pre class=" language-lang-python"><code class="language-lang-python">name = input("please input your name:")print(name)</code></pre><h2 id="列表（list）"><a href="#列表（list）" class="headerlink" title="列表（list）"></a>列表（list）</h2><p>列表的形式类似于C语言中的数组。</p><pre class=" language-lang-python"><code class="language-lang-python"># 创建列表fruits = ['apple', 'peach', 'pear', 'strawberry']# 打印列表第一个元素print(fruits[0])# 打印列表最后一个元素print(fruits[-1])# 遍历列表for fruit in fruits:    print(fruit)</code></pre><h3 id="列表常见操作"><a href="#列表常见操作" class="headerlink" title="列表常见操作"></a>列表常见操作</h3><pre class=" language-lang-python"><code class="language-lang-python"># 向列表中添加元素fruits.append('banana')fruits.insert(1, 'grapes')# 从列表中删除元素fruits.pop()fruits.remove(’pear‘)# 列表拷贝fruits.copy()copy_of_fruits = fruits[:]# 创建数值列表square = []for x in range(1,11):    square.append(x**2)# 列表推导square = [x**2 for x in range(1,11)]# 列表切片first_two = fruits[:2]# 逆置列表fruits.reverse()# 列表排序fruits.sort()# 列表元素计数fruits.count('apple')# 清空列表fruits.clear()</code></pre><h2 id="元组（tuple）"><a href="#元组（tuple）" class="headerlink" title="元组（tuple）"></a>元组（tuple）</h2><p>元组的形式类似于列表，区别在于元组一旦定义了便不可以修改。</p><h3 id="元组常见操作"><a href="#元组常见操作" class="headerlink" title="元组常见操作"></a>元组常见操作</h3><pre class=" language-lang-python"><code class="language-lang-python"># 创建元组dims = (800, 600)print(dims)# 遍历元组for dim in dims:    print(dim)# 元素计数print(dims.count(800))</code></pre><h2 id="集合（set）"><a href="#集合（set）" class="headerlink" title="集合（set）"></a>集合（set）</h2><p>集合是一个无序的不重复元素序列。</p><h3 id="集合常用操作"><a href="#集合常用操作" class="headerlink" title="集合常用操作"></a>集合常用操作</h3><pre class=" language-lang-python"><code class="language-lang-python"># 创建集合fruits = set()# 创建空集fruits_a = {'apple', 'orange', 'apple', 'pear', 'orange', 'banana'}fruits_b = {'apple', 'peach', 'pear', 'strawberry'}# 向集合添加元素fruits_b.add('watermelon')# 从集合删除元素fruits_a.remove('apple')fruits_a.discard('apple')fruits_a.pop()# 拷贝一个集合fruits_a.copy()# 返回集合的差集fruits_a.difference(fruits_b)# 移除集合中的元素，该元素在指定的集合也存在fruits_a.difference_update(fruits_b)# 返回集合的交集fruits_a.intersection(fruits_b)# 判断两个集合是否包含相同的元素，如果没有返回 True，否则返回 Falsefruits_a.isdisjoint(fruits_b)# 判断指定集合是否为该方法参数集合的子集fruits_a.issubset(fruits_b)# 返回两个集合中不重复的元素集合fruits_a.symmetric_difference(fruits_b)# 移除当前集合中在另外一个指定集合相同的元素，并将另外一个指定集合中不同的元素插入到当前集合中fruits_a.symmetric_difference_update(fruits_b)</code></pre><h2 id="字典（dict）"><a href="#字典（dict）" class="headerlink" title="字典（dict）"></a>字典（dict）</h2><p>字典是另一种可变容器模型，且可存储任意类型对象，形式通常为<code>key: value</code>这样的键值对</p><h3 id="字典的特性"><a href="#字典的特性" class="headerlink" title="字典的特性"></a>字典的特性</h3><p>字典值可以是任何的 python 对象，既可以是标准的对象，也可以是用户定义的，但键不行。</p><ul><li>不允许同一个键出现两次。创建时如果同一个键被赋值两次，后一个值会覆盖前一个值</li><li>键必须不可变，所以可以用数字，字符串或元组充当，而用列表就不行。</li></ul><h3 id="字典常用操作"><a href="#字典常用操作" class="headerlink" title="字典常用操作"></a>字典常用操作</h3><pre class=" language-lang-python"><code class="language-lang-python"># 创建字典dict1 = {'abc': 456}dict2 = {'abc': 123, 98.6: 37}# 访问字典值dict1['abc']# 修改字典值dict1['abc'] = 789# 删除字典元素del dict2['abc']dict2.pop('abc')dict2.popitem()# 随机返回并删除字典中的最后一对键和值# 清空字典dict1.clear()# 返回字典浅拷贝dict1.copy()# 以列表返回可遍历的 (键，值) 元组数组dict1.items()# 返回所有的键序列，可以使用 list () 来转换为列表dict1.keys()# 返回所有的值序列，可以使用 list () 来转换为列表dict1.values()</code></pre><h2 id="条件表达式"><a href="#条件表达式" class="headerlink" title="条件表达式"></a>条件表达式</h2><pre class=" language-lang-python"><code class="language-lang-python"># 简单的if表达式a = 10b = 15if a == 10:    print('a equal 10')# if-else表达式if a == 10:    print('a equal 10')else:    print('a not equal 10')# if-elif-else表达式if a == 10:    print('a equal 10')elif b == 15 :    print('a  equal 10 and b equal 15')else:    print('a not equal 10,b not equal 15')</code></pre><h2 id="循环表达式"><a href="#循环表达式" class="headerlink" title="循环表达式"></a>循环表达式</h2><h3 id="For循环"><a href="#For循环" class="headerlink" title="For循环"></a>For循环</h3><pre class=" language-lang-python"><code class="language-lang-python">fruits = ['apple', 'peach', 'pear', 'strawberry']for fruit in fruits:    print(fruit)</code></pre><h3 id="while-循环"><a href="#while-循环" class="headerlink" title="while 循环"></a>while 循环</h3><pre class=" language-lang-python"><code class="language-lang-python">banned_users = ['eve', 'fred', 'gary', 'helen']prompt = "\nAdd a player to your team."prompt += "\nEnter 'quit' when you're done. "players = []while True:    player = input(prompt)    if player == 'quit':        break    elif player in banned_users:        print(player + " is banned!")        continue    else:        players.append(player)</code></pre><h2 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h2><p><img src="https://uptolimit.top/img/py-tup-10-26-1.png" alt="img" style="zoom:80%;"></p><pre class=" language-lang-python"><code class="language-lang-python">def describe_pet(animal, name):    """Display information about a pet."""    print("\nI have a " + animal + ".")    print("Its name is " + name + ".")describe_pet('hamster', 'harry')describe_pet('dog', 'willie')</code></pre><h3 id="参数类型"><a href="#参数类型" class="headerlink" title="参数类型"></a>参数类型</h3><ul><li><p>必须参数</p><p>默认情况下，如果函数指定了参数（非默认参数），那么调用的时候就必须传参。</p><pre class=" language-lang-python"><code class="language-lang-python">def printme( str ):    " 打印任何传入的字符串 "    print (str)    return# 调用 printme 函数，不加参数会报错printme()</code></pre></li><li><p>关键字参数</p><p>关键字参数和函数调用关系紧密，函数调用使用关键字参数来确定传入的参数值。使用关键字参数允许函数调用时参数的顺序与声明时不一致，因为 Python 解释器能够用参数名匹配参数值。</p><pre class=" language-lang-python"><code class="language-lang-python">#可写函数说明def printinfo( name, age ):    " 打印任何传入的字符串 "    print (" 名字: ", name)    print (" 年龄: ", age)    return#调用 printinfo 函数printinfo( age=50, name="xiaoming" )</code></pre></li><li><p>默认参数</p><p>调用函数时，如果没有传递参数，则会使用默认参数。</p><pre class=" language-lang-python"><code class="language-lang-python">#可写函数说明def printinfo( name, age = 35 ):    " 打印任何传入的字符串 "    print (" 名字: ", name)    print (" 年龄: ", age)    return#调用 printinfo 函数printinfo( age=50, name="xiaoming" )print ("------------------------")printinfo( name="xiaoming" )</code></pre></li><li><p>不定长参数</p><p>一个函数能处理比当初声明时更多的参数。这些参数叫做不定长参数，和上述 2 种参数不同，声明时不会命名。加了星号 <strong>*</strong> 的参数会以元组 (tuple) 的形式导入，存放所有未命名的变量参数。</p><pre class=" language-lang-python"><code class="language-lang-python"># 可写函数说明def printinfo( arg1, *vartuple ):    " 打印任何传入的参数 "    print (" 输出: ")    print (arg1)    for var in vartuple:        print (var)        return# 调用 printinfo 函数printinfo( 10 )printinfo( 70, 60, 50 )</code></pre></li></ul><h3 id="匿名函数"><a href="#匿名函数" class="headerlink" title="匿名函数"></a>匿名函数</h3><p>使用 lambda 来创建匿名函数。</p><ul><li><p>lambda 只是一个表达式，函数体比 def 简单很多。</p></li><li><p>lambda 的主体是一个表达式，而不是一个代码块。仅仅能在 lambda 表达式中封装有限的逻辑进去。</p></li><li><p>lambda 函数拥有自己的命名空间，且不能访问自己参数列表之外或全局命名空间里的参数。</p></li><li><p>虽然 lambda 函数看起来只能写一行，却不等同于 C 或 C++ 的内联函数，后者的目的是调用小函数时不占用栈内存从而增加运行效率</p><pre class=" language-lang-python"><code class="language-lang-python"># 可写函数说明sum = lambda arg1, arg2: arg1 + arg2# 调用 sum 函数print (" 相加后的值为 : ", sum( 10, 20 ))print (" 相加后的值为 : ", sum( 20, 20 ))</code></pre></li></ul><h2 id="类"><a href="#类" class="headerlink" title="类"></a>类</h2><pre class=" language-lang-python"><code class="language-lang-python">class MyClass:    """ 一个简单的类实例 """    i = 12345    def f(self):        return 'hello world'    # 实例化类x = MyClass()# 访问类的属性和方法print("MyClass 类的属性 i 为：", x.i)print("MyClass 类的方法 f 输出为：", x.f())</code></pre>]]></content>
      
      
      <categories>
          
          <category> 重学Python语言 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程语言 </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL基础知识</title>
      <link href="2021/05/10/mysql-ji-chu-zhi-shi/"/>
      <url>2021/05/10/mysql-ji-chu-zhi-shi/</url>
      
        <content type="html"><![CDATA[<h2 id="数据库介绍"><a href="#数据库介绍" class="headerlink" title="数据库介绍"></a>数据库介绍</h2><h3 id="数据库管理系统"><a href="#数据库管理系统" class="headerlink" title="数据库管理系统"></a>数据库管理系统</h3><p>数据库管理系统（DataBase Management System，DBMS ）：是指一种操作数据库、管理数据库的大型软件。主要<strong>用于数据库的的建立、使用和维护</strong>。</p><h3 id="数据库"><a href="#数据库" class="headerlink" title="数据库"></a>数据库</h3><p>数据库（database）：就是 存储数据的容器，其本质是一个文件系统，按照特定的格式将数据存储起来。用户可以对数据库中的数据进行增加，修改，删除及查询操作  。</p><p><img src="https://uptolimit.top/img/image-20210510214437890.png" alt="img" style="zoom:80%;"></p><h3 id="表"><a href="#表" class="headerlink" title="表"></a>表</h3><p>表（table）：是一种结构化的文件，可用来存储某种特定类型的数据。</p><p><img src="https://uptolimit.top/img/image-20210511001228204.png" alt="img" style="zoom:80%;"></p><p>表的特性决定了表中的数据如何存储，数据如何拆分等。</p><p>表名：数据库中的每个表都有一个名字，用来标识自己。在同一个数据库中，表名是唯一的。但在不同的数据库中却可以使用相同的表名。<br>模式（schema）：描述表的信息就是所谓的模式，模式可以用来描述数据库和表的布局及特性的信息。有时，<strong>模式</strong>和<strong>数据库</strong>是同义词。</p><h3 id="列"><a href="#列" class="headerlink" title="列"></a>列</h3><p>列（column）：是表中的一个字段。例如上图的<code>USER</code>、<code>HOST</code>、<code>ATTRIBUTE</code>。所有表都是由一个或多个列组成的。数据库中每个列都有相应的数据类型。数据类型规定了数据的格式，例如整型，浮点型，时间类型等。</p><h3 id="行"><a href="#行" class="headerlink" title="行"></a>行</h3><p>行（row）：是指表中的一条记录。表中的数据是按行存储的，所保存的每个记录存储在自己的行内  </p><h3 id="主键"><a href="#主键" class="headerlink" title="主键"></a>主键</h3><p>主键（primary key）：一列（或多列），能够唯一区分表中每个行。其作用类似我们身份证号，车牌号等可以唯一区分人，车的数据。 </p><h2 id="SQL语言"><a href="#SQL语言" class="headerlink" title="SQL语言"></a>SQL语言</h2><p>SQL（发音为字母S-Q-L或sequel）是结构化查询语言（Structured Query Language）的缩写。SQL是一种专门用来与数据库通信的语言。</p><h3 id="SQL的特点"><a href="#SQL的特点" class="headerlink" title="SQL的特点"></a>SQL的特点</h3><ul><li>SQL不是某个特定数据库供应商专有的语言。几乎所有重要的DBMS都支持SQL，所以，学习此语言使你几乎能与所有数据库打交道。</li><li>SQL简单易学。它的语句全都是由描述性很强的英语单词组成，而且这些单词的数目不多。</li><li>SQL尽管看上去很简单，但它实际上是一种强有力的语言，灵活使用其语言元素，可以进行非常复杂和高级的数据库操作。</li></ul><h3 id="SQL的分类"><a href="#SQL的分类" class="headerlink" title="SQL的分类"></a>SQL的分类</h3><ul><li><p>数据定义语言：简称DDL(Data Definition Language)，用来定义数据库对象：数据库，表，列等。关键字：create，alter，drop等</p></li><li><p>数据控制语言：简称DCL(Data Control Language)，用来定义数据库的访问权限和安全级别，及创建用户。</p></li><li>数据操作语言：简称DML(Data Manipulation Language)，用来对数据库中表的记录进行更新。关键字：insert，delete，update等</li><li>数据查询语言：简称DQL(Data Query Language)，用来查询数据库中表的记录。关键字：select，from，where等  </li></ul>]]></content>
      
      
      <categories>
          
          <category> 重学数据库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SQL </tag>
            
            <tag> MySQL </tag>
            
            <tag> 数据库 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>程序流程控制方法</title>
      <link href="2021/05/09/cheng-xu-liu-cheng-kong-zhi-fang-fa/"/>
      <url>2021/05/09/cheng-xu-liu-cheng-kong-zhi-fang-fa/</url>
      
        <content type="html"><![CDATA[<h2 id="C语言关系运算符"><a href="#C语言关系运算符" class="headerlink" title="C语言关系运算符"></a>C语言关系运算符</h2><div class="table-container"><table><thead><tr><th style="text-align:center">运算符</th><th style="text-align:center">说明</th><th style="text-align:center">例子</th></tr></thead><tbody><tr><td style="text-align:center"><code>==</code></td><td style="text-align:center">等于</td><td style="text-align:center"><code>a == b</code></td></tr><tr><td style="text-align:center"><code>！=</code></td><td style="text-align:center">不等于</td><td style="text-align:center"><code>a != b</code></td></tr><tr><td style="text-align:center"><code>&lt;、&gt;</code></td><td style="text-align:center">小于和大于</td><td style="text-align:center"><code>a &gt; b、a &lt; b</code></td></tr><tr><td style="text-align:center"><code>&lt;=(=&lt;)、=&gt;(&gt;=)</code></td><td style="text-align:center">小于等于和大于等于</td><td style="text-align:center"><code>a &gt;= b、a &lt;= b</code></td></tr><tr><td style="text-align:center"><code>!</code></td><td style="text-align:center">非（不等于<code>~</code>）</td><td style="text-align:center"><code>!(0)、!(NULL)</code></td></tr></tbody></table></div><p><em>Tips</em>：<code>!!(x)</code>逻辑归一化：不管真值为何，都归结为整型值1；不管假值为何，都归结为整型值0。</p><p><em>Tips</em>：C99后，C语言开始支持<code>bool</code>类型，需要引入<code>&lt;bool.h&gt;</code>头文件。</p><h2 id="if-语句"><a href="#if-语句" class="headerlink" title="if 语句"></a><code>if</code> 语句</h2><h3 id="三种形式"><a href="#三种形式" class="headerlink" title="三种形式"></a>三种形式</h3><pre class=" language-lang-c"><code class="language-lang-c">if (表达式) {    代码块;}</code></pre><pre class=" language-lang-c"><code class="language-lang-c">if (表达式) {    代码块1;} else {    代码块2;}</code></pre><pre class=" language-lang-c"><code class="language-lang-c">if (表达式1) {    代码块1;} else if (表达式2) {    代码块2;} else {    代码块3;}</code></pre>]]></content>
      
      
      <categories>
          
          <category> 重学C语言 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C语言 </tag>
            
            <tag> 编程语言 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C语言中的数学运算</title>
      <link href="2021/03/30/c-yu-yan-zhong-de-shu-xue-yun-suan/"/>
      <url>2021/03/30/c-yu-yan-zhong-de-shu-xue-yun-suan/</url>
      
        <content type="html"><![CDATA[<h2 id="C语言基本运算符"><a href="#C语言基本运算符" class="headerlink" title="C语言基本运算符"></a>C语言基本运算符</h2><div class="table-container"><table><thead><tr><th style="text-align:center">运算符</th><th style="text-align:center">说明</th><th style="text-align:center">例子</th></tr></thead><tbody><tr><td style="text-align:center">=</td><td style="text-align:center">赋值运算符</td><td style="text-align:center"><code>a=b;</code></td></tr><tr><td style="text-align:center">+、-、*、/、()</td><td style="text-align:center">基本四则运算</td><td style="text-align:center"><code>a=(b+c)*d;</code></td></tr><tr><td style="text-align:center">%</td><td style="text-align:center">求余运算符</td><td style="text-align:center"><code>a=b%2;</code></td></tr><tr><td style="text-align:center">&amp;、|、^、~</td><td style="text-align:center">位运算符（非常重要的一类）</td><td style="text-align:center"><code>a=~b|c;</code></td></tr><tr><td style="text-align:center">&lt;&lt;、&gt;&gt;</td><td style="text-align:center">左移和右移</td><td style="text-align:center"><code>a=b&gt;&gt;2;</code></td></tr></tbody></table></div><h2 id="运算符优先级"><a href="#运算符优先级" class="headerlink" title="运算符优先级"></a>运算符优先级</h2><p><img src="https://cdn.jsdelivr.net/gh/UPTOLIMIT/PicBed//img/20160309140917049" alt="img"></p><h2 id="位运算"><a href="#位运算" class="headerlink" title="位运算"></a>位运算</h2><ul><li><p><code>&amp;</code>运算</p><p>与运算</p></li><li><p><code>|</code>运算</p><p>或运算</p></li><li><p><code>^</code>运算</p><p>异或运算，对应二进制位上相同为0，不同为1，是一类<strong>逆运算</strong>，例如减法是加法的逆运算，即<code>a+b=c</code>可以推出<code>c-b=a</code>和<code>c-a=b</code>，加法不是减法的逆运算，因为<code>a-b=c</code>不能推出<code>c+a=b</code>。同样在异或运算中互为逆运算，即<code>a^b=c</code>可以得到<code>c^b=a</code>与<code>c^a=b</code>，例如<code>2^3=1</code>，<code>1^3=2</code>且<code>1^2=3</code>。</p><p>同时异或运算还有如下性质：<code>a^a=0</code>，<code>0^N+=N+</code>。</p><p><strong>思考</strong>：我们有若干个整数值，其中任意数字均出现了2次，其中只有一个数字出现了1次，问只出现1次的值为多少？</p></li><li><p><code>~</code>运算</p><p>在64bit操作系统中，C语言一个int型变量占用4个字节=32bit，也即整型变量3在PC中的存储格式为<code>0000 0000 0000 0000 0000 0000 0000 0011</code>所以<code>~3=1111 1111 1111 1111 1111 1111 1111 1100</code></p></li><li><p><code>&lt;&lt;</code>运算</p><p>左移运算，在二进制下运算，例如刚才的<code>3=0000 0000 0000 0000 0000 0000 0000 0011</code>，<code>3&lt;&lt;1即0000 0000 0000 0000 0000 0000 0000 0110=6</code>即左移一位相当于扩大2倍</p></li><li><p><code>&gt;&gt;</code>运算</p><p>右移运算，在二进制下运算，例如刚才的<code>3=0000 0000 0000 0000 0000 0000 0000 0011</code>，<code>3&gt;&gt;1即0000 0000 0000 0000 0000 0000 0000 0001</code>，即右移一位相当于除2，向下取整，右移一位补符号位</p></li></ul><h2 id="C语言中的数学函数库"><a href="#C语言中的数学函数库" class="headerlink" title="C语言中的数学函数库"></a>C语言中的数学函数库</h2><h3 id="头文件-Math-h"><a href="#头文件-Math-h" class="headerlink" title="头文件 Math.h"></a>头文件 Math.h</h3><ul><li><code>pow(a,n)</code></li><li><code>sqrt(n)</code></li><li><code>ceil(n)</code>：向上取整</li><li><code>floor(n)</code>：向下取整</li><li><code>fabs(n)</code>：实数绝对值函数</li><li><code>log(n)</code></li><li><code>log10(n)</code></li><li><code>acos(n)</code></li></ul><h3 id="头文件stdlib-h"><a href="#头文件stdlib-h" class="headerlink" title="头文件stdlib.h"></a>头文件stdlib.h</h3><ul><li><code>abs(n )</code></li></ul><h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><h3 id="循环读入的另一种实现方式"><a href="#循环读入的另一种实现方式" class="headerlink" title="循环读入的另一种实现方式"></a>循环读入的另一种实现方式</h3><p>在之前的学习过程中，我们循环读入的方式如下：</p><pre class=" language-lang-c"><code class="language-lang-c">while (scanf("%d",&n) != EOF){}</code></pre><pre class=" language-lang-c"><code class="language-lang-c">while (scanf("%d",&n) != -1){}</code></pre><p>在这里我们引入取反运算符，实现循环读入：</p><pre class=" language-lang-c"><code class="language-lang-c">while (~scanf("%d",&n)){}</code></pre><p>因为<code>scanf()</code>返回值为负的时候绝对不合法，比如<code>scanf()=-1</code>时，输入是不合法的，所以，如上我们可以把判断条件写成<code>scanf() != -1</code>或者<code>scanf() != EOF</code>，那么<code>-1</code>在计算机中是如何表示的呢？</p><h2 id="负数的表示"><a href="#负数的表示" class="headerlink" title="负数的表示"></a>负数的表示</h2><h3 id="二进制补码表示法"><a href="#二进制补码表示法" class="headerlink" title="二进制补码表示法"></a>二进制补码表示法</h3><p><code>补码 = 反码 + 1</code></p><h3 id="二进制反码表示法"><a href="#二进制反码表示法" class="headerlink" title="二进制反码表示法"></a>二进制反码表示法</h3><p><code>反码=~原码</code></p><p>所以，<code>补码 = ~原码 + 1</code>，<code>-1 =1111 1111 1111 1111 1111 1111 1111 1110 + 1 = 1111 1111 1111 1111 1111 1111 1111 1111</code>，</p><h3 id="另外一种解释"><a href="#另外一种解释" class="headerlink" title="另外一种解释"></a>另外一种解释</h3><p><code>-1 = 0 - 1</code>即 <code>0000 0000 0000 0000 0000 0000 0000 0000 - 1 = 1111 1111 1111 1111 1111 1111 1111 1111</code></p><h2 id="代码演示"><a href="#代码演示" class="headerlink" title="代码演示"></a>代码演示</h2><pre class=" language-lang-C"><code class="language-lang-C">#include<stdio.h>#define swap(a, b) {\    __typeof(a) __temp = a;\    a = b; b = __temp;\}int main(){    int a, b;    scanf("%d%d", &a, &b);    printf("a = %d, b = %d\n", a, b);    //swap(a, b);    a ^= b;    b ^= a;    a ^= b;    printf("swap : a = %d, b = %d\n", a, b);    return 0;}</code></pre><p>这里注意交换变量值的两种方式。另外在基本运算中，位运算是最快的，取模运算是最慢的，<code>n%2</code>等价于<code>n&amp;1</code>因此如果我们在编码过程中，用位运算代替上述的取模运算，能够大大加快运算速度。</p>]]></content>
      
      
      <categories>
          
          <category> 重学C语言 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C语言 </tag>
            
            <tag> 编程语言 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C语言基础</title>
      <link href="2021/03/30/c-yu-yan-ji-chu/"/>
      <url>2021/03/30/c-yu-yan-ji-chu/</url>
      
        <content type="html"><![CDATA[<hr><h2 id="汇编与C语言"><a href="#汇编与C语言" class="headerlink" title="汇编与C语言"></a>汇编与C语言</h2><ul><li>开发效率</li><li>程序可移植性</li><li>运行效率</li></ul><h2 id="C语言的优势"><a href="#C语言的优势" class="headerlink" title="C语言的优势"></a>C语言的优势</h2><ul><li>高级语言</li><li>代码量变少</li><li>程序的可移植性<ul><li>不同系统平台皆可编译运行</li></ul></li></ul><h2 id="汇编语言的优势"><a href="#汇编语言的优势" class="headerlink" title="汇编语言的优势"></a>汇编语言的优势</h2><ul><li>与计算机硬件强相关，或者可以认为汇编语言直接与硬件交互</li></ul><h2 id="输入输出函数说明"><a href="#输入输出函数说明" class="headerlink" title="输入输出函数说明"></a>输入输出函数说明</h2><h3 id="printf-函数"><a href="#printf-函数" class="headerlink" title="printf 函数"></a><code>printf</code> 函数</h3><ul><li>头文件：<code>stdio.h</code></li><li><p>函数原型：<code>int printf(const char *format, ...)</code></p></li><li><p><code>format</code>：格式控制字符串</p></li><li><p><code>...</code>：可变参数列表，在使用<code>printf</code>函数的时候，可以传入任意多个参数。</p></li><li>返回值：输出字符的数量</li></ul><h3 id="scanf函数"><a href="#scanf函数" class="headerlink" title="scanf函数"></a><code>scanf</code>函数</h3><ul><li>头文件：<code>stdio.h</code></li><li>函数原型：<code>int scanf(const char *format, ...)</code></li><li><code>format</code>：格式控制字符串</li><li><code>...</code>：可变参数列表</li><li>返回值：成功读入的参数个数（返回值为0合法）</li></ul><h4 id="循环读入命题"><a href="#循环读入命题" class="headerlink" title="循环读入命题"></a>循环读入命题</h4><pre class=" language-lang-c"><code class="language-lang-c">while(scanf()!=EOF){//或者while(scanf()!=-1)}</code></pre><p>==思考==</p><p>程序如何知道什么时候停止读取呢？</p><p>==Tips：==</p><p><code>EOF</code>：end of file（在linux操作系统之中，一切皆文件），<code>EOF</code>可以看作一类隐藏的文件描述符，当文件指针碰到<code>EOF</code>是表示读到文件末尾，<code>EOF</code>对应整数值<code>-1</code></p><h2 id="sprintf函数"><a href="#sprintf函数" class="headerlink" title="sprintf函数"></a><code>sprintf</code>函数</h2><p>将输出的内容输出到字符数组里。经常用于字符串的拼接操作。</p><h2 id="sscanf函数"><a href="#sscanf函数" class="headerlink" title="sscanf函数"></a><code>sscanf</code>函数</h2><p>将输入的内容输出到字符数组中。经常用于字符串的分割操作。</p><pre class=" language-lang-C"><code class="language-lang-C">#include<stdio.h>int main(){    int n;    char str[100] = {0};    int arr[4];    scanf("%d", &n);    printf("%d\n", n);//打印12345    sprintf(str, "%d.%d.%d.%d", 192, 168, 0, 1);//字符串拼接    printf("%s\n", str);//打印str,内容是192.168.0.1    sscanf(str,"%d.%d.%d.%d", &arr[0], &arr[1], &arr[2], &arr[3]);//字符串分割    for( int i = 0; i < 4; i++ ){        printf("%d\n", arr[i]);    }    FILE *fp = fopen("./output", "a+");    fprintf(fp, "str=%s\n", str);    fclose(fp);    char temp[100][100] = {0};    int ret = 0, k = 0;    fp = fopen("./output", "r+");    while(( ret = fscanf(fp, "%[^\n]s", temp[k++])) != EOF ){        printf("%s ret = %d\n", temp[k-1], ret);        fgetc();//在文件流中使用fgetc()来读入换行符    }    fclose(fp);    for(int i=0; i < k; i++){        printf( "%s\n", temp[i] );    }    return 0;}</code></pre><h2 id="练习"><a href="#练习" class="headerlink" title="练习"></a>练习</h2><ol><li><p>请使用<code>printf</code>函数，求解一个数字n的十进制表示的数字位数</p><pre class=" language-lang-c"><code class="language-lang-c">#include<stdio.h>int main(){    int n;    scanf("%d", &n);    printf(" has %d digits.\n", printf("%d", n));    return 0;}</code></pre></li><li><p>为题1添加一个循环读入</p><pre class=" language-lang-C"><code class="language-lang-C">#include<stdio.h>int main(){    int n;    while(scanf("%d", &n) != EOF){        printf(" has %d digits.\n", printf("%d", n));    }    return 0;}</code></pre></li><li><p>请写一个程序，读入一个行字符串的（可能包含空格），输出这个字符串中字符的数量。</p><pre class=" language-lang-c"><code class="language-lang-c">#include<stdio.h>int main(){    char str[100] = {0};    while(scanf("%[^\n]s", str) != EOF){        getchar();        printf(" has %d chars!\n", printf("%s", str));    }     return 0;}</code></pre><p>==Tips：==</p><p><code>gets()</code>方法在开发时被认为是一种很危险的方法，是不允许使用的，<code>%c</code>会读入所有字符，使用时注意误读入的情况发生。</p><p><code>%[^\n]s</code>：这里使用了字符匹配集，<code>[]</code>中为正则表达式，指定输入规则。</p></li></ol><h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><ol><li><p>如何停止循环读入？</p><p><code>ctrl+z</code>相当于是给<code>scanf</code>一个<code>EOF</code></p><p><code>ctrl+c</code>结束当前进程，从操作系统层面去强制结束当前进程</p></li><li><p>区别<code>printf</code>家族函数用法，尝试阅读man手册</p><pre class=" language-lang-c"><code class="language-lang-c">#include <stdio.h>int printf(const char *format, ...);int fprintf(FILE *stream, const char *format, ...);int dprintf(int fd, const char *format, ...);int sprintf(char *str, const char *format, ...);int snprintf(char *str, size_t size, const char *format, ...);</code></pre><pre class=" language-lang-c"><code class="language-lang-c">#include <stdarg.h>int vprintf(const char *format, va_list ap);int vfprintf(FILE *stream, const char *format, va_list ap);int vdprintf(int fd, const char *format, va_list ap);int vsprintf(char *str, const char *format, va_list ap);int vsnprintf(char *str, size_t size, const char *format, va_list ap);</code></pre></li></ol><ol><li><p>区别<code>%c</code>与<code>%s</code>的用法</p><p><code>%c</code>能够接收所有的字符，<code>%s</code>接收除空格、<code>\n</code>、<code>\t</code>以外的所有字符，同时空格、<code>\n</code>、<code>\t</code>会被当做分隔符处理。</p></li><li><p>对于练习3如果没有<code>getchar()</code>会发生什么问题，为什么？</p><p><code>scanf()</code>中的正则表达式会使<code>\n</code>卡在缓冲区的出口处，被输入规则排除在外，因此<code>scanf()</code>的返回值为0，因此程序会陷入死循环。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 重学C语言 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C语言 </tag>
            
            <tag> 编程语言 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DeepFM理论与实践</title>
      <link href="2021/03/21/deepfm-li-lun-yu-shi-jian/"/>
      <url>2021/03/21/deepfm-li-lun-yu-shi-jian/</url>
      
        <content type="html"><![CDATA[<h2 id="理论背景"><a href="#理论背景" class="headerlink" title="理论背景"></a>理论背景</h2><p>DeepFM模型是2017年哈工大深圳与华为诺亚方舟联合实验室提出的，论文名称《<a href="https://arxiv.org/abs/1703.04247">DeepFM: A Factorization-Machine based Neural Network for CTR Prediction</a>》，DFM模型是在W&amp;D模型上的改进，W&amp;D模型理论参照上一篇<a href="https://zhuanlan.zhihu.com/p/358174532">笔记</a>，在W&amp;D的基础上，将Wide部分替换为FM模型，不再需要人工特征工程，同时巧妙地在FM的二阶部分和神经网络的Embedding层共享权重，减少了大量参数，极大的提高了训练速度。</p><p>在CTR预估任务中，业界常用的方法有人工特征工程+逻辑回归、梯度增强决策树（GBDT）+逻辑回归，FM（Factorization Machine）和FFM（Field-aware Factorization Machine）模型，在这些模型中FM、FFM模型表现突出。</p><h2 id="FM模型"><a href="#FM模型" class="headerlink" title="FM模型"></a>FM模型</h2><p>在学习DFM之前，先简单的学习一下FM模型和FFM模型，FM模型是由Konstanz 大学 Steffen Rendle于2010年提出，旨在解决稀疏数据下的特征组合问题。下面我们用一个<a href="http://www.cs.cmu.edu/~wcohen/10-605/2015-guest-lecture/FM.pdf">例子</a>引入FM模型，假设我们要处理一个广告问题，根据用户和广告类型特征，预测用户是否点击了广告。</p><div class="table-container"><table><thead><tr><th>Clicked？</th><th>Country</th><th>Day</th><th>Ad_type</th></tr></thead><tbody><tr><td>1</td><td>USA</td><td>26/11/15</td><td>Movie</td></tr><tr><td>0</td><td>China</td><td>1/7/14</td><td>Game</td></tr><tr><td>1</td><td>China</td><td>19/2/15</td><td>Game</td></tr></tbody></table></div><p>其中<strong>Clicked？</strong>是标签label，<strong>Country</strong>、<strong>Day</strong>、<strong>Ad_type</strong>是特征。由于三种特征都是类别类型，我们需要通过One-Hot编码将他们转换成数值型特征。</p><div class="table-container"><table><thead><tr><th style="text-align:left"><strong>Clicked?</strong></th><th style="text-align:left">Country=USA</th><th style="text-align:left">Country=China</th><th style="text-align:left">Day=26/11/15</th><th style="text-align:left">Day=1/7/14</th><th style="text-align:left">Day=19/2/15</th><th style="text-align:left">Ad_type=Movie</th><th style="text-align:left">Ad_type=Game</th></tr></thead><tbody><tr><td style="text-align:left"><strong>1</strong></td><td style="text-align:left">1</td><td style="text-align:left">0</td><td style="text-align:left">1</td><td style="text-align:left">0</td><td style="text-align:left">0</td><td style="text-align:left">1</td><td style="text-align:left">0</td></tr><tr><td style="text-align:left"><strong>0</strong></td><td style="text-align:left">0</td><td style="text-align:left">1</td><td style="text-align:left">0</td><td style="text-align:left">1</td><td style="text-align:left">0</td><td style="text-align:left">0</td><td style="text-align:left">1</td></tr><tr><td style="text-align:left"><strong>1</strong></td><td style="text-align:left">0</td><td style="text-align:left">1</td><td style="text-align:left">0</td><td style="text-align:left">0</td><td style="text-align:left">1</td><td style="text-align:left">0</td><td style="text-align:left">1</td></tr></tbody></table></div><p>经过One-Hot编码之后，产生的样本数据大部分都是比较稀疏的，每个样本具有7维特征，但平均仅有3维特征具有非零值。在真实场景中，这样的情况也是普遍存在的，例如我们扩展上述数据的特征，加入用户的性别、职业、教育水平，商品品类等，经过One-Hot编码转换后都会导致样本数据的稀疏性。特别是商品品类这种类型的特征，如商品的品类约有550个，采用One-Hot编码生成550个数值特征，但是每个样本的550个特征中，有且仅有一个是有效的（非零）。由此可见，数据的稀疏性，在真实场景中是不可避免地挑战。</p><p>另外，我们也可以发现，经过One-Hot编码之后，特征空间陡然增大。例如，商品品类有550维特征，且是一个类别特征引起的。</p><p>通过观察大量样本数据可以发现，某些特征经过关联之后，与label之间的相关性就会提高，例如，“USA”与“Thanksgiving”、“China”与“Chinese New Year”这样的关联特征。对用户的点击行为有着正向的影响。同样，“化妆品”类商品与“女性”，“球类运动”类商品与“男”性，可见这种关联特征也是普遍存在的，因此进行特征的组合是非常有意义的。</p><p>在众多模型中，多项式模型是包含特征组合的最直观的模型。在多项式模型中，特征$x_i$和$x_j$的组合我们采用$x_ix_j$表示，即$x_i$和$x_j$都非零时，组合特征$x_ix_j$才有意义，在这里我们只讨论二阶多项式模型。表达式如下：</p><script type="math/tex; mode=display">y(x)=w_0+\sum^{n}_{i=1}{w_ix_i}+\sum^{n}_{i=1}\sum^{n}_{j=i+1}{w_{ij}x_ix_j}</script><p>其中，$n$代表样本的特征数量，$x_i$是第$i$个特征的值，$w_0$、$w_i$、$w_{ij}$是模型参数。</p><p>在多项式模型中，组合特征的参数一共有${n(n-1)\over{2} }$个，且任意两个参数都是独立的。但是因为每个参数$w_{ij}$的训练都需要大量的$x_i$和$x_j$都非零的样本，在数据稀疏性普遍存在的实际场景中，这样的训练是非常困难的，参数$w_{ij}$的不准确将严重影响模型的性能。</p><p>那么，我们如何解决二次项参数的训练问题呢？我们借助矩阵分解的思想，将所有二次项参数$w_{ij}$组成一个对称矩阵$W$，这个矩阵可以分解为$W=V^TV$，$V$的第$j$列便是第$j$维特征的隐向量，即$w_{ij}=\langle{v_i,v_j}\rangle$，这就是FM模型的核心思想。因此二阶FM模型的方程可以表述为：</p><script type="math/tex; mode=display">y(x)=w_0+\sum^{n}_{i=1}{w_ix_i}+\sum^{n}_{i=1}\sum^{n}_{j=i+1}{\langle{v_i,v_j}\rangle}{x_ix_j}</script><p>其中，$v_i$是第$i$维特征的隐向量，${\langle{\cdot,\cdot}\rangle}$表示向量点积。</p><h2 id="FFM模型"><a href="#FFM模型" class="headerlink" title="FFM模型"></a>FFM模型</h2><p>FFM模型（Field-aware Factorization Machine）来源于Yu-Chin Juan与其比赛队员，他们借鉴了Michael Jahrer的<a href="https://kaggle2.blob.core.windows.net/competitions/kddcup2012/2748/media/Opera.pdf">论文</a>中的field概念，提出了FM的升级版模型。FFM把相同性质的特征归于同一个Field。以上面的广告点击案例为例，“Day=26/11/15”、“Day=1/7/14”、“Day=19/2/15” 这三个特征都是代表日期的，可以放到同一个 Field 中。同理，550个商品类别特征也可以放到一个Field中。即同一个类别特征经过One-Hot编码生成的数值特征都可以放到同一个Field中。在FFM中，每一维特征$x_i$，针对其它特征的每一个Field记为$f_j$，都会学习一个隐向量$v_{i,f_j}$。因此，隐向量不仅与特征相关，也与Field相关。也就是说，“Day=26/11/15” 这个特征与 “Country” 特征和 “Ad_type” 特征进行关联的时候使用不同的隐向量，这与 “Country” 和 “Ad_type” 的内在差异相符，也是 FFM 中 “field-aware” 的由来。</p><p>假设样本的 $n$个特征属于$f$个 field，那么 FFM 的二次项有 $nf$个隐向量。而在 FM 模型中，每一维特征的隐向量只有一个。<strong>FM 可以看作 FFM 的特例</strong>，是把所有特征都归属到一个 field 时的 FFM 模型。根据 FFM 的 field 敏感特性，可以导出其模型方程。如下：</p><script type="math/tex; mode=display">y(x)=w_0+\sum^{n}_{i=1}{w_ix_i}+\sum^{n}_{i=1}\sum^{n}_{j=i+1}{\langle{v_{i,f_j},v_{j,f_i}}\rangle}{x_ix_j}</script><h2 id="DFM模型"><a href="#DFM模型" class="headerlink" title="DFM模型"></a>DFM模型</h2><p>FM模型和FFM模型都在特征组合方面进行了探索，FM通过对每一维特征的隐变量内积来提取特征组合，虽然也能取得非常好的结果，但是由于计算复杂度的原因，往往只能局限在二阶特征组合进行建模。如果要使用FFM模型，所有的特征必须要转换成”field_id:feat_id:value”的格式，field_id表示特征所属field的编号，feat_id代表特征编号，value是特征的值。</p><p>对于高阶的特征组合来说，我们通常会想到通过DNN去解决，但是正如我们之前介绍的那样，对于类别型特征，我们使用One-Hot编码的方式处理，但是将One-Hot编码输入到DNN中，会使网络参数陡增。</p><p><img src="https://cdn.jsdelivr.net/gh/UPTOLIMIT/PicBed//img/image-20210322193403055.png" alt="image-20210322193403055" style="zoom:50%;"></p><p>为了解决参数量过大的问题，我们可以借助FFM的思想，将特征分为不同的Field，避免全连接，分而治之。</p><p><img src="https://cdn.jsdelivr.net/gh/UPTOLIMIT/PicBed//img/image-20210322193423532.png" alt="image-20210322193423532" style="zoom:50%;"></p><p>然后通过增加全连接层就可以实现高阶的特征组合，如下：</p><p><img src="https://cdn.jsdelivr.net/gh/UPTOLIMIT/PicBed//img/image-20210322193448474.png" alt="image-20210322193448474" style="zoom:50%;"></p><p>但是这里仍然缺少低阶的特征组合，我们使用FM把低阶特征组合单独建模。</p><p><img src="https://cdn.jsdelivr.net/gh/UPTOLIMIT/PicBed//img/image-20210322193551205.png" alt="image-20210322193551205" style="zoom:50%;"></p><p>然后把低阶特征组合模型插入到网络结构中</p><p><img src="https://cdn.jsdelivr.net/gh/UPTOLIMIT/PicBed//img/image-20210322193622982.png" alt="image-20210322193622982" style="zoom:50%;"></p><h2 id="网络融合的结构"><a href="#网络融合的结构" class="headerlink" title="网络融合的结构"></a>网络融合的结构</h2><h3 id="串行结构"><a href="#串行结构" class="headerlink" title="串行结构"></a>串行结构</h3><p><img src="https://cdn.jsdelivr.net/gh/UPTOLIMIT/PicBed//img/image-20210322193658105.png" alt="image-20210322193658105" style="zoom:50%;"></p><h3 id="并行结构"><a href="#并行结构" class="headerlink" title="并行结构"></a>并行结构</h3><p><img src="https://cdn.jsdelivr.net/gh/UPTOLIMIT/PicBed//img/image-20210322194746626.png" alt="image-20210322194746626" style="zoom: 50%;"></p><p><a href="https://uptolimit.github.io/2021/03/21/wide-deep-lun-wen-jie-xi-yu-dai-ma-shi-xian/">Wide&amp;Deep模型</a>就是典型的并行结构。</p><h2 id="DeepFM模型结构和原理"><a href="#DeepFM模型结构和原理" class="headerlink" title="DeepFM模型结构和原理"></a>DeepFM模型结构和原理</h2><p>我们先来看下DeepFM模型的结构：</p><p><img src="https://cdn.jsdelivr.net/gh/UPTOLIMIT/PicBed//img/image-20210322193940693.png" alt="image-20210322193940693" style="zoom: 80%;"></p><p>可以发现模型结构由FM与DNN两部分组成，FM负责低阶特征的组合提取，DNN负责高阶特征的组合提取，这两部分共享同一的输入，预测结果是对FM和DNN的结果一起做Embedding，可以如下表示：</p><script type="math/tex; mode=display">\hat{y}=sigmoid(y_{FM}+y_{DNN})</script><h3 id="FM-部分"><a href="#FM-部分" class="headerlink" title="FM 部分"></a>FM 部分</h3><p><img src="https://cdn.jsdelivr.net/gh/UPTOLIMIT/PicBed//img/image-20210322194036684.png" alt="image-20210322194036684" style="zoom:80%;"></p><script type="math/tex; mode=display">\hat{y}_{FM}(x)=w_0+\sum^{N}_{i=1}{w_ix_i}+\sum^N_{i=1}\sum^N_{j=i+1}v^T_iv_jx_ix_j</script><p>由上图和公式大致可以看出FM部分是由一阶特征和二阶特征Concatenate到一起再经过一个Sigmoid得到logits，所以在实现的时候要分别考虑线性相加部分和FM交叉特征部分。</p><h3 id="Deep部分"><a href="#Deep部分" class="headerlink" title="Deep部分"></a>Deep部分</h3><p><img src="https://cdn.jsdelivr.net/gh/UPTOLIMIT/PicBed//img/image-20210322194136996.png" alt="image-20210322194136996" style="zoom:80%;"></p><p>在第一层隐藏层之前，引入一个Embedding层将高维稀疏向量转为低维稠密向量，以解决参数爆炸问题。Embedding层的结构如下图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/UPTOLIMIT/PicBed//img/image-20210322194205481.png" alt="image-20210322194205481" style="zoom:80%;"></p><p>这里需要注意的是尽管Field的输入长度不同，但是Embedding之后向量的长度均为K。另外FM里面得到的隐变量$ V_{ik} $作为Embedding层网络的权重。Embedding层的输出是将所有id类特征对应embedding向量concat到一起输入到DNN中。其中$v_i$表示第$i$个field的embedding，$m$是field的数量。我们得到第一层的输入：</p><script type="math/tex; mode=display">z_1=[v_1,v_2,\cdots,v_m]</script><p>把上一层的输出作为下一层的输入，得到：</p><script type="math/tex; mode=display">z_L=\sigma(W_{L-1}z_{L-1}+b_{L-1})</script><p>其中$\sigma$是激活函数，$z$表示该层的输入，$W$表示该层的权重，$b$表示偏置。</p><p>DNN部分输出如下：</p><script type="math/tex; mode=display">\hat{y}_{DNN}=\sigma{(W^L\alpha^L+b^L)}</script><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>DeepFM模型大致由两部分组成，分别为FM和DNN，而FM部分又由一阶特征部分和二阶特征交叉部分组成，所以模型大概可以拆成三部分，分别为FM一阶特征Linear部分，二阶特征交叉部分和DNN的高阶特征交叉部分。</p><h2 id="两个思考题"><a href="#两个思考题" class="headerlink" title="两个思考题"></a>两个思考题</h2><ul><li><p>如果对于FM采用随机梯度下降SGD训练模型参数，请写出模型各个参数的梯度和FM参数训练的复杂度</p><p>FM的模型方程为：</p><script type="math/tex; mode=display">y(x)=w_0+\sum^{n}_{i=1}{w_ix_i}+\sum^{n}_{i=1}\sum^{n}_{j=i+1}{w_{ij}x_ix_j}</script><p>如果直接计算，复杂度为$O(\bar{n}^2)$,其中$\sum^{n}_{i=1}\sum^{n}_{j=i+1}{w_{ij}x_ix_j}$可化简为,复杂度为$O(\bar{n})$.</p><script type="math/tex; mode=display">\begin{equation}\begin{aligned}\sum^{n}_{i=1}\sum^{n}_{j=i+1}{w_{ij}x_ix_j}&=\sum^{n}_{i=1}\sum^{n}_{j=i+1}\sum_{k=1}^{n}{v_{ik}v_{jk} }x_ix_j\\&={1\over2}\sum_{k=1}^{n}((\sum_{i=1}^{n}v_{ik}x_i)^2-\sum_{i=1}^{n}{v_{ik}x_i^2})\end{aligned}\end{equation}</script></li></ul><p>  利用SGD（Stochastic Gradient Descent）训练模型，模型各个参数的梯度如下：</p><script type="math/tex; mode=display">  \frac{\partial }{ { \partial \theta }}y(x) = \left\{ {\begin{array}{*{20}{c} }  {1,if\ \theta\ is\ w_0}\\  {x_i,if\ \theta\ is\ w_i}\\  {x_i\sum}_{j=i+1}^{n}{v_{jk}x_j}-v_{ik}x_i^2,if\ \theta\ is\ w_{ik}  \end{array} } \right.</script><ul><li><p>对于下图所示，根据你的理解Sparse Feature中的不同颜色节点分别表示什么意思？</p><p><img src="https://cdn.jsdelivr.net/gh/UPTOLIMIT/PicBed//img/image-20210322194851954.png" alt="image-20210322194851954"></p><p>不同的颜色对应的是稀疏特征向量中不同维度不同的值。浅蓝色表示对应的类别特征的onehot向量，黄色的点表示有效值（1），浅蓝色的点表示无效值（0）。</p></li></ul><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>我们还是按照代码的执行顺序来学习DeepFM的实现逻辑，使用的数据集还是criteo。首先是main函数：</p><pre class=" language-lang-python"><code class="language-lang-python">if __name__ == "__main__":    # 读取数据    data = pd.read_csv('./data/criteo_sample.txt')    # 划分dense和sparse特征    columns = data.columns.values    dense_features = [feat for feat in columns if 'I' in feat]    sparse_features = [feat for feat in columns if 'C' in feat]    # 简单的数据预处理    train_data = data_process(data, dense_features, sparse_features)    train_data['label'] = data['label']    # 将特征分组，分成linear部分和dnn部分(根据实际场景进行选择)，并将分组之后的特征做标记（使用DenseFeat, SparseFeat）    linear_feature_columns = [SparseFeat(feat, vocabulary_size=data[feat].nunique(),embedding_dim=4)                            for i,feat in enumerate(sparse_features)] + [DenseFeat(feat, 1,)                            for feat in dense_features]    dnn_feature_columns = [SparseFeat(feat, vocabulary_size=data[feat].nunique(),embedding_dim=4)                            for i,feat in enumerate(sparse_features)] + [DenseFeat(feat, 1,)                            for feat in dense_features]    # 构建DeepFM模型    history = DeepFM(linear_feature_columns, dnn_feature_columns)    history.summary()    history.compile(optimizer="adam",                 loss="binary_crossentropy",                 metrics=["binary_crossentropy", tf.keras.metrics.AUC(name='auc')])    # 将输入数据转化成字典的形式输入    train_model_input = {name: data[name] for name in dense_features + sparse_features}    # 模型训练    history.fit(train_model_input, train_data['label'].values,            batch_size=64, epochs=5, validation_split=0.2, )</code></pre><p>数据预处理部分和特征分组部分在前两部分有所解释，这里不再赘述。FM部分实现如下：</p><pre class=" language-lang-python"><code class="language-lang-python">def DeepFM(linear_feature_columns, dnn_feature_columns):    # 构建输入层，即所有特征对应的Input()层，这里使用字典的形式返回，方便后续构建模型    dense_input_dict, sparse_input_dict = build_input_layers(linear_feature_columns + dnn_feature_columns)    # 将linear部分的特征中sparse特征筛选出来，后面用来做1维的embedding    linear_sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeat), linear_feature_columns))    # 构建模型的输入层，模型的输入层不能是字典的形式，应该将字典的形式转换成列表的形式    # 注意：这里实际的输入与Input()层的对应，是通过模型输入时候的字典数据的key与对应name的Input层    input_layers = list(dense_input_dict.values()) + list(sparse_input_dict.values())    # linear_logits由两部分组成，分别是dense特征的logits和sparse特征的logits    linear_logits = get_linear_logits(dense_input_dict, sparse_input_dict, linear_sparse_feature_columns)    # 构建维度为k的embedding层，这里使用字典的形式返回，方便后面搭建模型    # embedding层用户构建FM交叉部分和DNN的输入部分    embedding_layers = build_embedding_layers(dnn_feature_columns, sparse_input_dict, is_linear=False)    # 将输入到dnn中的所有sparse特征筛选出来    dnn_sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeat), dnn_feature_columns))    fm_logits = get_fm_logits(sparse_input_dict, dnn_sparse_feature_columns, embedding_layers) # 只考虑二阶项    # 将所有的Embedding都拼起来，一起输入到dnn中    dnn_logits = get_dnn_logits(sparse_input_dict, dnn_sparse_feature_columns, embedding_layers)    # 将linear,FM,dnn的logits相加作为最终的logits    output_logits = Add()([linear_logits, fm_logits, dnn_logits])    # 这里的激活函数使用sigmoid    output_layers = Activation("sigmoid")(output_logits)    model = Model(input_layers, output_layers)    return model</code></pre><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol><li><a href="https://tech.meituan.com/2016/03/03/deep-understanding-of-ffm-principles-and-practices.html">深入 FFM 原理与实践</a></li><li><a href="https://zhuanlan.zhihu.com/p/35815532">推荐系统遇上深度学习 (三)—DeepFM 模型理论和实践</a></li><li><a href="https://www.jiqizhixin.com/graph/technologies/f82b7976-b182-40fa-b7d8-a3aad9952937">深度神经网络</a></li><li><a href="https://zhuanlan.zhihu.com/p/57873613">深度推荐模型之DeepFM</a></li><li><a href="https://zhuanlan.zhihu.com/p/53361519">详解 Wide &amp; Deep 结构背后的动机</a></li><li><a href="https://blog.csdn.net/qq_32486393/article/details/103498519">FM算法公式推导</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 深度推荐系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 推荐系统 </tag>
            
            <tag> DNN </tag>
            
            <tag> 特征工程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Wide&amp;Deep论文解析与代码实现</title>
      <link href="2021/03/21/wide-deep-lun-wen-jie-xi-yu-dai-ma-shi-xian/"/>
      <url>2021/03/21/wide-deep-lun-wen-jie-xi-yu-dai-ma-shi-xian/</url>
      
        <content type="html"><![CDATA[<h2 id="论文背景"><a href="#论文背景" class="headerlink" title="论文背景"></a>论文背景</h2><p>《Wide &amp; Deep Learning for Recommender Systems 》这篇论文是Google于2016年发表在DLRS上的文章，该方法在Google Play的推荐业务中得到了成功的应用。</p><p>在推荐系统中，我们的主要挑战之一就是同时解决Memorization和Generalization，也就是推荐系统的记忆能力和泛化能力。Memorization通过一系列人工的特征叉乘（cross-product）来构造非线性特征，捕捉稀疏特征之间的高阶相关性，能够从历史数据中学习到高频共现的特征组合。例如在CTR预估任务中利用手工构造的交叉组合特征来使线性模型具有“记忆性”，这里的记忆性是指“记忆”历史数据中曾共同出现过的特征对。Generalization为稀疏特征学习低维的稠密嵌入来捕获其中的特征相关性，能够利用特征之间的传递性去探索历史数据中从未出现过的特征组合，学习到的embeddings本身具有一定的语义信息。 </p><p>以上的描述可能比较抽象，类比到我们的大脑认识新事物的过程，起初老师，父母教导我们这个世界的规则，形成对这个世界最初的启蒙，我们知道麻雀会飞，它有一对翅膀，喜鹊也可以飞，因为它也有一对翅膀。但是随着认知的拓展我们又发现并不是有翅膀就可以飞，比如鸵鸟，到这里我们认知的泛化能力产生了局限，我们通过记忆来修正繁华的规则。</p><h2 id="模型结构及原理"><a href="#模型结构及原理" class="headerlink" title="模型结构及原理"></a>模型结构及原理</h2><p><img src="https://uptolimit.top/img/image-20210318170410345.png" alt="image-20210318170410345" style="zoom:80%;"></p><h3 id="Wide-Models部分"><a href="#Wide-Models部分" class="headerlink" title="Wide Models部分"></a>Wide Models部分</h3><p><img src="https://uptolimit.top/img/image-20220915222635109.png" alt="image-20220915222635109" style="zoom: 50%;"></p><p>Wide是一个泛化的线性模型$y=w^Tx+b$，$y$是我们要预测的结果，$x$是特征，它是一个$d$维的向量$x=[x_1,x_2,\dots,x_d]$。$w$是$d$维的权重向量$w=[w_1,w_2,\cdots,w_d]$，$b$是偏移量。特征包含两个部分，一个是原始数据中直接拿过来的数据，另一种是经过特征转化之后得到的特征。最重要的一种特征转化方式是交叉组合，定义如下：</p><script type="math/tex; mode=display">\phi_k{(x)}=\prod^{d}_{i=1}{ {x_i}^{c_{ki} } },c_{ki}\in\{0,1\}</script><p>这里$c_{ki}$表示的是第$i$个特征的第$k$种转化函数$\phi_k$的结果。对于这个特征转化结果来说，只有所有的项都为真，最终的结果才为1，否则是0。比如“<code>AND(gender=female,language=en)</code>”这就是一个交叉特征，只有当用户的性别为女并且使用的语言是英语时，这个特征的结果才为1。通过这种方式，我们可以捕捉到特征之间的交互。以及为线性模型加入非线性的特征。</p><h2 id="Deep-Models部分"><a href="#Deep-Models部分" class="headerlink" title="Deep Models部分"></a>Deep Models部分</h2><p><img src="https://raw.githubusercontent.com/UPTOLIMIT/PicBed/master//img/image-20210318183236289.png" alt="image-20210318183236289" style="zoom:80%;"></p><p>如上图当中的右侧部分，Deep Models是一个前馈神经网络，它的输入是一个稀疏的特征，这个输入会在神经网络的第一层转化为一个低维度的embedding，维度量级通常在$O(10)$到$O(100)$之间，然后和一些原始的稠密特征一起递交给神经网络训练，这个模块主要被设计用来处理一些类别特征，比如性别，语言等。每一层的隐层计算方式如下：</p><script type="math/tex; mode=display">\alpha^{l+1}=f(W^l\alpha^l+b^l)</script><p>其中$\alpha^l$是第$l$层的激活值，$b^l$是第$l$层的偏置，$W^l$是第$l$层的权重，$f$是激活函数。</p><h2 id="Wide-amp-Deep-Models-联合训练"><a href="#Wide-amp-Deep-Models-联合训练" class="headerlink" title="Wide &amp; Deep Models 联合训练"></a>Wide &amp; Deep Models 联合训练</h2><p><img src="https://uptolimit.top/img/image-20220915222843032.png" alt="image-20220915222843032"></p><p>通过加权的方式将Wide部分和Deep部分合并在一起，最上面的输出层是一个sigmoid层，或者是一个线性层，就是一个简单的线性累加，文中称为<code>joint</code>，论文中还降讲到了联合（joint）和集成（ensemble）的区别，集成是每个模型单独训练，再将模型的结果融合，相比于联合训练，集成的每个独立的模型都得学的足够好才有利于随后的回合，因此模型的size也会更大。而在联合训练中，wide部分只需要做一小部分的特征叉乘来弥补deep部分的不足，并不需要一个完整的Wide Models。在集成学习中，每个部分的参数是互不影响的，而在联合学习中，它们的参数是一起训练的。模型选取logistic损失函数，最后的预测输出概率值。公式如下：</p><script type="math/tex; mode=display">P(Y=1|x)=\delta(w_{wide}^T[x,\phi(x)] + w_{deep}^T a^{(lf)} + b)</script><p>其中$\sigma$表示sigmoid函数，$\phi(x)$表示叉乘特征，$\alpha^{(lf)}$表示神经网络最后一层激活值，$b$表示偏置。</p><p>论文中，作者通过梯度的反向传播，使用<code>mini-batch stochastic optimization</code>方法训练参数，并且对wide部分使用带L1正则的<code>FTRL(Follow-the-regularized-leader)</code>算法，对Deep Models部分使用<code>AdaGrad</code>算法。</p><h2 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h2><h3 id="在应用场景中，哪些特征适合放在Wide侧，哪些特征适合放在Deep侧，为什么？"><a href="#在应用场景中，哪些特征适合放在Wide侧，哪些特征适合放在Deep侧，为什么？" class="headerlink" title="在应用场景中，哪些特征适合放在Wide侧，哪些特征适合放在Deep侧，为什么？"></a>在应用场景中，哪些特征适合放在Wide侧，哪些特征适合放在Deep侧，为什么？</h3><p>显然的，直接的，有规律可循的特征适合放在Wide侧，对于一些受上下文影响较大的，简单的规律或许能够反映更大的上下文原因的特征适合放在Deep层。</p><h3 id="为什么Wide部分要用L1-FTRL训练"><a href="#为什么Wide部分要用L1-FTRL训练" class="headerlink" title="为什么Wide部分要用L1 FTRL训练?"></a>为什么Wide部分要用L1 FTRL训练?</h3><p>L1正则化比L2正则化更容易产生稀疏解，FTRL本身是一个稀疏性很好，精度也不错的随机梯度下降方法。L1 FTRL非常注重模型的稀疏性，会让Wide部分的大部分权重都为0，我们无需准备大量0权重特征，大大压缩了模型的权重，也压缩了特征向量的维度。</p><h3 id="为什么Deep部分不特别考虑稀疏性的问题？"><a href="#为什么Deep部分不特别考虑稀疏性的问题？" class="headerlink" title="为什么Deep部分不特别考虑稀疏性的问题？"></a>为什么Deep部分不特别考虑稀疏性的问题？</h3><p>在Deep部分输入类别是数值类特征，或者是已经降维并稠密化的Embedding向量，因此不需要考虑特征稀疏问题。</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>模型部分，特征的选择应该根据实际的业务场景选择哪些特征应该放在wide部分，哪些特征应该放在deep部分</p><pre class=" language-lang-python"><code class="language-lang-python">def WideNDeep(linear_feature_columns, dnn_feature_columns):    # 构建输入层，即所有特征对应的Input()层，这里使用字典的形式返回，方便后续构建模型    dense_input_dict, sparse_input_dict = build_input_layers(linear_feature_columns + dnn_feature_columns)    # 将linear部分的特征中sparse特征筛选出来，后面用来做1维的embedding    linear_sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeat), linear_feature_columns))    # 构建模型的输入层，模型的输入层不能是字典的形式，应该将字典的形式转换成列表的形式    # 注意：这里实际的输入与Input()层的对应，是通过模型输入时候的字典数据的key与对应name的Input层    input_layers = list(dense_input_dict.values()) + list(sparse_input_dict.values())    # Wide&Deep模型论文中Wide部分使用的特征比较简单，并且得到的特征非常的稀疏，所以使用了FTRL优化Wide部分（这里没有实现FTRL）    # 但是是根据他们业务进行选择的，我们这里将所有可能用到的特征都输入到Wide部分，具体的细节可以根据需求进行修改    linear_logits = get_linear_logits(dense_input_dict, sparse_input_dict, linear_sparse_feature_columns)    # 构建维度为k的embedding层，这里使用字典的形式返回，方便后面搭建模型    embedding_layers = build_embedding_layers(dnn_feature_columns, sparse_input_dict, is_linear=False)    dnn_sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeat), dnn_feature_columns))    # 在Wide&Deep模型中，deep部分的输入是将dense特征和embedding特征拼在一起输入到dnn中    dnn_logits = get_dnn_logits(dense_input_dict, sparse_input_dict, dnn_sparse_feature_columns, embedding_layers)    # 将linear,dnn的logits相加作为最终的logits    output_logits = Add()([linear_logits, dnn_logits])    # 这里的激活函数使用sigmoid    output_layer = Activation("sigmoid")(output_logits)    model = Model(input_layers, output_layer)    return model</code></pre><p>模块导入</p><pre class=" language-lang-python"><code class="language-lang-python">import warningswarnings.filterwarnings("ignore")import itertoolsimport pandas as pdimport numpy as npfrom tqdm import tqdmfrom collections import namedtupleimport tensorflow as tffrom tensorflow.keras.layers import *from tensorflow.keras.models import *from sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import  MinMaxScaler, LabelEncoderfrom utils import SparseFeat, DenseFeat, VarLenSparseFeat</code></pre><p>数据预处理</p><pre class=" language-lang-python"><code class="language-lang-python">def data_process(data_df, dense_features, sparse_features):    data_df[dense_features] = data_df[dense_features].fillna(0.0)#填充缺失值    for f in dense_features:# 数据平滑处理        data_df[f] = data_df[f].apply(lambda x: np.log(x+1) if x > -1 else -1)    data_df[sparse_features] = data_df[sparse_features].fillna("-1")#填充缺失值    for f in sparse_features:# 类别编码        lbe = LabelEncoder()        data_df[f] = lbe.fit_transform(data_df[f])    return data_df[dense_features + sparse_features]</code></pre><p>构建输入层并以dense和sparse两类字典的形式返回</p><pre class=" language-lang-python"><code class="language-lang-python">def build_input_layers(feature_columns):    dense_input_dict, sparse_input_dict = {}, {}    for fc in feature_columns:        if isinstance(fc, SparseFeat):            sparse_input_dict[fc.name] = Input(shape=(1, ), name=fc.name)        elif isinstance(fc, DenseFeat):            dense_input_dict[fc.name] = Input(shape=(fc.dimension, ), name=fc.name)    return dense_input_dict, sparse_input_dict</code></pre><p>构建Embedding层并以字典形式返回</p><pre class=" language-lang-python"><code class="language-lang-python">def build_embedding_layers(feature_columns, input_layers_dict, is_linear):    embedding_layers_dict = dict()    # 将特征中的sparse特征筛选出来    sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeat), feature_columns)) if feature_columns else []    # 如果是用于线性部分的embedding层，其维度为1，否则维度就是自己定义的embedding维度    if is_linear:        for fc in sparse_feature_columns:            embedding_layers_dict[fc.name] = Embedding(fc.vocabulary_size, 1, name='1d_emb_' + fc.name)    else:        for fc in sparse_feature_columns:            embedding_layers_dict[fc.name] = Embedding(fc.vocabulary_size, fc.embedding_dim, name='kd_emb_' + fc.name)    return embedding_layers_dict</code></pre><pre class=" language-lang-python"><code class="language-lang-python">def get_linear_logits(dense_input_dict, sparse_input_dict, sparse_feature_columns):    # 将所有的dense特征的Input层，经过一个全连接层得到dense特征的logits    concat_dense_inputs = Concatenate(axis=1)(list(dense_input_dict.values()))    dense_logits_output = Dense(1)(concat_dense_inputs)    # 获取linear部分sparse特征的embedding层，这里使用embedding的原因是：    # 对于linear部分直接将特征进行onehot然后通过一个全连接层，当维度特别大的时候，计算比较慢    # 使用embedding层的好处就是可以通过查表的方式获取到哪些非零的元素对应的权重，然后在将这些权重相加，效率比较高    linear_embedding_layers = build_embedding_layers(sparse_feature_columns, sparse_input_dict, is_linear=True)    # 将一维的embedding拼接，注意这里需要使用一个Flatten层，使维度对应    sparse_1d_embed = []    for fc in sparse_feature_columns:        feat_input = sparse_input_dict[fc.name]        embed = Flatten()(linear_embedding_layers[fc.name](feat_input)) # B x 1        sparse_1d_embed.append(embed)    # embedding中查询得到的权重就是对应onehot向量中一个位置的权重，所以后面不用再接一个全连接了，本身一维的embedding就相当于全连接    # 只不过是这里的输入特征只有0和1，所以直接向非零元素对应的权重相加就等同于进行了全连接操作(非零元素部分乘的是1)    sparse_logits_output = Add()(sparse_1d_embed)    # 最终将dense特征和sparse特征对应的logits相加，得到最终linear的logits    linear_logits = Add()([dense_logits_output, sparse_logits_output])    return linear_logits</code></pre><p>将所有的稀疏特征的embedding拼接</p><pre class=" language-lang-python"><code class="language-lang-python">def concat_embedding_list(feature_columns, input_layer_dict, embedding_layer_dict, flatten=False):    # 将sparse特征筛选出来    sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeat), feature_columns))    embedding_list = []    for fc in sparse_feature_columns:        _input = input_layer_dict[fc.name] # 获取输入层         _embed = embedding_layer_dict[fc.name] # B x 1 x dim  获取对应的embedding层        embed = _embed(_input) # B x dim  将input层输入到embedding层中        # 是否需要flatten, 如果embedding列表最终是直接输入到Dense层中，需要进行Flatten，否则不需要        if flatten:            embed = Flatten()(embed)        embedding_list.append(embed)    return embedding_list</code></pre><p>构建深度神经网络</p><pre class=" language-lang-python"><code class="language-lang-python">def get_dnn_logits(dense_input_dict, sparse_input_dict, sparse_feature_columns, dnn_embedding_layers):    concat_dense_inputs = Concatenate(axis=1)(list(dense_input_dict.values())) # B x n1 (n1表示的是dense特征的维度)     sparse_kd_embed = concat_embedding_list(sparse_feature_columns, sparse_input_dict, dnn_embedding_layers, flatten=True)    concat_sparse_kd_embed = Concatenate(axis=1)(sparse_kd_embed) # B x n2  (n2表示的是Sparse特征的维度)    dnn_input = Concatenate(axis=1)([concat_dense_inputs, concat_sparse_kd_embed]) # B x (n2 + n1)    # dnn层，这里的Dropout参数，Dense中的参数及Dense的层数都可以自己设定    dnn_out = Dropout(0.5)(Dense(1024, activation='relu')(dnn_input))      dnn_out = Dropout(0.3)(Dense(512, activation='relu')(dnn_out))    dnn_out = Dropout(0.1)(Dense(256, activation='relu')(dnn_out))    dnn_logits = Dense(1)(dnn_out)    return dnn_logits</code></pre><p>main函数入口</p><pre class=" language-lang-python"><code class="language-lang-python">if __name__ == "__main__":    # 读取数据    data = pd.read_csv('./data/criteo_sample.txt')    # 划分dense和sparse特征    columns = data.columns.values    dense_features = [feat for feat in columns if 'I' in feat]    sparse_features = [feat for feat in columns if 'C' in feat]    # 简单的数据预处理    train_data = data_process(data, dense_features, sparse_features)    train_data['label'] = data['label']    # 将特征分组，分成linear部分和dnn部分(根据实际场景进行选择)，并将分组之后的特征做标记（使用DenseFeat, SparseFeat）    linear_feature_columns = [SparseFeat(feat, vocabulary_size=data[feat].nunique(),embedding_dim=4)                            for i,feat in enumerate(sparse_features)] + [DenseFeat(feat, 1,)                            for feat in dense_features]    dnn_feature_columns = [SparseFeat(feat, vocabulary_size=data[feat].nunique(),embedding_dim=4)                            for i,feat in enumerate(sparse_features)] + [DenseFeat(feat, 1,)                            for feat in dense_features]    # 构建WideNDeep模型    history = WideNDeep(linear_feature_columns, dnn_feature_columns)    history.summary()    history.compile(optimizer="adam",                 loss="binary_crossentropy",                 metrics=["binary_crossentropy", tf.keras.metrics.AUC(name='auc')])    # 将输入数据转化成字典的形式输入    train_model_input = {name: data[name] for name in dense_features + sparse_features}    # 模型训练    history.fit(train_model_input, train_data['label'].values,            batch_size=64, epochs=5, validation_split=0.2, )</code></pre><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://blog.csdn.net/anshuai_aw1/article/details/105113980">推荐系统（一）：Wide &amp; Deep 源论文整理和思考</a></li><li><a href="https://zhuanlan.zhihu.com/p/334991736">巨经典论文！详解推荐系统经典模型 Wide &amp; Deep</a></li><li><a href="https://zhuanlan.zhihu.com/p/53361519">详解 Wide &amp; Deep 结构背后的动机</a></li><li><a href="https://arxiv.org/abs/1606.07792">Wide &amp; Deep Learning for Recommender Systems</a></li><li><a href="https://zhuanlan.zhihu.com/p/142958834">见微知著，你真的搞懂 Google 的 Wide&amp;Deep 模型了吗？</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 深度推荐系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 推荐系统 </tag>
            
            <tag> DNN </tag>
            
            <tag> 特征工程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DeepCrossing理论与实践</title>
      <link href="2021/03/21/deepcrossing-li-lun-yu-shi-jian/"/>
      <url>2021/03/21/deepcrossing-li-lun-yu-shi-jian/</url>
      
        <content type="html"><![CDATA[<hr><h1 id="论文背景"><a href="#论文背景" class="headerlink" title="论文背景"></a>论文背景</h1><p>DeepCrossing是2016年由微软提出，旨在特征工程中减少人力特征组合的时间开销，通过模型自动学习特征的组合方式，解决特征组合的难题。是第一个企业以正式论文的形式分享推荐系统技术细节的模型。</p><h1 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h1><p><img src="https://raw.githubusercontent.com/UPTOLIMIT/PicBed/master//img/image-20210315215320120.png" alt="image-20210315215320120" style="zoom: 67%;"></p><p>模型的输入是一系列独立的特征，输出是用户的点击率预测值。模型一共分为4层，分别是Embedding层，Stacking层，Multiple Residual Units层，Scoring层。</p><h2 id="Embedding层"><a href="#Embedding层" class="headerlink" title="Embedding层"></a>Embedding层</h2><p>Embedding这个概念最初来源于Manifold Hypothesis（<a href="https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/">流形假设</a>）。流形假设是指<a href="https://www.zhihu.com/question/38002635">“自然的原始数据是低维的流形嵌入于(embedded in)原始数据所在的高维空间”</a>，而深度学习的任务就是把高维原始数据映射到低维流形，使得原始数据变得可分，而这个映射就叫做embedding（嵌入），比如word embedding（词嵌入）就是把单词组成的句子映射到一个表征向量。久而久之，把低维流形的表征向量叫做Embedding成为一种习惯（其实是一种误用）。在本文中，Embedding 层的作用就是将从原始独立特征中提取出来的高维稀疏特征向量转换成低维稠密的Embedding向量。</p><pre class=" language-lang-python"><code class="language-lang-python">def build_embedding_layers(feature_columns, input_layers_dict, is_linear):    # 定义一个embedding层对应的字典    embedding_layers_dict = dict()    # 将特征中的sparse特征筛选出来    sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeat), feature_columns)) if feature_columns else []    # 如果是用于线性部分的embedding层，其维度为1，否则维度就是自己定义的embedding维度    if is_linear:        for fc in sparse_feature_columns:            embedding_layers_dict[fc.name] = Embedding(fc.vocabulary_size + 1, 1, name='1d_emb_' + fc.name)    else:        for fc in sparse_feature_columns:            embedding_layers_dict[fc.name] = Embedding(fc.vocabulary_size + 1, fc.embedding_dim, name='kd_emb_' + fc.name)    return embedding_layers_dict</code></pre><h2 id="Stacking层"><a href="#Stacking层" class="headerlink" title="Stacking层"></a>Stacking层</h2><p>得到了所有特征的Embedding表示之后，Stacking层接收Embedding层的输入并且把这些特征聚合起来，形成一个向量</p><pre class=" language-lang-python"><code class="language-lang-python"># 将所有的sparse特征embedding拼接def concat_embedding_list(feature_columns, input_layer_dict, embedding_layer_dict, flatten=False):    # 将sparse特征筛选出来    sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeat), feature_columns))    embedding_list = []    for fc in sparse_feature_columns:        _input = input_layer_dict[fc.name] # 获取输入层         _embed = embedding_layer_dict[fc.name] # B x 1 x dim  获取对应的embedding层        embed = _embed(_input) # B x dim  将input层输入到embedding层中        # 是否需要flatten, 如果embedding列表最终是直接输入到Dense层中，需要进行Flatten，否则不需要        if flatten:            embed = Flatten()(embed)        embedding_list.append(embed)    return embedding_list</code></pre><h2 id="Multiple-Residual-Units层"><a href="#Multiple-Residual-Units层" class="headerlink" title="Multiple Residual Units层"></a>Multiple Residual Units层</h2><p><img src="F:%5CMyBlog%5Csource%5C_posts%5Cimage-20210316193133492.png" alt="image-20210316193133492"></p><p>DeepCrossing中的Crossing就是通过多个残差单元来实现的，残差单元的结构如上图所示。与ResNet不同的是，它不包含卷积操作，也是第一次在图像识别之外应用残差单元。</p><script type="math/tex; mode=display">X^O=F(X^I,\{W_0,W_1\},\{b_0,b_1\})+X^I</script><p>将$X^I$移动到左侧,可以看出$F$函数拟合的是输入和输出之间的残差。对输入进行全连接变换之后，经过<code>relu</code>激活函数送入第二个全连接层，将输出结果与原始输入进行<code>element-wise add</code>操作，再经过<code>relu</code>激活输出，实验表明，残差结构能更敏感的捕获输入输出之间的信息差。</p><pre class=" language-lang-python"><code class="language-lang-python"># DNN残差块的定义class ResidualBlock(Layer):    def __init__(self, units): # units表示的是DNN隐藏层神经元数量        super(ResidualBlock, self).__init__()        self.units = units    def build(self, input_shape):        out_dim = input_shape[-1]        self.dnn1 = Dense(self.units, activation='relu')        self.dnn2 = Dense(out_dim, activation='relu') # 保证输入的维度和输出的维度一致才能进行残差连接    def call(self, inputs):        x = inputs        x = self.dnn1(x)        x = self.dnn2(x)        x = Activation('relu')(x + inputs) # 残差操作        return x</code></pre><h2 id="Scoring层"><a href="#Scoring层" class="headerlink" title="Scoring层"></a>Scoring层</h2><p>Scoring层作为输出层，输出广告预测点击率，是为了拟合优化目标而存在，对于二分类问题，往往使用逻辑回归，模型通过叠加多个残差块加深网络的深度，最后将结果转换为一个概率值输出。</p><pre class=" language-lang-python"><code class="language-lang-python"># block_nums表示DNN残差块的数量，通过叠加残差块的数量可以加深网络深度def get_dnn_logits(dnn_inputs, block_nums=3):    dnn_out = dnn_inputs    for i in range(block_nums):        dnn_out = ResidualBlock(64)(dnn_out)        # 这里使用sigmoid激活函数，将dnn的输出转化成logits        dnn_logits = Dense(1, activation='sigmoid')(dnn_out)        return dnn_logits</code></pre><h2 id="代码解读"><a href="#代码解读" class="headerlink" title="代码解读"></a>代码解读</h2><p>从<code>main</code>函数开始，按照代码执行顺序，阅读代码。</p><pre class=" language-lang-python"><code class="language-lang-python"># 读取数据data = pd.read_csv('./data/criteo_sample.txt')# 划分dense和sparse特征，即密集特征和稀疏特征，观察原数据集可以发现是按列分好的columns = data.columns.valuesdense_features = [feat for feat in columns if 'I' in feat]sparse_features = [feat for feat in columns if 'C' in feat]</code></pre><pre class=" language-lang-python"><code class="language-lang-python"> # 简单的数据预处理    train_data = data_process(data, dense_features, sparse_features)    train_data['label'] = data['label']</code></pre><pre class=" language-lang-python"><code class="language-lang-python"># 数据预处理函数def data_process(data_df, dense_features, sparse_features):    """    简单处理特征，包括填充缺失值，数值处理，类别编码    param data_df: DataFrame格式的数据    param dense_features: 数值特征名称列表    param sparse_features: 类别特征名称列表    """    data_df[dense_features] = data_df[dense_features].fillna(0.0)    # 使用log函数对数据进行平滑处理    for f in dense_features:        data_df[f] = data_df[f].apply(lambda x: np.log(x+1) if x > -1 else -1)    # 将缺失值填充为-1      data_df[sparse_features] = data_df[sparse_features].fillna("-1")    # 将类别型数据one-hot编码    for f in sparse_features:        lbe = LabelEncoder()        data_df[f] = lbe.fit_transform(data_df[f])    return data_df[dense_features + sparse_features]</code></pre><pre class=" language-lang-PYTHON"><code class="language-lang-PYTHON"># 将特征做标记dnn_feature_columns = [SparseFeat(feat, vocabulary_size=data[feat].nunique(),embedding_dim=4)                            for feat in sparse_features] + [DenseFeat(feat, 1,)                            for feat in dense_features]</code></pre><p>这里用到了python的一个工厂函数<code>namedtuple()</code>可以译为<code>具名元组</code>，可以理解为元组的增强版本，为元组中的每一个元素赋予了含义，从而增强代码可读性。</p><pre class=" language-lang-python"><code class="language-lang-python">SparseFeat = namedtuple('SparseFeat', ['name', 'vocabulary_size', 'embedding_dim'])</code></pre><pre class=" language-lang-python"><code class="language-lang-python"># 构建DeepCrossing模型    history = DeepCrossing(dnn_feature_columns)</code></pre><p>DeepCrossing模型代码如下：</p><pre class=" language-lang-python"><code class="language-lang-python">def DeepCrossing(dnn_feature_columns):    # 构建输入层，即所有特征对应的Input()层，这里使用字典的形式返回，方便后续构建模型    dense_input_dict, sparse_input_dict = build_input_layers(dnn_feature_columns)    # 构建模型的输入层，模型的输入层不能是字典的形式，应该将字典的形式转换成列表的形式    # 注意：这里实际的输入与Input()层的对应，是通过模型输入时候的字典数据的key与对应name的Input层    input_layers = list(dense_input_dict.values()) + list(sparse_input_dict.values())    # 构建维度为k的embedding层，这里使用字典的形式返回，方便后面搭建模型    embedding_layer_dict = build_embedding_layers(dnn_feature_columns, sparse_input_dict, is_linear=False)    #将所有的dense特征拼接到一起，B为Batchsize    dense_dnn_list = list(dense_input_dict.values())    dense_dnn_inputs = Concatenate(axis=1)(dense_dnn_list) # B x n (n表示数值特征的数量)    # 因为需要将其与dense特征拼接到一起所以需要Flatten，不进行Flatten的Embedding层输出的维度为：Bx1xdim    sparse_dnn_list = concat_embedding_list(dnn_feature_columns, sparse_input_dict, embedding_layer_dict, flatten=True)     sparse_dnn_inputs = Concatenate(axis=1)(sparse_dnn_list) # B x m*dim (n表示类别特征的数量，dim表示embedding的维度)    # 将dense特征和Sparse特征拼接到一起    dnn_inputs = Concatenate(axis=1)([dense_dnn_inputs, sparse_dnn_inputs]) # B x (n + m*dim)    # 输入到dnn中，需要提前定义需要几个残差块    output_layer = get_dnn_logits(dnn_inputs, block_nums=3)    model = Model(input_layers, output_layer)    return model</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>DeepCrossing的结构比较简单清晰，没有引入特殊的模型结构，只是常规的Embedding+多层神经网络。但是在DeepCrossing中没有任何人工特征工程的参与，只需要简单的特征处理，原始特征经过Embedding层输入神经网络层，自主交叉和学习，通过调整神经网络的深度进行特征之间的深度交叉，也是DeepCrossing名称的由来（大悟）。</p><p>最后附上论文<a href="https://link.zhihu.com/?target=http%3A//www.kdd.org/kdd2016/papers/files/adf0975-shanA.pdf">链接</a></p>]]></content>
      
      
      <categories>
          
          <category> 深度推荐系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 推荐系统 </tag>
            
            <tag> DNN </tag>
            
            <tag> 特征工程 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
