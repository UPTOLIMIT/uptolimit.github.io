<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>C语言基础</title>
      <link href="2021/03/30/c-yu-yan-ji-chu/"/>
      <url>2021/03/30/c-yu-yan-ji-chu/</url>
      
        <content type="html"><![CDATA[<hr><h2 id="汇编与C语言"><a href="#汇编与C语言" class="headerlink" title="汇编与C语言"></a>汇编与C语言</h2><ul><li>开发效率</li><li>程序可移植性</li><li>运行效率</li></ul><h2 id="C语言的优势"><a href="#C语言的优势" class="headerlink" title="C语言的优势"></a>C语言的优势</h2><ul><li>高级语言</li><li>代码量变少</li><li>程序的可移植性<ul><li>不同系统平台皆可编译运行</li></ul></li></ul><h2 id="汇编语言的优势"><a href="#汇编语言的优势" class="headerlink" title="汇编语言的优势"></a>汇编语言的优势</h2><ul><li>与计算机硬件强相关，或者可以认为汇编语言直接与硬件交互</li></ul><h2 id="输入输出函数说明"><a href="#输入输出函数说明" class="headerlink" title="输入输出函数说明"></a>输入输出函数说明</h2><h3 id="printf-函数"><a href="#printf-函数" class="headerlink" title="printf 函数"></a><code>printf</code> 函数</h3><ul><li>头文件：<code>stdio.h</code></li><li><p>函数原型：<code>int printf(const char *format, ...)</code></p></li><li><p><code>format</code>：格式控制字符串</p></li><li><p><code>...</code>：可变参数列表，在使用<code>printf</code>函数的时候，可以传入任意多个参数。</p></li><li>返回值：输出字符的数量</li></ul><h3 id="scanf函数"><a href="#scanf函数" class="headerlink" title="scanf函数"></a><code>scanf</code>函数</h3><ul><li>头文件：<code>stdio.h</code></li><li>函数原型：<code>int scanf(const char *format, ...)</code></li><li><code>format</code>：格式控制字符串</li><li><code>...</code>：可变参数列表</li><li>返回值：成功读入的参数个数（返回值为0合法）</li></ul><h4 id="循环读入命题"><a href="#循环读入命题" class="headerlink" title="循环读入命题"></a>循环读入命题</h4><pre class=" language-lang-c"><code class="language-lang-c">while(scanf()!=EOF){//或者while(scanf()!=-1)}</code></pre><p>==思考==</p><p>程序如何知道什么时候停止读取呢？</p><p>==Tips：==</p><p><code>EOF</code>：end of file（在linux操作系统之中，一切皆文件），<code>EOF</code>可以看作一类隐藏的文件描述符，当文件指针碰到<code>EOF</code>是表示读到文件末尾，<code>EOF</code>对应整数值<code>-1</code></p><h2 id="练习"><a href="#练习" class="headerlink" title="练习"></a>练习</h2><ol><li><p>请使用<code>printf</code>函数，求解一个数字n的十进制表示的数字位数</p><pre class=" language-lang-c"><code class="language-lang-c">#include<stdio.h>int main(){    int n;    scanf("%d", &n);    printf(" has %d digits.\n", printf("%d", n));    return 0;}</code></pre></li><li><p>为题1添加一个循环读入</p><pre class=" language-lang-C"><code class="language-lang-C">#include<stdio.h>int main(){    int n;    while(scanf("%d", &n) != EOF){        printf(" has %d digits.\n", printf("%d", n));    }    return 0;}</code></pre></li><li><p>请写一个程序，读入一个行字符串的（可能包含空格），输出这个字符串中字符的数量。</p><pre class=" language-lang-c"><code class="language-lang-c">#include<stdio.h>int main(){    char str[100] = {0};    while(scanf("%[^\n]s", str) != EOF){        getchar();        printf(" has %d chars!\n", printf("%s", str));    }     return 0;}</code></pre><p>==Tips：==</p><p><code>gets()</code>方法在开发时被认为是一种很危险的方法，是不允许使用的，<code>%c</code>会读入所有字符，使用时注意误读入的情况发生。</p><p><code>%[^\n]s</code>：这里使用了字符匹配集，<code>[]</code>中为正则表达式，指定输入规则。</p></li></ol><h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><ol><li><p>如何停止循环读入？</p><p><code>ctrl+z</code>相当于是给<code>scanf</code>一个<code>EOF</code></p><p><code>ctrl+c</code>结束当前进程，从操作系统层面去强制结束当前进程</p></li><li><p>区别<code>printf</code>家族函数用法，尝试阅读man手册</p><pre class=" language-lang-c"><code class="language-lang-c">#include <stdio.h>int printf(const char *format, ...);int fprintf(FILE *stream, const char *format, ...);int dprintf(int fd, const char *format, ...);int sprintf(char *str, const char *format, ...);int snprintf(char *str, size_t size, const char *format, ...);</code></pre><pre class=" language-lang-c"><code class="language-lang-c">#include <stdarg.h>int vprintf(const char *format, va_list ap);int vfprintf(FILE *stream, const char *format, va_list ap);int vdprintf(int fd, const char *format, va_list ap);int vsprintf(char *str, const char *format, va_list ap);int vsnprintf(char *str, size_t size, const char *format, va_list ap);</code></pre></li></ol><ol><li><p>区别<code>%c</code>与<code>%s</code>的用法</p><p><code>%c</code>能够接收所有的字符，<code>%s</code>接收除空格、<code>\n</code>、<code>\t</code>以外的所有字符，同时空格、<code>\n</code>、<code>\t</code>会被当做分隔符处理。</p></li><li><p>对于练习3如果没有<code>getchar()</code>会发生什么问题，为什么？</p><p><code>scanf()</code>中的正则表达式会使<code>\n</code>卡在缓冲区的出口处，被输入规则排除在外，因此<code>scanf()</code>的返回值为0，因此程序会陷入死循环。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 重学C语言 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C语言 </tag>
            
            <tag> 编程语言 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DeepFM理论与实践</title>
      <link href="2021/03/21/deepfm-li-lun-yu-shi-jian/"/>
      <url>2021/03/21/deepfm-li-lun-yu-shi-jian/</url>
      
        <content type="html"><![CDATA[<h2 id="理论背景"><a href="#理论背景" class="headerlink" title="理论背景"></a>理论背景</h2><p>DeepFM模型是2017年哈工大深圳与华为诺亚方舟联合实验室提出的，论文名称《<a href="https://arxiv.org/abs/1703.04247">DeepFM: A Factorization-Machine based Neural Network for CTR Prediction</a>》，DFM模型是在W&amp;D模型上的改进，W&amp;D模型理论参照上一篇<a href="https://zhuanlan.zhihu.com/p/358174532">笔记</a>，在W&amp;D的基础上，将Wide部分替换为FM模型，不再需要人工特征工程，同时巧妙地在FM的二阶部分和神经网络的Embedding层共享权重，减少了大量参数，极大的提高了训练速度。</p><p>在CTR预估任务中，业界常用的方法有人工特征工程+逻辑回归、梯度增强决策树（GBDT）+逻辑回归，FM（Factorization Machine）和FFM（Field-aware Factorization Machine）模型，在这些模型中FM、FFM模型表现突出。</p><h2 id="FM模型"><a href="#FM模型" class="headerlink" title="FM模型"></a>FM模型</h2><p>在学习DFM之前，先简单的学习一下FM模型和FFM模型，FM模型是由Konstanz 大学 Steffen Rendle于2010年提出，旨在解决稀疏数据下的特征组合问题。下面我们用一个<a href="http://www.cs.cmu.edu/~wcohen/10-605/2015-guest-lecture/FM.pdf">例子</a>引入FM模型，假设我们要处理一个广告问题，根据用户和广告类型特征，预测用户是否点击了广告。</p><div class="table-container"><table><thead><tr><th>Clicked？</th><th>Country</th><th>Day</th><th>Ad_type</th></tr></thead><tbody><tr><td>1</td><td>USA</td><td>26/11/15</td><td>Movie</td></tr><tr><td>0</td><td>China</td><td>1/7/14</td><td>Game</td></tr><tr><td>1</td><td>China</td><td>19/2/15</td><td>Game</td></tr></tbody></table></div><p>其中<strong>Clicked？</strong>是标签label，<strong>Country</strong>、<strong>Day</strong>、<strong>Ad_type</strong>是特征。由于三种特征都是类别类型，我们需要通过One-Hot编码将他们转换成数值型特征。</p><div class="table-container"><table><thead><tr><th style="text-align:left"><strong>Clicked?</strong></th><th style="text-align:left">Country=USA</th><th style="text-align:left">Country=China</th><th style="text-align:left">Day=26/11/15</th><th style="text-align:left">Day=1/7/14</th><th style="text-align:left">Day=19/2/15</th><th style="text-align:left">Ad_type=Movie</th><th style="text-align:left">Ad_type=Game</th></tr></thead><tbody><tr><td style="text-align:left"><strong>1</strong></td><td style="text-align:left">1</td><td style="text-align:left">0</td><td style="text-align:left">1</td><td style="text-align:left">0</td><td style="text-align:left">0</td><td style="text-align:left">1</td><td style="text-align:left">0</td></tr><tr><td style="text-align:left"><strong>0</strong></td><td style="text-align:left">0</td><td style="text-align:left">1</td><td style="text-align:left">0</td><td style="text-align:left">1</td><td style="text-align:left">0</td><td style="text-align:left">0</td><td style="text-align:left">1</td></tr><tr><td style="text-align:left"><strong>1</strong></td><td style="text-align:left">0</td><td style="text-align:left">1</td><td style="text-align:left">0</td><td style="text-align:left">0</td><td style="text-align:left">1</td><td style="text-align:left">0</td><td style="text-align:left">1</td></tr></tbody></table></div><p>经过One-Hot编码之后，产生的样本数据大部分都是比较稀疏的，每个样本具有7维特征，但平均仅有3维特征具有非零值。在真实场景中，这样的情况也是普遍存在的，例如我们扩展上述数据的特征，加入用户的性别、职业、教育水平，商品品类等，经过One-Hot编码转换后都会导致样本数据的稀疏性。特别是商品品类这种类型的特征，如商品的品类约有550个，采用One-Hot编码生成550个数值特征，但是每个样本的550个特征中，有且仅有一个是有效的（非零）。由此可见，数据的稀疏性，在真实场景中是不可避免地挑战。</p><p>另外，我们也可以发现，经过One-Hot编码之后，特征空间陡然增大。例如，商品品类有550维特征，且是一个类别特征引起的。</p><p>通过观察大量样本数据可以发现，某些特征经过关联之后，与label之间的相关性就会提高，例如，“USA”与“Thanksgiving”、“China”与“Chinese New Year”这样的关联特征。对用户的点击行为有着正向的影响。同样，“化妆品”类商品与“女性”，“球类运动”类商品与“男”性，可见这种关联特征也是普遍存在的，因此进行特征的组合是非常有意义的。</p><p>在众多模型中，多项式模型是包含特征组合的最直观的模型。在多项式模型中，特征$x_i$和$x_j$的组合我们采用$x_ix_j$表示，即$x_i$和$x_j$都非零时，组合特征$x_ix_j$才有意义，在这里我们只讨论二阶多项式模型。表达式如下：</p><script type="math/tex; mode=display">y(x)=w_0+\sum^{n}_{i=1}{w_ix_i}+\sum^{n}_{i=1}\sum^{n}_{j=i+1}{w_{ij}x_ix_j}</script><p>其中，$n$代表样本的特征数量，$x_i$是第$i$个特征的值，$w_0$、$w_i$、$w_{ij}$是模型参数。</p><p>在多项式模型中，组合特征的参数一共有${n(n-1)\over{2} }$个，且任意两个参数都是独立的。但是因为每个参数$w_{ij}$的训练都需要大量的$x_i$和$x_j$都非零的样本，在数据稀疏性普遍存在的实际场景中，这样的训练是非常困难的，参数$w_{ij}$的不准确将严重影响模型的性能。</p><p>那么，我们如何解决二次项参数的训练问题呢？我们借助矩阵分解的思想，将所有二次项参数$w_{ij}$组成一个对称矩阵$W$，这个矩阵可以分解为$W=V^TV$，$V$的第$j$列便是第$j$维特征的隐向量，即$w_{ij}=\langle{v_i,v_j}\rangle$，这就是FM模型的核心思想。因此二阶FM模型的方程可以表述为：</p><script type="math/tex; mode=display">y(x)=w_0+\sum^{n}_{i=1}{w_ix_i}+\sum^{n}_{i=1}\sum^{n}_{j=i+1}{\langle{v_i,v_j}\rangle}{x_ix_j}</script><p>其中，$v_i$是第$i$维特征的隐向量，${\langle{\cdot,\cdot}\rangle}$表示向量点积。</p><h2 id="FFM模型"><a href="#FFM模型" class="headerlink" title="FFM模型"></a>FFM模型</h2><p>FFM模型（Field-aware Factorization Machine）来源于Yu-Chin Juan与其比赛队员，他们借鉴了Michael Jahrer的<a href="https://kaggle2.blob.core.windows.net/competitions/kddcup2012/2748/media/Opera.pdf">论文</a>中的field概念，提出了FM的升级版模型。FFM把相同性质的特征归于同一个Field。以上面的广告点击案例为例，“Day=26/11/15”、“Day=1/7/14”、“Day=19/2/15” 这三个特征都是代表日期的，可以放到同一个 Field 中。同理，550个商品类别特征也可以放到一个Field中。即同一个类别特征经过One-Hot编码生成的数值特征都可以放到同一个Field中。在FFM中，每一维特征$x_i$，针对其它特征的每一个Field记为$f_j$，都会学习一个隐向量$v_{i,f_j}$。因此，隐向量不仅与特征相关，也与Field相关。也就是说，“Day=26/11/15” 这个特征与 “Country” 特征和 “Ad_type” 特征进行关联的时候使用不同的隐向量，这与 “Country” 和 “Ad_type” 的内在差异相符，也是 FFM 中 “field-aware” 的由来。</p><p>假设样本的 $n$个特征属于$f$个 field，那么 FFM 的二次项有 $nf$个隐向量。而在 FM 模型中，每一维特征的隐向量只有一个。<strong>FM 可以看作 FFM 的特例</strong>，是把所有特征都归属到一个 field 时的 FFM 模型。根据 FFM 的 field 敏感特性，可以导出其模型方程。如下：</p><script type="math/tex; mode=display">y(x)=w_0+\sum^{n}_{i=1}{w_ix_i}+\sum^{n}_{i=1}\sum^{n}_{j=i+1}{\langle{v_{i,f_j},v_{j,f_i}}\rangle}{x_ix_j}</script><h2 id="DFM模型"><a href="#DFM模型" class="headerlink" title="DFM模型"></a>DFM模型</h2><p>FM模型和FFM模型都在特征组合方面进行了探索，FM通过对每一维特征的隐变量内积来提取特征组合，虽然也能取得非常好的结果，但是由于计算复杂度的原因，往往只能局限在二阶特征组合进行建模。如果要使用FFM模型，所有的特征必须要转换成”field_id:feat_id:value”的格式，field_id表示特征所属field的编号，feat_id代表特征编号，value是特征的值。</p><p>对于高阶的特征组合来说，我们通常会想到通过DNN去解决，但是正如我们之前介绍的那样，对于类别型特征，我们使用One-Hot编码的方式处理，但是将One-Hot编码输入到DNN中，会使网络参数陡增。</p><p><img src="https://cdn.jsdelivr.net/gh/UPTOLIMIT/PicBed//img/image-20210322193403055.png" alt="image-20210322193403055" style="zoom:50%;"></p><p>为了解决参数量过大的问题，我们可以借助FFM的思想，将特征分为不同的Field，避免全连接，分而治之。</p><p><img src="https://cdn.jsdelivr.net/gh/UPTOLIMIT/PicBed//img/image-20210322193423532.png" alt="image-20210322193423532" style="zoom:50%;"></p><p>然后通过增加全连接层就可以实现高阶的特征组合，如下：</p><p><img src="https://cdn.jsdelivr.net/gh/UPTOLIMIT/PicBed//img/image-20210322193448474.png" alt="image-20210322193448474" style="zoom:50%;"></p><p>但是这里仍然缺少低阶的特征组合，我们使用FM把低阶特征组合单独建模。</p><p><img src="https://cdn.jsdelivr.net/gh/UPTOLIMIT/PicBed//img/image-20210322193551205.png" alt="image-20210322193551205" style="zoom:50%;"></p><p>然后把低阶特征组合模型插入到网络结构中</p><p><img src="https://cdn.jsdelivr.net/gh/UPTOLIMIT/PicBed//img/image-20210322193622982.png" alt="image-20210322193622982" style="zoom:50%;"></p><h2 id="网络融合的结构"><a href="#网络融合的结构" class="headerlink" title="网络融合的结构"></a>网络融合的结构</h2><h3 id="串行结构"><a href="#串行结构" class="headerlink" title="串行结构"></a>串行结构</h3><p><img src="https://cdn.jsdelivr.net/gh/UPTOLIMIT/PicBed//img/image-20210322193658105.png" alt="image-20210322193658105" style="zoom:50%;"></p><h3 id="并行结构"><a href="#并行结构" class="headerlink" title="并行结构"></a>并行结构</h3><p><img src="https://cdn.jsdelivr.net/gh/UPTOLIMIT/PicBed//img/image-20210322194746626.png" alt="image-20210322194746626" style="zoom: 50%;"></p><p><a href="https://uptolimit.github.io/2021/03/21/wide-deep-lun-wen-jie-xi-yu-dai-ma-shi-xian/">Wide&amp;Deep模型</a>就是典型的并行结构。</p><h2 id="DeepFM模型结构和原理"><a href="#DeepFM模型结构和原理" class="headerlink" title="DeepFM模型结构和原理"></a>DeepFM模型结构和原理</h2><p>我们先来看下DeepFM模型的结构：</p><p><img src="https://cdn.jsdelivr.net/gh/UPTOLIMIT/PicBed//img/image-20210322193940693.png" alt="image-20210322193940693" style="zoom: 80%;"></p><p>可以发现模型结构由FM与DNN两部分组成，FM负责低阶特征的组合提取，DNN负责高阶特征的组合提取，这两部分共享同一的输入，预测结果是对FM和DNN的结果一起做Embedding，可以如下表示：</p><script type="math/tex; mode=display">\hat{y}=sigmoid(y_{FM}+y_{DNN})</script><h3 id="FM-部分"><a href="#FM-部分" class="headerlink" title="FM 部分"></a>FM 部分</h3><p><img src="https://cdn.jsdelivr.net/gh/UPTOLIMIT/PicBed//img/image-20210322194036684.png" alt="image-20210322194036684" style="zoom:80%;"></p><script type="math/tex; mode=display">\hat{y}_{FM}(x)=w_0+\sum^{N}_{i=1}{w_ix_i}+\sum^N_{i=1}\sum^N_{j=i+1}v^T_iv_jx_ix_j</script><p>由上图和公式大致可以看出FM部分是由一阶特征和二阶特征Concatenate到一起再经过一个Sigmoid得到logits，所以在实现的时候要分别考虑线性相加部分和FM交叉特征部分。</p><h3 id="Deep部分"><a href="#Deep部分" class="headerlink" title="Deep部分"></a>Deep部分</h3><p><img src="https://cdn.jsdelivr.net/gh/UPTOLIMIT/PicBed//img/image-20210322194136996.png" alt="image-20210322194136996" style="zoom:80%;"></p><p>在第一层隐藏层之前，引入一个Embedding层将高维稀疏向量转为低维稠密向量，以解决参数爆炸问题。Embedding层的结构如下图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/UPTOLIMIT/PicBed//img/image-20210322194205481.png" alt="image-20210322194205481" style="zoom:80%;"></p><p>这里需要注意的是尽管Field的输入长度不同，但是Embedding之后向量的长度均为K。另外FM里面得到的隐变量$ V_{ik} $作为Embedding层网络的权重。Embedding层的输出是将所有id类特征对应embedding向量concat到一起输入到DNN中。其中$v_i$表示第$i$个field的embedding，$m$是field的数量。我们得到第一层的输入：</p><script type="math/tex; mode=display">z_1=[v_1,v_2,\cdots,v_m]</script><p>把上一层的输出作为下一层的输入，得到：</p><script type="math/tex; mode=display">z_L=\sigma(W_{L-1}z_{L-1}+b_{L-1})</script><p>其中$\sigma$是激活函数，$z$表示该层的输入，$W$表示该层的权重，$b$表示偏置。</p><p>DNN部分输出如下：</p><script type="math/tex; mode=display">\hat{y}_{DNN}=\sigma{(W^L\alpha^L+b^L)}</script><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>DeepFM模型大致由两部分组成，分别为FM和DNN，而FM部分又由一阶特征部分和二阶特征交叉部分组成，所以模型大概可以拆成三部分，分别为FM一阶特征Linear部分，二阶特征交叉部分和DNN的高阶特征交叉部分。</p><h2 id="两个思考题"><a href="#两个思考题" class="headerlink" title="两个思考题"></a>两个思考题</h2><ul><li><p>如果对于FM采用随机梯度下降SGD训练模型参数，请写出模型各个参数的梯度和FM参数训练的复杂度</p><p>FM的模型方程为：</p><script type="math/tex; mode=display">y(x)=w_0+\sum^{n}_{i=1}{w_ix_i}+\sum^{n}_{i=1}\sum^{n}_{j=i+1}{w_{ij}x_ix_j}</script><p>如果直接计算，复杂度为$O(\bar{n}^2)$,其中$\sum^{n}_{i=1}\sum^{n}_{j=i+1}{w_{ij}x_ix_j}$可化简为,复杂度为$O(\bar{n})$.</p><script type="math/tex; mode=display">\begin{equation}\begin{aligned}\sum^{n}_{i=1}\sum^{n}_{j=i+1}{w_{ij}x_ix_j}&=\sum^{n}_{i=1}\sum^{n}_{j=i+1}\sum_{k=1}^{n}{v_{ik}v_{jk} }x_ix_j\\&={1\over2}\sum_{k=1}^{n}((\sum_{i=1}^{n}v_{ik}x_i)^2-\sum_{i=1}^{n}{v_{ik}x_i^2})\end{aligned}\end{equation}</script></li></ul><p>  利用SGD（Stochastic Gradient Descent）训练模型，模型各个参数的梯度如下：</p><script type="math/tex; mode=display">  \frac{\partial }{ { \partial \theta }}y(x) = \left\{ {\begin{array}{*{20}{c} }  {1,if\ \theta\ is\ w_0}\\  {x_i,if\ \theta\ is\ w_i}\\  {x_i\sum}_{j=i+1}^{n}{v_{jk}x_j}-v_{ik}x_i^2,if\ \theta\ is\ w_{ik}  \end{array} } \right.</script><ul><li><p>对于下图所示，根据你的理解Sparse Feature中的不同颜色节点分别表示什么意思？</p><p><img src="https://cdn.jsdelivr.net/gh/UPTOLIMIT/PicBed//img/image-20210322194851954.png" alt="image-20210322194851954"></p><p>不同的颜色对应的是稀疏特征向量中不同维度不同的值。浅蓝色表示对应的类别特征的onehot向量，黄色的点表示有效值（1），浅蓝色的点表示无效值（0）。</p></li></ul><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>我们还是按照代码的执行顺序来学习DeepFM的实现逻辑，使用的数据集还是criteo。首先是main函数：</p><pre class=" language-lang-python"><code class="language-lang-python">if __name__ == "__main__":    # 读取数据    data = pd.read_csv('./data/criteo_sample.txt')    # 划分dense和sparse特征    columns = data.columns.values    dense_features = [feat for feat in columns if 'I' in feat]    sparse_features = [feat for feat in columns if 'C' in feat]    # 简单的数据预处理    train_data = data_process(data, dense_features, sparse_features)    train_data['label'] = data['label']    # 将特征分组，分成linear部分和dnn部分(根据实际场景进行选择)，并将分组之后的特征做标记（使用DenseFeat, SparseFeat）    linear_feature_columns = [SparseFeat(feat, vocabulary_size=data[feat].nunique(),embedding_dim=4)                            for i,feat in enumerate(sparse_features)] + [DenseFeat(feat, 1,)                            for feat in dense_features]    dnn_feature_columns = [SparseFeat(feat, vocabulary_size=data[feat].nunique(),embedding_dim=4)                            for i,feat in enumerate(sparse_features)] + [DenseFeat(feat, 1,)                            for feat in dense_features]    # 构建DeepFM模型    history = DeepFM(linear_feature_columns, dnn_feature_columns)    history.summary()    history.compile(optimizer="adam",                 loss="binary_crossentropy",                 metrics=["binary_crossentropy", tf.keras.metrics.AUC(name='auc')])    # 将输入数据转化成字典的形式输入    train_model_input = {name: data[name] for name in dense_features + sparse_features}    # 模型训练    history.fit(train_model_input, train_data['label'].values,            batch_size=64, epochs=5, validation_split=0.2, )</code></pre><p>数据预处理部分和特征分组部分在前两部分有所解释，这里不再赘述。FM部分实现如下：</p><pre class=" language-lang-python"><code class="language-lang-python">def DeepFM(linear_feature_columns, dnn_feature_columns):    # 构建输入层，即所有特征对应的Input()层，这里使用字典的形式返回，方便后续构建模型    dense_input_dict, sparse_input_dict = build_input_layers(linear_feature_columns + dnn_feature_columns)    # 将linear部分的特征中sparse特征筛选出来，后面用来做1维的embedding    linear_sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeat), linear_feature_columns))    # 构建模型的输入层，模型的输入层不能是字典的形式，应该将字典的形式转换成列表的形式    # 注意：这里实际的输入与Input()层的对应，是通过模型输入时候的字典数据的key与对应name的Input层    input_layers = list(dense_input_dict.values()) + list(sparse_input_dict.values())    # linear_logits由两部分组成，分别是dense特征的logits和sparse特征的logits    linear_logits = get_linear_logits(dense_input_dict, sparse_input_dict, linear_sparse_feature_columns)    # 构建维度为k的embedding层，这里使用字典的形式返回，方便后面搭建模型    # embedding层用户构建FM交叉部分和DNN的输入部分    embedding_layers = build_embedding_layers(dnn_feature_columns, sparse_input_dict, is_linear=False)    # 将输入到dnn中的所有sparse特征筛选出来    dnn_sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeat), dnn_feature_columns))    fm_logits = get_fm_logits(sparse_input_dict, dnn_sparse_feature_columns, embedding_layers) # 只考虑二阶项    # 将所有的Embedding都拼起来，一起输入到dnn中    dnn_logits = get_dnn_logits(sparse_input_dict, dnn_sparse_feature_columns, embedding_layers)    # 将linear,FM,dnn的logits相加作为最终的logits    output_logits = Add()([linear_logits, fm_logits, dnn_logits])    # 这里的激活函数使用sigmoid    output_layers = Activation("sigmoid")(output_logits)    model = Model(input_layers, output_layers)    return model</code></pre><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol><li><a href="https://tech.meituan.com/2016/03/03/deep-understanding-of-ffm-principles-and-practices.html">深入 FFM 原理与实践</a></li><li><a href="https://zhuanlan.zhihu.com/p/35815532">推荐系统遇上深度学习 (三)—DeepFM 模型理论和实践</a></li><li><a href="https://www.jiqizhixin.com/graph/technologies/f82b7976-b182-40fa-b7d8-a3aad9952937">深度神经网络</a></li><li><a href="https://zhuanlan.zhihu.com/p/57873613">深度推荐模型之DeepFM</a></li><li><a href="https://zhuanlan.zhihu.com/p/53361519">详解 Wide &amp; Deep 结构背后的动机</a></li><li><a href="https://blog.csdn.net/qq_32486393/article/details/103498519">FM算法公式推导</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 深度推荐系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 推荐系统 </tag>
            
            <tag> DNN </tag>
            
            <tag> 特征工程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Wide&amp;Deep论文解析与代码实现</title>
      <link href="2021/03/21/wide-deep-lun-wen-jie-xi-yu-dai-ma-shi-xian/"/>
      <url>2021/03/21/wide-deep-lun-wen-jie-xi-yu-dai-ma-shi-xian/</url>
      
        <content type="html"><![CDATA[<h2 id="论文背景"><a href="#论文背景" class="headerlink" title="论文背景"></a>论文背景</h2><p>《Wide &amp; Deep Learning for Recommender Systems 》这篇论文是Google于2016年发表在DLRS上的文章，该方法在Google Play的推荐业务中得到了成功的应用。</p><p>在推荐系统中，我们的主要挑战之一就是同时解决Memorization和Generalization，也就是推荐系统的记忆能力和泛化能力。Memorization通过一系列人工的特征叉乘（cross-product）来构造非线性特征，捕捉稀疏特征之间的高阶相关性，能够从历史数据中学习到高频共现的特征组合。例如在CTR预估任务中利用手工构造的交叉组合特征来使线性模型具有“记忆性”，这里的记忆性是指“记忆”历史数据中曾共同出现过的特征对。Generalization为稀疏特征学习低维的稠密嵌入来捕获其中的特征相关性，能够利用特征之间的传递性去探索历史数据中从未出现过的特征组合，学习到的embeddings本身具有一定的语义信息。 </p><p>以上的描述可能比较抽象，类比到我们的大脑认识新事物的过程，起初老师，父母教导我们这个世界的规则，形成对这个世界最初的启蒙，我们知道麻雀会飞，它有一对翅膀，喜鹊也可以飞，因为它也有一对翅膀。但是随着认知的拓展我们又发现并不是有翅膀就可以飞，比如鸵鸟，到这里我们认知的泛化能力产生了局限，我们通过记忆来修正繁华的规则。</p><h2 id="模型结构及原理"><a href="#模型结构及原理" class="headerlink" title="模型结构及原理"></a>模型结构及原理</h2><p><img src="F:%5CMyBlog%5Csource%5C_posts%5Cimage-20210318170410345.png" alt="image-20210318170410345"></p><h3 id="Wide-Models部分"><a href="#Wide-Models部分" class="headerlink" title="Wide Models部分"></a>Wide Models部分</h3><p><img src="C:%5CUsers%5CGu%5CDesktop%5Cteam-learning-rs-master%5CDeepRecommendationModel%5Cimage-20210318183126297.png" alt="image-20210318183126297" style="zoom:80%;"></p><p>Wide是一个泛化的线性模型$y=w^Tx+b$，$y$是我们要预测的结果，$x$是特征，它是一个$d$维的向量$x=[x_1,x_2,\dots,x_d]$。$w$是$d$维的权重向量$w=[w_1,w_2,\cdots,w_d]$，$b$是偏移量。特征包含两个部分，一个是原始数据中直接拿过来的数据，另一种是经过特征转化之后得到的特征。最重要的一种特征转化方式是交叉组合，定义如下：</p><script type="math/tex; mode=display">\phi_k{(x)}=\prod^{d}_{i=1}{ {x_i}^{c_{ki} } },c_{ki}\in\{0,1\}</script><p>这里$c_{ki}$表示的是第$i$个特征的第$k$种转化函数$\phi_k$的结果。对于这个特征转化结果来说，只有所有的项都为真，最终的结果才为1，否则是0。比如“<code>AND(gender=female,language=en)</code>”这就是一个交叉特征，只有当用户的性别为女并且使用的语言是英语时，这个特征的结果才为1。通过这种方式，我们可以捕捉到特征之间的交互。以及为线性模型加入非线性的特征。</p><h2 id="Deep-Models部分"><a href="#Deep-Models部分" class="headerlink" title="Deep Models部分"></a>Deep Models部分</h2><p><img src="https://raw.githubusercontent.com/UPTOLIMIT/PicBed/master//img/image-20210318183236289.png" alt="image-20210318183236289" style="zoom:80%;"></p><p>如上图当中的右侧部分，Deep Models是一个前馈神经网络，它的输入是一个稀疏的特征，这个输入会在神经网络的第一层转化为一个低维度的embedding，维度量级通常在$O(10)$到$O(100)$之间，然后和一些原始的稠密特征一起递交给神经网络训练，这个模块主要被设计用来处理一些类别特征，比如性别，语言等。每一层的隐层计算方式如下：</p><script type="math/tex; mode=display">\alpha^{l+1}=f(W^l\alpha^l+b^l)</script><p>其中$\alpha^l$是第$l$层的激活值，$b^l$是第$l$层的偏置，$W^l$是第$l$层的权重，$f$是激活函数。</p><h2 id="Wide-amp-Deep-Models-联合训练"><a href="#Wide-amp-Deep-Models-联合训练" class="headerlink" title="Wide &amp; Deep Models 联合训练"></a>Wide &amp; Deep Models 联合训练</h2><p><img src="C:%5CUsers%5CGu%5CDesktop%5Cteam-learning-rs-master%5CDeepRecommendationModel%5Cimage-20210318183303549.png" alt="image-20210318183303549" style="zoom:80%;"></p><p>通过加权的方式将Wide部分和Deep部分合并在一起，最上面的输出层是一个sigmoid层，或者是一个线性层，就是一个简单的线性累加，文中成为<code>joint</code>，论文中还降讲到了联合（joint）和集成（ensemble）的区别，集成是每个模型单独训练，再将模型的结果融合，相比于联合训练，集成的每个独立的模型都得学的足够好才有利于随后的回合，因此模型的size也会更大。而在联合训练中，wide部分只需要做一小部分的特征叉乘来弥补deep部分的不足，并不需要一个完整的Wide Models。在集成学习中，每个部分的参数是互不影响的，而在联合学习中，它们的参数是一起训练的。模型选取logistic损失函数，最后的预测输出概率值。公式如下：</p><script type="math/tex; mode=display">P(Y=1|x)=\delta(w_{wide}^T[x,\phi(x)] + w_{deep}^T a^{(lf)} + b)</script><p>其中$\sigma$表示sigmoid函数，$\phi(x)$表示叉乘特征，$\alpha^{(lf)}$表示神经网络最后一层激活值，$b$表示偏置。</p><p>论文中，作者通过梯度的反向传播，使用<code>mini-batch stochastic optimization</code>方法训练参数，并且对wide部分使用带L1正则的<code>FTRL(Follow-the-regularized-leader)</code>算法，对Deep Models部分使用<code>AdaGrad</code>算法。</p><h2 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h2><h3 id="在应用场景中，哪些特征适合放在Wide侧，哪些特征适合放在Deep侧，为什么？"><a href="#在应用场景中，哪些特征适合放在Wide侧，哪些特征适合放在Deep侧，为什么？" class="headerlink" title="在应用场景中，哪些特征适合放在Wide侧，哪些特征适合放在Deep侧，为什么？"></a>在应用场景中，哪些特征适合放在Wide侧，哪些特征适合放在Deep侧，为什么？</h3><p>显然的，直接的，有规律可循的特征适合放在Wide侧，对于一些受上下文影响较大的，简单的规律或许能够反映更大的上下文原因的特征适合放在Deep层。</p><h3 id="为什么Wide部分要用L1-FTRL训练"><a href="#为什么Wide部分要用L1-FTRL训练" class="headerlink" title="为什么Wide部分要用L1 FTRL训练?"></a>为什么Wide部分要用L1 FTRL训练?</h3><p>L1正则化比L2正则化更容易产生稀疏解，FTRL本身是一个稀疏性很好，精度也不错的随机梯度下降方法。L1 FTRL非常注重模型的稀疏性，会让Wide部分的大部分权重都为0，我们无需准备大量0权重特征，大大压缩了模型的权重，也压缩了特征向量的维度。</p><h3 id="为什么Deep部分不特别考虑稀疏性的问题？"><a href="#为什么Deep部分不特别考虑稀疏性的问题？" class="headerlink" title="为什么Deep部分不特别考虑稀疏性的问题？"></a>为什么Deep部分不特别考虑稀疏性的问题？</h3><p>在Deep部分输入类别是数值类特征，或者是已经降维并稠密化的Embedding向量，因此不需要考虑特征稀疏问题。</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>模型部分，特征的选择应该根据实际的业务场景选择哪些特征应该放在wide部分，哪些特征应该放在deep部分</p><pre class=" language-lang-python"><code class="language-lang-python">def WideNDeep(linear_feature_columns, dnn_feature_columns):    # 构建输入层，即所有特征对应的Input()层，这里使用字典的形式返回，方便后续构建模型    dense_input_dict, sparse_input_dict = build_input_layers(linear_feature_columns + dnn_feature_columns)    # 将linear部分的特征中sparse特征筛选出来，后面用来做1维的embedding    linear_sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeat), linear_feature_columns))    # 构建模型的输入层，模型的输入层不能是字典的形式，应该将字典的形式转换成列表的形式    # 注意：这里实际的输入与Input()层的对应，是通过模型输入时候的字典数据的key与对应name的Input层    input_layers = list(dense_input_dict.values()) + list(sparse_input_dict.values())    # Wide&Deep模型论文中Wide部分使用的特征比较简单，并且得到的特征非常的稀疏，所以使用了FTRL优化Wide部分（这里没有实现FTRL）    # 但是是根据他们业务进行选择的，我们这里将所有可能用到的特征都输入到Wide部分，具体的细节可以根据需求进行修改    linear_logits = get_linear_logits(dense_input_dict, sparse_input_dict, linear_sparse_feature_columns)    # 构建维度为k的embedding层，这里使用字典的形式返回，方便后面搭建模型    embedding_layers = build_embedding_layers(dnn_feature_columns, sparse_input_dict, is_linear=False)    dnn_sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeat), dnn_feature_columns))    # 在Wide&Deep模型中，deep部分的输入是将dense特征和embedding特征拼在一起输入到dnn中    dnn_logits = get_dnn_logits(dense_input_dict, sparse_input_dict, dnn_sparse_feature_columns, embedding_layers)    # 将linear,dnn的logits相加作为最终的logits    output_logits = Add()([linear_logits, dnn_logits])    # 这里的激活函数使用sigmoid    output_layer = Activation("sigmoid")(output_logits)    model = Model(input_layers, output_layer)    return model</code></pre><p>模块导入</p><pre class=" language-lang-python"><code class="language-lang-python">import warningswarnings.filterwarnings("ignore")import itertoolsimport pandas as pdimport numpy as npfrom tqdm import tqdmfrom collections import namedtupleimport tensorflow as tffrom tensorflow.keras.layers import *from tensorflow.keras.models import *from sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import  MinMaxScaler, LabelEncoderfrom utils import SparseFeat, DenseFeat, VarLenSparseFeat</code></pre><p>数据预处理</p><pre class=" language-lang-python"><code class="language-lang-python">def data_process(data_df, dense_features, sparse_features):    data_df[dense_features] = data_df[dense_features].fillna(0.0)#填充缺失值    for f in dense_features:# 数据平滑处理        data_df[f] = data_df[f].apply(lambda x: np.log(x+1) if x > -1 else -1)    data_df[sparse_features] = data_df[sparse_features].fillna("-1")#填充缺失值    for f in sparse_features:# 类别编码        lbe = LabelEncoder()        data_df[f] = lbe.fit_transform(data_df[f])    return data_df[dense_features + sparse_features]</code></pre><p>构建输入层并以dense和sparse两类字典的形式返回</p><pre class=" language-lang-python"><code class="language-lang-python">def build_input_layers(feature_columns):    dense_input_dict, sparse_input_dict = {}, {}    for fc in feature_columns:        if isinstance(fc, SparseFeat):            sparse_input_dict[fc.name] = Input(shape=(1, ), name=fc.name)        elif isinstance(fc, DenseFeat):            dense_input_dict[fc.name] = Input(shape=(fc.dimension, ), name=fc.name)    return dense_input_dict, sparse_input_dict</code></pre><p>构建Embedding层并以字典形式返回</p><pre class=" language-lang-python"><code class="language-lang-python">def build_embedding_layers(feature_columns, input_layers_dict, is_linear):    embedding_layers_dict = dict()    # 将特征中的sparse特征筛选出来    sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeat), feature_columns)) if feature_columns else []    # 如果是用于线性部分的embedding层，其维度为1，否则维度就是自己定义的embedding维度    if is_linear:        for fc in sparse_feature_columns:            embedding_layers_dict[fc.name] = Embedding(fc.vocabulary_size, 1, name='1d_emb_' + fc.name)    else:        for fc in sparse_feature_columns:            embedding_layers_dict[fc.name] = Embedding(fc.vocabulary_size, fc.embedding_dim, name='kd_emb_' + fc.name)    return embedding_layers_dict</code></pre><pre class=" language-lang-python"><code class="language-lang-python">def get_linear_logits(dense_input_dict, sparse_input_dict, sparse_feature_columns):    # 将所有的dense特征的Input层，经过一个全连接层得到dense特征的logits    concat_dense_inputs = Concatenate(axis=1)(list(dense_input_dict.values()))    dense_logits_output = Dense(1)(concat_dense_inputs)    # 获取linear部分sparse特征的embedding层，这里使用embedding的原因是：    # 对于linear部分直接将特征进行onehot然后通过一个全连接层，当维度特别大的时候，计算比较慢    # 使用embedding层的好处就是可以通过查表的方式获取到哪些非零的元素对应的权重，然后在将这些权重相加，效率比较高    linear_embedding_layers = build_embedding_layers(sparse_feature_columns, sparse_input_dict, is_linear=True)    # 将一维的embedding拼接，注意这里需要使用一个Flatten层，使维度对应    sparse_1d_embed = []    for fc in sparse_feature_columns:        feat_input = sparse_input_dict[fc.name]        embed = Flatten()(linear_embedding_layers[fc.name](feat_input)) # B x 1        sparse_1d_embed.append(embed)    # embedding中查询得到的权重就是对应onehot向量中一个位置的权重，所以后面不用再接一个全连接了，本身一维的embedding就相当于全连接    # 只不过是这里的输入特征只有0和1，所以直接向非零元素对应的权重相加就等同于进行了全连接操作(非零元素部分乘的是1)    sparse_logits_output = Add()(sparse_1d_embed)    # 最终将dense特征和sparse特征对应的logits相加，得到最终linear的logits    linear_logits = Add()([dense_logits_output, sparse_logits_output])    return linear_logits</code></pre><p>将所有的稀疏特征的embedding拼接</p><pre class=" language-lang-python"><code class="language-lang-python">def concat_embedding_list(feature_columns, input_layer_dict, embedding_layer_dict, flatten=False):    # 将sparse特征筛选出来    sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeat), feature_columns))    embedding_list = []    for fc in sparse_feature_columns:        _input = input_layer_dict[fc.name] # 获取输入层         _embed = embedding_layer_dict[fc.name] # B x 1 x dim  获取对应的embedding层        embed = _embed(_input) # B x dim  将input层输入到embedding层中        # 是否需要flatten, 如果embedding列表最终是直接输入到Dense层中，需要进行Flatten，否则不需要        if flatten:            embed = Flatten()(embed)        embedding_list.append(embed)    return embedding_list</code></pre><p>构建深度神经网络</p><pre class=" language-lang-python"><code class="language-lang-python">def get_dnn_logits(dense_input_dict, sparse_input_dict, sparse_feature_columns, dnn_embedding_layers):    concat_dense_inputs = Concatenate(axis=1)(list(dense_input_dict.values())) # B x n1 (n1表示的是dense特征的维度)     sparse_kd_embed = concat_embedding_list(sparse_feature_columns, sparse_input_dict, dnn_embedding_layers, flatten=True)    concat_sparse_kd_embed = Concatenate(axis=1)(sparse_kd_embed) # B x n2  (n2表示的是Sparse特征的维度)    dnn_input = Concatenate(axis=1)([concat_dense_inputs, concat_sparse_kd_embed]) # B x (n2 + n1)    # dnn层，这里的Dropout参数，Dense中的参数及Dense的层数都可以自己设定    dnn_out = Dropout(0.5)(Dense(1024, activation='relu')(dnn_input))      dnn_out = Dropout(0.3)(Dense(512, activation='relu')(dnn_out))    dnn_out = Dropout(0.1)(Dense(256, activation='relu')(dnn_out))    dnn_logits = Dense(1)(dnn_out)    return dnn_logits</code></pre><p>main函数入口</p><pre class=" language-lang-python"><code class="language-lang-python">if __name__ == "__main__":    # 读取数据    data = pd.read_csv('./data/criteo_sample.txt')    # 划分dense和sparse特征    columns = data.columns.values    dense_features = [feat for feat in columns if 'I' in feat]    sparse_features = [feat for feat in columns if 'C' in feat]    # 简单的数据预处理    train_data = data_process(data, dense_features, sparse_features)    train_data['label'] = data['label']    # 将特征分组，分成linear部分和dnn部分(根据实际场景进行选择)，并将分组之后的特征做标记（使用DenseFeat, SparseFeat）    linear_feature_columns = [SparseFeat(feat, vocabulary_size=data[feat].nunique(),embedding_dim=4)                            for i,feat in enumerate(sparse_features)] + [DenseFeat(feat, 1,)                            for feat in dense_features]    dnn_feature_columns = [SparseFeat(feat, vocabulary_size=data[feat].nunique(),embedding_dim=4)                            for i,feat in enumerate(sparse_features)] + [DenseFeat(feat, 1,)                            for feat in dense_features]    # 构建WideNDeep模型    history = WideNDeep(linear_feature_columns, dnn_feature_columns)    history.summary()    history.compile(optimizer="adam",                 loss="binary_crossentropy",                 metrics=["binary_crossentropy", tf.keras.metrics.AUC(name='auc')])    # 将输入数据转化成字典的形式输入    train_model_input = {name: data[name] for name in dense_features + sparse_features}    # 模型训练    history.fit(train_model_input, train_data['label'].values,            batch_size=64, epochs=5, validation_split=0.2, )</code></pre><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://blog.csdn.net/anshuai_aw1/article/details/105113980">推荐系统（一）：Wide &amp; Deep 源论文整理和思考</a></li><li><a href="https://zhuanlan.zhihu.com/p/334991736">巨经典论文！详解推荐系统经典模型 Wide &amp; Deep</a></li><li><a href="https://zhuanlan.zhihu.com/p/53361519">详解 Wide &amp; Deep 结构背后的动机</a></li><li><a href="https://arxiv.org/abs/1606.07792">Wide &amp; Deep Learning for Recommender Systems</a></li><li><a href="https://zhuanlan.zhihu.com/p/142958834">见微知著，你真的搞懂 Google 的 Wide&amp;Deep 模型了吗？</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 深度推荐系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 推荐系统 </tag>
            
            <tag> DNN </tag>
            
            <tag> 特征工程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DeepCrossing理论与实践</title>
      <link href="2021/03/21/deepcrossing-li-lun-yu-shi-jian/"/>
      <url>2021/03/21/deepcrossing-li-lun-yu-shi-jian/</url>
      
        <content type="html"><![CDATA[<hr><h1 id="论文背景"><a href="#论文背景" class="headerlink" title="论文背景"></a>论文背景</h1><p>DeepCrossing是2016年由微软提出，旨在特征工程中减少人力特征组合的时间开销，通过模型自动学习特征的组合方式，解决特征组合的难题。是第一个企业以正式论文的形式分享推荐系统技术细节的模型。</p><h1 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h1><p><img src="https://raw.githubusercontent.com/UPTOLIMIT/PicBed/master//img/image-20210315215320120.png" alt="image-20210315215320120" style="zoom: 67%;"></p><p>模型的输入是一系列独立的特征，输出是用户的点击率预测值。模型一共分为4层，分别是Embedding层，Stacking层，Multiple Residual Units层，Scoring层。</p><h2 id="Embedding层"><a href="#Embedding层" class="headerlink" title="Embedding层"></a>Embedding层</h2><p>Embedding这个概念最初来源于Manifold Hypothesis（<a href="https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/">流形假设</a>）。流形假设是指<a href="https://www.zhihu.com/question/38002635">“自然的原始数据是低维的流形嵌入于(embedded in)原始数据所在的高维空间”</a>，而深度学习的任务就是把高维原始数据映射到低维流形，使得原始数据变得可分，而这个映射就叫做embedding（嵌入），比如word embedding（词嵌入）就是把单词组成的句子映射到一个表征向量。久而久之，把低维流形的表征向量叫做Embedding成为一种习惯（其实是一种误用）。在本文中，Embedding 层的作用就是将从原始独立特征中提取出来的高维稀疏特征向量转换成低维稠密的Embedding向量。</p><pre class=" language-lang-python"><code class="language-lang-python">def build_embedding_layers(feature_columns, input_layers_dict, is_linear):    # 定义一个embedding层对应的字典    embedding_layers_dict = dict()    # 将特征中的sparse特征筛选出来    sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeat), feature_columns)) if feature_columns else []    # 如果是用于线性部分的embedding层，其维度为1，否则维度就是自己定义的embedding维度    if is_linear:        for fc in sparse_feature_columns:            embedding_layers_dict[fc.name] = Embedding(fc.vocabulary_size + 1, 1, name='1d_emb_' + fc.name)    else:        for fc in sparse_feature_columns:            embedding_layers_dict[fc.name] = Embedding(fc.vocabulary_size + 1, fc.embedding_dim, name='kd_emb_' + fc.name)    return embedding_layers_dict</code></pre><h2 id="Stacking层"><a href="#Stacking层" class="headerlink" title="Stacking层"></a>Stacking层</h2><p>得到了所有特征的Embedding表示之后，Stacking层接收Embedding层的输入并且把这些特征聚合起来，形成一个向量</p><pre class=" language-lang-python"><code class="language-lang-python"># 将所有的sparse特征embedding拼接def concat_embedding_list(feature_columns, input_layer_dict, embedding_layer_dict, flatten=False):    # 将sparse特征筛选出来    sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeat), feature_columns))    embedding_list = []    for fc in sparse_feature_columns:        _input = input_layer_dict[fc.name] # 获取输入层         _embed = embedding_layer_dict[fc.name] # B x 1 x dim  获取对应的embedding层        embed = _embed(_input) # B x dim  将input层输入到embedding层中        # 是否需要flatten, 如果embedding列表最终是直接输入到Dense层中，需要进行Flatten，否则不需要        if flatten:            embed = Flatten()(embed)        embedding_list.append(embed)    return embedding_list</code></pre><h2 id="Multiple-Residual-Units层"><a href="#Multiple-Residual-Units层" class="headerlink" title="Multiple Residual Units层"></a>Multiple Residual Units层</h2><p><img src="F:%5CMyBlog%5Csource%5C_posts%5Cimage-20210316193133492.png" alt="image-20210316193133492"></p><p>DeepCrossing中的Crossing就是通过多个残差单元来实现的，残差单元的结构如上图所示。与ResNet不同的是，它不包含卷积操作，也是第一次在图像识别之外应用残差单元。</p><script type="math/tex; mode=display">X^O=F(X^I,\{W_0,W_1\},\{b_0,b_1\})+X^I</script><p>将$X^I$移动到左侧,可以看出$F$函数拟合的是输入和输出之间的残差。对输入进行全连接变换之后，经过<code>relu</code>激活函数送入第二个全连接层，将输出结果与原始输入进行<code>element-wise add</code>操作，再经过<code>relu</code>激活输出，实验表明，残差结构能更敏感的捕获输入输出之间的信息差。</p><pre class=" language-lang-python"><code class="language-lang-python"># DNN残差块的定义class ResidualBlock(Layer):    def __init__(self, units): # units表示的是DNN隐藏层神经元数量        super(ResidualBlock, self).__init__()        self.units = units    def build(self, input_shape):        out_dim = input_shape[-1]        self.dnn1 = Dense(self.units, activation='relu')        self.dnn2 = Dense(out_dim, activation='relu') # 保证输入的维度和输出的维度一致才能进行残差连接    def call(self, inputs):        x = inputs        x = self.dnn1(x)        x = self.dnn2(x)        x = Activation('relu')(x + inputs) # 残差操作        return x</code></pre><h2 id="Scoring层"><a href="#Scoring层" class="headerlink" title="Scoring层"></a>Scoring层</h2><p>Scoring层作为输出层，输出广告预测点击率，是为了拟合优化目标而存在，对于二分类问题，往往使用逻辑回归，模型通过叠加多个残差块加深网络的深度，最后将结果转换为一个概率值输出。</p><pre class=" language-lang-python"><code class="language-lang-python"># block_nums表示DNN残差块的数量，通过叠加残差块的数量可以加深网络深度def get_dnn_logits(dnn_inputs, block_nums=3):    dnn_out = dnn_inputs    for i in range(block_nums):        dnn_out = ResidualBlock(64)(dnn_out)        # 这里使用sigmoid激活函数，将dnn的输出转化成logits        dnn_logits = Dense(1, activation='sigmoid')(dnn_out)        return dnn_logits</code></pre><h2 id="代码解读"><a href="#代码解读" class="headerlink" title="代码解读"></a>代码解读</h2><p>从<code>main</code>函数开始，按照代码执行顺序，阅读代码。</p><pre class=" language-lang-python"><code class="language-lang-python"># 读取数据data = pd.read_csv('./data/criteo_sample.txt')# 划分dense和sparse特征，即密集特征和稀疏特征，观察原数据集可以发现是按列分好的columns = data.columns.valuesdense_features = [feat for feat in columns if 'I' in feat]sparse_features = [feat for feat in columns if 'C' in feat]</code></pre><pre class=" language-lang-python"><code class="language-lang-python"> # 简单的数据预处理    train_data = data_process(data, dense_features, sparse_features)    train_data['label'] = data['label']</code></pre><pre class=" language-lang-python"><code class="language-lang-python"># 数据预处理函数def data_process(data_df, dense_features, sparse_features):    """    简单处理特征，包括填充缺失值，数值处理，类别编码    param data_df: DataFrame格式的数据    param dense_features: 数值特征名称列表    param sparse_features: 类别特征名称列表    """    data_df[dense_features] = data_df[dense_features].fillna(0.0)    # 使用log函数对数据进行平滑处理    for f in dense_features:        data_df[f] = data_df[f].apply(lambda x: np.log(x+1) if x > -1 else -1)    # 将缺失值填充为-1      data_df[sparse_features] = data_df[sparse_features].fillna("-1")    # 将类别型数据one-hot编码    for f in sparse_features:        lbe = LabelEncoder()        data_df[f] = lbe.fit_transform(data_df[f])    return data_df[dense_features + sparse_features]</code></pre><pre class=" language-lang-PYTHON"><code class="language-lang-PYTHON"># 将特征做标记dnn_feature_columns = [SparseFeat(feat, vocabulary_size=data[feat].nunique(),embedding_dim=4)                            for feat in sparse_features] + [DenseFeat(feat, 1,)                            for feat in dense_features]</code></pre><p>这里用到了python的一个工厂函数<code>namedtuple()</code>可以译为<code>具名元组</code>，可以理解为元组的增强版本，为元组中的每一个元素赋予了含义，从而增强代码可读性。</p><pre class=" language-lang-python"><code class="language-lang-python">SparseFeat = namedtuple('SparseFeat', ['name', 'vocabulary_size', 'embedding_dim'])</code></pre><pre class=" language-lang-python"><code class="language-lang-python"># 构建DeepCrossing模型    history = DeepCrossing(dnn_feature_columns)</code></pre><p>DeepCrossing模型代码如下：</p><pre class=" language-lang-python"><code class="language-lang-python">def DeepCrossing(dnn_feature_columns):    # 构建输入层，即所有特征对应的Input()层，这里使用字典的形式返回，方便后续构建模型    dense_input_dict, sparse_input_dict = build_input_layers(dnn_feature_columns)    # 构建模型的输入层，模型的输入层不能是字典的形式，应该将字典的形式转换成列表的形式    # 注意：这里实际的输入与Input()层的对应，是通过模型输入时候的字典数据的key与对应name的Input层    input_layers = list(dense_input_dict.values()) + list(sparse_input_dict.values())    # 构建维度为k的embedding层，这里使用字典的形式返回，方便后面搭建模型    embedding_layer_dict = build_embedding_layers(dnn_feature_columns, sparse_input_dict, is_linear=False)    #将所有的dense特征拼接到一起，B为Batchsize    dense_dnn_list = list(dense_input_dict.values())    dense_dnn_inputs = Concatenate(axis=1)(dense_dnn_list) # B x n (n表示数值特征的数量)    # 因为需要将其与dense特征拼接到一起所以需要Flatten，不进行Flatten的Embedding层输出的维度为：Bx1xdim    sparse_dnn_list = concat_embedding_list(dnn_feature_columns, sparse_input_dict, embedding_layer_dict, flatten=True)     sparse_dnn_inputs = Concatenate(axis=1)(sparse_dnn_list) # B x m*dim (n表示类别特征的数量，dim表示embedding的维度)    # 将dense特征和Sparse特征拼接到一起    dnn_inputs = Concatenate(axis=1)([dense_dnn_inputs, sparse_dnn_inputs]) # B x (n + m*dim)    # 输入到dnn中，需要提前定义需要几个残差块    output_layer = get_dnn_logits(dnn_inputs, block_nums=3)    model = Model(input_layers, output_layer)    return model</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>DeepCrossing的结构比较简单清晰，没有引入特殊的模型结构，只是常规的Embedding+多层神经网络。但是在DeepCrossing中没有任何人工特征工程的参与，只需要简单的特征处理，原始特征经过Embedding层输入神经网络层，自主交叉和学习，通过调整神经网络的深度进行特征之间的深度交叉，也是DeepCrossing名称的由来（大悟）。</p><p>最后附上论文<a href="https://link.zhihu.com/?target=http%3A//www.kdd.org/kdd2016/papers/files/adf0975-shanA.pdf">链接</a></p>]]></content>
      
      
      <categories>
          
          <category> 深度推荐系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 推荐系统 </tag>
            
            <tag> DNN </tag>
            
            <tag> 特征工程 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
