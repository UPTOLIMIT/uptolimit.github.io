<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>DeepCrossing理论与实践</title>
      <link href="2021/03/21/deepcrossing-li-lun-yu-shi-jian/"/>
      <url>2021/03/21/deepcrossing-li-lun-yu-shi-jian/</url>
      
        <content type="html"><![CDATA[<hr><h1 id="论文背景"><a href="#论文背景" class="headerlink" title="论文背景"></a>论文背景</h1><p>DeepCrossing是2016年由微软提出，旨在特征工程中减少人力特征组合的时间开销，通过模型自动学习特征的组合方式，解决特征组合的难题。是第一个企业以正式论文的形式分享推荐系统技术细节的模型。</p><h1 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h1><img src="https://raw.githubusercontent.com/UPTOLIMIT/PicBed/master//img/image-20210315215320120.png" alt="image-20210315215320120" style="zoom: 67%;"><p>模型的输入是一系列独立的特征，输出是用户的点击率预测值。模型一共分为4层，分别是Embedding层，Stacking层，Multiple Residual Units层，Scoring层。</p><h2 id="Embedding层"><a href="#Embedding层" class="headerlink" title="Embedding层"></a>Embedding层</h2><p>Embedding这个概念最初来源于Manifold Hypothesis（<a href="https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/">流形假设</a>）。流形假设是指<a href="https://www.zhihu.com/question/38002635">“自然的原始数据是低维的流形嵌入于(embedded in)原始数据所在的高维空间”</a>，而深度学习的任务就是把高维原始数据映射到低维流形，使得原始数据变得可分，而这个映射就叫做embedding（嵌入），比如word embedding（词嵌入）就是把单词组成的句子映射到一个表征向量。久而久之，把低维流形的表征向量叫做Embedding成为一种习惯（其实是一种误用）。在本文中，Embedding 层的作用就是将从原始独立特征中提取出来的高维稀疏特征向量转换成低维稠密的Embedding向量。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_embedding_layers</span>(<span class="params">feature_columns, input_layers_dict, is_linear</span>):</span></span><br><span class="line">    <span class="comment"># 定义一个embedding层对应的字典</span></span><br><span class="line">    embedding_layers_dict = <span class="built_in">dict</span>()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将特征中的sparse特征筛选出来</span></span><br><span class="line">    sparse_feature_columns = <span class="built_in">list</span>(<span class="built_in">filter</span>(<span class="keyword">lambda</span> x: <span class="built_in">isinstance</span>(x, SparseFeat), feature_columns)) <span class="keyword">if</span> feature_columns <span class="keyword">else</span> []</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 如果是用于线性部分的embedding层，其维度为1，否则维度就是自己定义的embedding维度</span></span><br><span class="line">    <span class="keyword">if</span> is_linear:</span><br><span class="line">        <span class="keyword">for</span> fc <span class="keyword">in</span> sparse_feature_columns:</span><br><span class="line">            embedding_layers_dict[fc.name] = Embedding(fc.vocabulary_size + <span class="number">1</span>, <span class="number">1</span>, name=<span class="string">'1d_emb_'</span> + fc.name)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">for</span> fc <span class="keyword">in</span> sparse_feature_columns:</span><br><span class="line">            embedding_layers_dict[fc.name] = Embedding(fc.vocabulary_size + <span class="number">1</span>, fc.embedding_dim, name=<span class="string">'kd_emb_'</span> + fc.name)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> embedding_layers_dict</span><br></pre></td></tr></tbody></table></figure><h2 id="Stacking层"><a href="#Stacking层" class="headerlink" title="Stacking层"></a>Stacking层</h2><p>得到了所有特征的Embedding表示之后，Stacking层接收Embedding层的输入并且把这些特征聚合起来，形成一个向量</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将所有的sparse特征embedding拼接</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">concat_embedding_list</span>(<span class="params">feature_columns, input_layer_dict, embedding_layer_dict, flatten=<span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="comment"># 将sparse特征筛选出来</span></span><br><span class="line">    sparse_feature_columns = <span class="built_in">list</span>(<span class="built_in">filter</span>(<span class="keyword">lambda</span> x: <span class="built_in">isinstance</span>(x, SparseFeat), feature_columns))</span><br><span class="line"></span><br><span class="line">    embedding_list = []</span><br><span class="line">    <span class="keyword">for</span> fc <span class="keyword">in</span> sparse_feature_columns:</span><br><span class="line">        _input = input_layer_dict[fc.name] <span class="comment"># 获取输入层 </span></span><br><span class="line">        _embed = embedding_layer_dict[fc.name] <span class="comment"># B x 1 x dim  获取对应的embedding层</span></span><br><span class="line">        embed = _embed(_input) <span class="comment"># B x dim  将input层输入到embedding层中</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 是否需要flatten, 如果embedding列表最终是直接输入到Dense层中，需要进行Flatten，否则不需要</span></span><br><span class="line">        <span class="keyword">if</span> flatten:</span><br><span class="line">            embed = Flatten()(embed)</span><br><span class="line">        </span><br><span class="line">        embedding_list.append(embed)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> embedding_list </span><br></pre></td></tr></tbody></table></figure><h2 id="Multiple-Residual-Units层"><a href="#Multiple-Residual-Units层" class="headerlink" title="Multiple Residual Units层"></a>Multiple Residual Units层</h2><p><img src="F:%5CMyBlog%5Csource%5C_posts%5Cimage-20210316193133492.png" alt="image-20210316193133492"></p><p>DeepCrossing中的Crossing就是通过多个残差单元来实现的，残差单元的结构如上图所示。与ResNet不同的是，它不包含卷积操作，也是第一次在图像识别之外应用残差单元。<br>$$<br>X^O=F(X^I,{W_0,W_1},{b_0,b_1})+X^I<br>$$<br>将$X^I$移动到左侧,可以看出$F$函数拟合的是输入和输出之间的残差。对输入进行全连接变换之后，经过<code>relu</code>激活函数送入第二个全连接层，将输出结果与原始输入进行<code>element-wise add</code>操作，再经过<code>relu</code>激活输出，实验表明，残差结构能更敏感的捕获输入输出之间的信息差。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># DNN残差块的定义</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResidualBlock</span>(<span class="params">Layer</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, units</span>):</span> <span class="comment"># units表示的是DNN隐藏层神经元数量</span></span><br><span class="line">        <span class="built_in">super</span>(ResidualBlock, self).__init__()</span><br><span class="line">        self.units = units</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span>(<span class="params">self, input_shape</span>):</span></span><br><span class="line">        out_dim = input_shape[-<span class="number">1</span>]</span><br><span class="line">        self.dnn1 = Dense(self.units, activation=<span class="string">'relu'</span>)</span><br><span class="line">        self.dnn2 = Dense(out_dim, activation=<span class="string">'relu'</span>) <span class="comment"># 保证输入的维度和输出的维度一致才能进行残差连接</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        x = inputs</span><br><span class="line">        x = self.dnn1(x)</span><br><span class="line">        x = self.dnn2(x)</span><br><span class="line">        x = Activation(<span class="string">'relu'</span>)(x + inputs) <span class="comment"># 残差操作</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><h2 id="Scoring层"><a href="#Scoring层" class="headerlink" title="Scoring层"></a>Scoring层</h2><p>Scoring层作为输出层，输出广告预测点击率，是为了拟合优化目标而存在，对于二分类问题，往往使用逻辑回归，模型通过叠加多个残差块加深网络的深度，最后将结果转换为一个概率值输出。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># block_nums表示DNN残差块的数量，通过叠加残差块的数量可以加深网络深度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_dnn_logits</span>(<span class="params">dnn_inputs, block_nums=<span class="number">3</span></span>):</span></span><br><span class="line">    dnn_out = dnn_inputs</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(block_nums):</span><br><span class="line">        dnn_out = ResidualBlock(<span class="number">64</span>)(dnn_out)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 这里使用sigmoid激活函数，将dnn的输出转化成logits</span></span><br><span class="line">        dnn_logits = Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>)(dnn_out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dnn_logits</span><br></pre></td></tr></tbody></table></figure><h2 id="代码解读"><a href="#代码解读" class="headerlink" title="代码解读"></a>代码解读</h2><p>从<code>main</code>函数开始，按照代码执行顺序，阅读代码。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line">data = pd.read_csv(<span class="string">'./data/criteo_sample.txt'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分dense和sparse特征，即密集特征和稀疏特征，观察原数据集可以发现是按列分好的</span></span><br><span class="line">columns = data.columns.values</span><br><span class="line">dense_features = [feat <span class="keyword">for</span> feat <span class="keyword">in</span> columns <span class="keyword">if</span> <span class="string">'I'</span> <span class="keyword">in</span> feat]</span><br><span class="line">sparse_features = [feat <span class="keyword">for</span> feat <span class="keyword">in</span> columns <span class="keyword">if</span> <span class="string">'C'</span> <span class="keyword">in</span> feat]</span><br></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 简单的数据预处理</span></span><br><span class="line">   train_data = data_process(data, dense_features, sparse_features)</span><br><span class="line">   train_data[<span class="string">'label'</span>] = data[<span class="string">'label'</span>]</span><br></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据预处理函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_process</span>(<span class="params">data_df, dense_features, sparse_features</span>):</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    简单处理特征，包括填充缺失值，数值处理，类别编码</span></span><br><span class="line"><span class="string">    param data_df: DataFrame格式的数据</span></span><br><span class="line"><span class="string">    param dense_features: 数值特征名称列表</span></span><br><span class="line"><span class="string">    param sparse_features: 类别特征名称列表</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    data_df[dense_features] = data_df[dense_features].fillna(<span class="number">0.0</span>)</span><br><span class="line">    <span class="comment"># 使用log函数对数据进行平滑处理</span></span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> dense_features:</span><br><span class="line">        data_df[f] = data_df[f].apply(<span class="keyword">lambda</span> x: np.log(x+<span class="number">1</span>) <span class="keyword">if</span> x &gt; -<span class="number">1</span> <span class="keyword">else</span> -<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 将缺失值填充为-1  </span></span><br><span class="line">    data_df[sparse_features] = data_df[sparse_features].fillna(<span class="string">"-1"</span>)</span><br><span class="line">    <span class="comment"># 将类别型数据one-hot编码</span></span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> sparse_features:</span><br><span class="line">        lbe = LabelEncoder()</span><br><span class="line">        data_df[f] = lbe.fit_transform(data_df[f])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> data_df[dense_features + sparse_features]</span><br></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将特征做标记</span></span><br><span class="line"></span><br><span class="line">dnn_feature_columns = [SparseFeat(feat, vocabulary_size=data[feat].nunique(),embedding_dim=<span class="number">4</span>)</span><br><span class="line">                            <span class="keyword">for</span> feat <span class="keyword">in</span> sparse_features] + [DenseFeat(feat, <span class="number">1</span>,)</span><br><span class="line">                            <span class="keyword">for</span> feat <span class="keyword">in</span> dense_features]</span><br></pre></td></tr></tbody></table></figure><p>这里用到了python的一个工厂函数<code>namedtuple()</code>可以译为<code>具名元组</code>，可以理解为元组的增强版本，为元组中的每一个元素赋予了含义，从而增强代码可读性。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SparseFeat = namedtuple(<span class="string">'SparseFeat'</span>, [<span class="string">'name'</span>, <span class="string">'vocabulary_size'</span>, <span class="string">'embedding_dim'</span>])</span><br></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建DeepCrossing模型</span></span><br><span class="line">    history = DeepCrossing(dnn_feature_columns)</span><br></pre></td></tr></tbody></table></figure><p>DeepCrossing模型代码如下：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">DeepCrossing</span>(<span class="params">dnn_feature_columns</span>):</span></span><br><span class="line">    <span class="comment"># 构建输入层，即所有特征对应的Input()层，这里使用字典的形式返回，方便后续构建模型</span></span><br><span class="line">    dense_input_dict, sparse_input_dict = build_input_layers(dnn_feature_columns)</span><br><span class="line">    <span class="comment"># 构建模型的输入层，模型的输入层不能是字典的形式，应该将字典的形式转换成列表的形式</span></span><br><span class="line">    <span class="comment"># 注意：这里实际的输入与Input()层的对应，是通过模型输入时候的字典数据的key与对应name的Input层</span></span><br><span class="line">    input_layers = <span class="built_in">list</span>(dense_input_dict.values()) + <span class="built_in">list</span>(sparse_input_dict.values())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 构建维度为k的embedding层，这里使用字典的形式返回，方便后面搭建模型</span></span><br><span class="line">    embedding_layer_dict = build_embedding_layers(dnn_feature_columns, sparse_input_dict, is_linear=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#将所有的dense特征拼接到一起，B为Batchsize</span></span><br><span class="line">    dense_dnn_list = <span class="built_in">list</span>(dense_input_dict.values())</span><br><span class="line">    dense_dnn_inputs = Concatenate(axis=<span class="number">1</span>)(dense_dnn_list) <span class="comment"># B x n (n表示数值特征的数量)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 因为需要将其与dense特征拼接到一起所以需要Flatten，不进行Flatten的Embedding层输出的维度为：Bx1xdim</span></span><br><span class="line">    sparse_dnn_list = concat_embedding_list(dnn_feature_columns, sparse_input_dict, embedding_layer_dict, flatten=<span class="literal">True</span>) </span><br><span class="line"></span><br><span class="line">    sparse_dnn_inputs = Concatenate(axis=<span class="number">1</span>)(sparse_dnn_list) <span class="comment"># B x m*dim (n表示类别特征的数量，dim表示embedding的维度)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将dense特征和Sparse特征拼接到一起</span></span><br><span class="line">    dnn_inputs = Concatenate(axis=<span class="number">1</span>)([dense_dnn_inputs, sparse_dnn_inputs]) <span class="comment"># B x (n + m*dim)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输入到dnn中，需要提前定义需要几个残差块</span></span><br><span class="line">    output_layer = get_dnn_logits(dnn_inputs, block_nums=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    model = Model(input_layers, output_layer)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>DeepCrossing的结构比较简单清晰，没有引入特殊的模型结构，只是常规的Embedding+多层神经网络。但是在DeepCrossing中没有任何人工特征工程的参与，只需要简单的特征处理，原始特征经过Embedding层输入神经网络层，自主交叉和学习，通过调整神经网络的深度进行特征之间的深度交叉，也是DeepCrossing名称的由来（大悟）。</p><p>最后附上论文<a href="https://link.zhihu.com/?target=http://www.kdd.org/kdd2016/papers/files/adf0975-shanA.pdf">链接</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 推荐系统 DNN 特征工程 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
