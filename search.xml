<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Wide&amp;Deep论文解析与代码实现</title>
      <link href="2021/03/21/wide-deep-lun-wen-jie-xi-yu-dai-ma-shi-xian/"/>
      <url>2021/03/21/wide-deep-lun-wen-jie-xi-yu-dai-ma-shi-xian/</url>
      
        <content type="html"><![CDATA[<h2 id="论文背景"><a href="#论文背景" class="headerlink" title="论文背景"></a>论文背景</h2><p>《Wide &amp; Deep Learning for Recommender Systems 》这篇论文是Google于2016年发表在DLRS上的文章，该方法在Google Play的推荐业务中得到了成功的应用。</p><p>在推荐系统中，我们的主要挑战之一就是同时解决Memorization和Generalization，也就是推荐系统的记忆能力和泛化能力。Memorization通过一系列人工的特征叉乘（cross-product）来构造非线性特征，捕捉稀疏特征之间的高阶相关性，能够从历史数据中学习到高频共现的特征组合。例如在CTR预估任务中利用手工构造的交叉组合特征来使线性模型具有“记忆性”，这里的记忆性是指“记忆”历史数据中曾共同出现过的特征对。Generalization为稀疏特征学习低维的稠密嵌入来捕获其中的特征相关性，能够利用特征之间的传递性去探索历史数据中从未出现过的特征组合，学习到的embeddings本身具有一定的语义信息。 </p><p>以上的描述可能比较抽象，类比到我们的大脑认识新事物的过程，起初老师，父母教导我们这个世界的规则，形成对这个世界最初的启蒙，我们知道麻雀会飞，它有一对翅膀，喜鹊也可以飞，因为它也有一对翅膀。但是随着认知的拓展我们又发现并不是有翅膀就可以飞，比如鸵鸟，到这里我们认知的泛化能力产生了局限，我们通过记忆来修正繁华的规则。</p><h2 id="模型结构及原理"><a href="#模型结构及原理" class="headerlink" title="模型结构及原理"></a>模型结构及原理</h2><p><img src="F:%5CMyBlog%5Csource%5C_posts%5Cimage-20210318170410345.png" alt="image-20210318170410345"></p><h3 id="Wide-Models部分"><a href="#Wide-Models部分" class="headerlink" title="Wide Models部分"></a>Wide Models部分</h3><img src="C:%5CUsers%5CGu%5CDesktop%5Cteam-learning-rs-master%5CDeepRecommendationModel%5Cimage-20210318183126297.png" alt="image-20210318183126297" style="zoom:80%;"><p>Wide是一个泛化的线性模型$y=w^Tx+b$，$y$是我们要预测的结果，$x$是特征，它是一个$d$维的向量$x=[x_1,x_2,\dots,x_d]$。$w$是$d$维的权重向量$w=[w_1,w_2,\cdots,w_d]$，$b$是偏移量。特征包含两个部分，一个是原始数据中直接拿过来的数据，另一种是经过特征转化之后得到的特征。最重要的一种特征转化方式是交叉组合，定义如下：<br>$$<br>\phi_k{(x)}=\prod^{d}<em>{i=1}{ {x_i}^{c</em>{ki} } },c_{ki}\in{0,1}<br>$$<br>这里$c_{ki}$表示的是第$i$个特征的第$k$种转化函数$\phi_k$的结果。对于这个特征转化结果来说，只有所有的项都为真，最终的结果才为1，否则是0。比如“<code>AND(gender=female,language=en)</code>”这就是一个交叉特征，只有当用户的性别为女并且使用的语言是英语时，这个特征的结果才为1。通过这种方式，我们可以捕捉到特征之间的交互。以及为线性模型加入非线性的特征。</p><h2 id="Deep-Models部分"><a href="#Deep-Models部分" class="headerlink" title="Deep Models部分"></a>Deep Models部分</h2><img src="https://raw.githubusercontent.com/UPTOLIMIT/PicBed/master//img/image-20210318183236289.png" alt="image-20210318183236289" style="zoom:80%;"><p>如上图当中的右侧部分，Deep Models是一个前馈神经网络，它的输入是一个稀疏的特征，这个输入会在神经网络的第一层转化为一个低维度的embedding，维度量级通常在$O(10)$到$O(100)$之间，然后和一些原始的稠密特征一起递交给神经网络训练，这个模块主要被设计用来处理一些类别特征，比如性别，语言等。每一层的隐层计算方式如下：<br>$$<br>\alpha^{l+1}=f(W^l\alpha^l+b^l)<br>$$<br>其中$\alpha^l$是第$l$层的激活值，$b^l$是第$l$层的偏置，$W^l$是第$l$层的权重，$f$是激活函数。</p><h2 id="Wide-amp-Deep-Models-联合训练"><a href="#Wide-amp-Deep-Models-联合训练" class="headerlink" title="Wide &amp; Deep Models 联合训练"></a>Wide &amp; Deep Models 联合训练</h2><img src="C:%5CUsers%5CGu%5CDesktop%5Cteam-learning-rs-master%5CDeepRecommendationModel%5Cimage-20210318183303549.png" alt="image-20210318183303549" style="zoom:80%;"><p>通过加权的方式将Wide部分和Deep部分合并在一起，最上面的输出层是一个sigmoid层，或者是一个线性层，就是一个简单的线性累加，文中成为<code>joint</code>，论文中还降讲到了联合（joint）和集成（ensemble）的区别，集成是每个模型单独训练，再将模型的结果融合，相比于联合训练，集成的每个独立的模型都得学的足够好才有利于随后的回合，因此模型的size也会更大。而在联合训练中，wide部分只需要做一小部分的特征叉乘来弥补deep部分的不足，并不需要一个完整的Wide Models。在集成学习中，每个部分的参数是互不影响的，而在联合学习中，它们的参数是一起训练的。模型选取logistic损失函数，最后的预测输出概率值。公式如下：<br>$$<br>P(Y=1|x)=\delta(w_{wide}^T[x,\phi(x)] + w_{deep}^T a^{(lf)} + b)<br>$$<br>其中$\sigma$表示sigmoid函数，$\phi(x)$表示叉乘特征，$\alpha^{(lf)}$表示神经网络最后一层激活值，$b$表示偏置。</p><p>论文中，作者通过梯度的反向传播，使用<code>mini-batch stochastic optimization</code>方法训练参数，并且对wide部分使用带L1正则的<code>FTRL(Follow-the-regularized-leader)</code>算法，对Deep Models部分使用<code>AdaGrad</code>算法。</p><h2 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h2><h3 id="在应用场景中，哪些特征适合放在Wide侧，哪些特征适合放在Deep侧，为什么？"><a href="#在应用场景中，哪些特征适合放在Wide侧，哪些特征适合放在Deep侧，为什么？" class="headerlink" title="在应用场景中，哪些特征适合放在Wide侧，哪些特征适合放在Deep侧，为什么？"></a>在应用场景中，哪些特征适合放在Wide侧，哪些特征适合放在Deep侧，为什么？</h3><p>显然的，直接的，有规律可循的特征适合放在Wide侧，对于一些受上下文影响较大的，简单的规律或许能够反映更大的上下文原因的特征适合放在Deep层。</p><h3 id="为什么Wide部分要用L1-FTRL训练"><a href="#为什么Wide部分要用L1-FTRL训练" class="headerlink" title="为什么Wide部分要用L1 FTRL训练?"></a>为什么Wide部分要用L1 FTRL训练?</h3><p>L1正则化比L2正则化更容易产生稀疏解，FTRL本身是一个稀疏性很好，精度也不错的随机梯度下降方法。L1 FTRL非常注重模型的稀疏性，会让Wide部分的大部分权重都为0，我们无需准备大量0权重特征，大大压缩了模型的权重，也压缩了特征向量的维度。</p><h3 id="为什么Deep部分不特别考虑稀疏性的问题？"><a href="#为什么Deep部分不特别考虑稀疏性的问题？" class="headerlink" title="为什么Deep部分不特别考虑稀疏性的问题？"></a>为什么Deep部分不特别考虑稀疏性的问题？</h3><p>在Deep部分输入类别是数值类特征，或者是已经降维并稠密化的Embedding向量，因此不需要考虑特征稀疏问题。</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>模型部分，特征的选择应该根据实际的业务场景选择哪些特征应该放在wide部分，哪些特征应该放在deep部分</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">WideNDeep</span>(<span class="params">linear_feature_columns, dnn_feature_columns</span>):</span></span><br><span class="line">    <span class="comment"># 构建输入层，即所有特征对应的Input()层，这里使用字典的形式返回，方便后续构建模型</span></span><br><span class="line">    dense_input_dict, sparse_input_dict = build_input_layers(linear_feature_columns + dnn_feature_columns)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将linear部分的特征中sparse特征筛选出来，后面用来做1维的embedding</span></span><br><span class="line">    linear_sparse_feature_columns = <span class="built_in">list</span>(<span class="built_in">filter</span>(<span class="keyword">lambda</span> x: <span class="built_in">isinstance</span>(x, SparseFeat), linear_feature_columns))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建模型的输入层，模型的输入层不能是字典的形式，应该将字典的形式转换成列表的形式</span></span><br><span class="line">    <span class="comment"># 注意：这里实际的输入与Input()层的对应，是通过模型输入时候的字典数据的key与对应name的Input层</span></span><br><span class="line">    input_layers = <span class="built_in">list</span>(dense_input_dict.values()) + <span class="built_in">list</span>(sparse_input_dict.values())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Wide&amp;Deep模型论文中Wide部分使用的特征比较简单，并且得到的特征非常的稀疏，所以使用了FTRL优化Wide部分（这里没有实现FTRL）</span></span><br><span class="line">    <span class="comment"># 但是是根据他们业务进行选择的，我们这里将所有可能用到的特征都输入到Wide部分，具体的细节可以根据需求进行修改</span></span><br><span class="line">    linear_logits = get_linear_logits(dense_input_dict, sparse_input_dict, linear_sparse_feature_columns)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 构建维度为k的embedding层，这里使用字典的形式返回，方便后面搭建模型</span></span><br><span class="line">    embedding_layers = build_embedding_layers(dnn_feature_columns, sparse_input_dict, is_linear=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    dnn_sparse_feature_columns = <span class="built_in">list</span>(<span class="built_in">filter</span>(<span class="keyword">lambda</span> x: <span class="built_in">isinstance</span>(x, SparseFeat), dnn_feature_columns))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 在Wide&amp;Deep模型中，deep部分的输入是将dense特征和embedding特征拼在一起输入到dnn中</span></span><br><span class="line">    dnn_logits = get_dnn_logits(dense_input_dict, sparse_input_dict, dnn_sparse_feature_columns, embedding_layers)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将linear,dnn的logits相加作为最终的logits</span></span><br><span class="line">    output_logits = Add()([linear_logits, dnn_logits])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 这里的激活函数使用sigmoid</span></span><br><span class="line">    output_layer = Activation(<span class="string">"sigmoid"</span>)(output_logits)</span><br><span class="line"></span><br><span class="line">    model = Model(input_layers, output_layer)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><p>模块导入</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">"ignore"</span>)</span><br><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span>  MinMaxScaler, LabelEncoder</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> SparseFeat, DenseFeat, VarLenSparseFeat</span><br></pre></td></tr></tbody></table></figure><p>数据预处理</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_process</span>(<span class="params">data_df, dense_features, sparse_features</span>):</span></span><br><span class="line">    data_df[dense_features] = data_df[dense_features].fillna(<span class="number">0.0</span>)<span class="comment">#填充缺失值</span></span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> dense_features:<span class="comment"># 数据平滑处理</span></span><br><span class="line">        data_df[f] = data_df[f].apply(<span class="keyword">lambda</span> x: np.log(x+<span class="number">1</span>) <span class="keyword">if</span> x &gt; -<span class="number">1</span> <span class="keyword">else</span> -<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">    data_df[sparse_features] = data_df[sparse_features].fillna(<span class="string">"-1"</span>)<span class="comment">#填充缺失值</span></span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> sparse_features:<span class="comment"># 类别编码</span></span><br><span class="line">        lbe = LabelEncoder()</span><br><span class="line">        data_df[f] = lbe.fit_transform(data_df[f])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> data_df[dense_features + sparse_features]</span><br></pre></td></tr></tbody></table></figure><p>构建输入层并以dense和sparse两类字典的形式返回</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_input_layers</span>(<span class="params">feature_columns</span>):</span></span><br><span class="line">    dense_input_dict, sparse_input_dict = {}, {}</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> fc <span class="keyword">in</span> feature_columns:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(fc, SparseFeat):</span><br><span class="line">            sparse_input_dict[fc.name] = Input(shape=(<span class="number">1</span>, ), name=fc.name)</span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">isinstance</span>(fc, DenseFeat):</span><br><span class="line">            dense_input_dict[fc.name] = Input(shape=(fc.dimension, ), name=fc.name)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> dense_input_dict, sparse_input_dict</span><br></pre></td></tr></tbody></table></figure><p>构建Embedding层并以字典形式返回</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_embedding_layers</span>(<span class="params">feature_columns, input_layers_dict, is_linear</span>):</span></span><br><span class="line">    embedding_layers_dict = <span class="built_in">dict</span>()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将特征中的sparse特征筛选出来</span></span><br><span class="line">    sparse_feature_columns = <span class="built_in">list</span>(<span class="built_in">filter</span>(<span class="keyword">lambda</span> x: <span class="built_in">isinstance</span>(x, SparseFeat), feature_columns)) <span class="keyword">if</span> feature_columns <span class="keyword">else</span> []</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 如果是用于线性部分的embedding层，其维度为1，否则维度就是自己定义的embedding维度</span></span><br><span class="line">    <span class="keyword">if</span> is_linear:</span><br><span class="line">        <span class="keyword">for</span> fc <span class="keyword">in</span> sparse_feature_columns:</span><br><span class="line">            embedding_layers_dict[fc.name] = Embedding(fc.vocabulary_size, <span class="number">1</span>, name=<span class="string">'1d_emb_'</span> + fc.name)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">for</span> fc <span class="keyword">in</span> sparse_feature_columns:</span><br><span class="line">            embedding_layers_dict[fc.name] = Embedding(fc.vocabulary_size, fc.embedding_dim, name=<span class="string">'kd_emb_'</span> + fc.name)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> embedding_layers_dict</span><br></pre></td></tr></tbody></table></figure><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">def get_linear_logits(dense_input_dict, sparse_input_dict, sparse_feature_columns):</span><br><span class="line">    # 将所有的dense特征的Input层，经过一个全连接层得到dense特征的logits</span><br><span class="line">    concat_dense_inputs = Concatenate(axis=1)(list(dense_input_dict.values()))</span><br><span class="line">    dense_logits_output = Dense(1)(concat_dense_inputs)</span><br><span class="line">    </span><br><span class="line">    # 获取linear部分sparse特征的embedding层，这里使用embedding的原因是：</span><br><span class="line">    # 对于linear部分直接将特征进行onehot然后通过一个全连接层，当维度特别大的时候，计算比较慢</span><br><span class="line">    # 使用embedding层的好处就是可以通过查表的方式获取到哪些非零的元素对应的权重，然后在将这些权重相加，效率比较高</span><br><span class="line">    linear_embedding_layers = build_embedding_layers(sparse_feature_columns, sparse_input_dict, is_linear=True)</span><br><span class="line">    </span><br><span class="line">    # 将一维的embedding拼接，注意这里需要使用一个Flatten层，使维度对应</span><br><span class="line">    sparse_1d_embed = []</span><br><span class="line">    for fc in sparse_feature_columns:</span><br><span class="line">        feat_input = sparse_input_dict[fc.name]</span><br><span class="line">        embed = Flatten()(linear_embedding_layers[fc.name](feat_input)) # B x 1</span><br><span class="line">        sparse_1d_embed.append(embed)</span><br><span class="line"></span><br><span class="line">    # embedding中查询得到的权重就是对应onehot向量中一个位置的权重，所以后面不用再接一个全连接了，本身一维的embedding就相当于全连接</span><br><span class="line">    # 只不过是这里的输入特征只有0和1，所以直接向非零元素对应的权重相加就等同于进行了全连接操作(非零元素部分乘的是1)</span><br><span class="line">    sparse_logits_output = Add()(sparse_1d_embed)</span><br><span class="line"></span><br><span class="line">    # 最终将dense特征和sparse特征对应的logits相加，得到最终linear的logits</span><br><span class="line">    linear_logits = Add()([dense_logits_output, sparse_logits_output])</span><br><span class="line">    return linear_logits</span><br></pre></td></tr></tbody></table></figure><p>将所有的稀疏特征的embedding拼接</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">concat_embedding_list</span>(<span class="params">feature_columns, input_layer_dict, embedding_layer_dict, flatten=<span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="comment"># 将sparse特征筛选出来</span></span><br><span class="line">    sparse_feature_columns = <span class="built_in">list</span>(<span class="built_in">filter</span>(<span class="keyword">lambda</span> x: <span class="built_in">isinstance</span>(x, SparseFeat), feature_columns))</span><br><span class="line"></span><br><span class="line">    embedding_list = []</span><br><span class="line">    <span class="keyword">for</span> fc <span class="keyword">in</span> sparse_feature_columns:</span><br><span class="line">        _input = input_layer_dict[fc.name] <span class="comment"># 获取输入层 </span></span><br><span class="line">        _embed = embedding_layer_dict[fc.name] <span class="comment"># B x 1 x dim  获取对应的embedding层</span></span><br><span class="line">        embed = _embed(_input) <span class="comment"># B x dim  将input层输入到embedding层中</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 是否需要flatten, 如果embedding列表最终是直接输入到Dense层中，需要进行Flatten，否则不需要</span></span><br><span class="line">        <span class="keyword">if</span> flatten:</span><br><span class="line">            embed = Flatten()(embed)</span><br><span class="line">        </span><br><span class="line">        embedding_list.append(embed)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> embedding_list </span><br></pre></td></tr></tbody></table></figure><p>构建深度神经网络</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_dnn_logits</span>(<span class="params">dense_input_dict, sparse_input_dict, sparse_feature_columns, dnn_embedding_layers</span>):</span></span><br><span class="line">    concat_dense_inputs = Concatenate(axis=<span class="number">1</span>)(<span class="built_in">list</span>(dense_input_dict.values())) <span class="comment"># B x n1 (n1表示的是dense特征的维度) </span></span><br><span class="line"></span><br><span class="line">    sparse_kd_embed = concat_embedding_list(sparse_feature_columns, sparse_input_dict, dnn_embedding_layers, flatten=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    concat_sparse_kd_embed = Concatenate(axis=<span class="number">1</span>)(sparse_kd_embed) <span class="comment"># B x n2  (n2表示的是Sparse特征的维度)</span></span><br><span class="line"></span><br><span class="line">    dnn_input = Concatenate(axis=<span class="number">1</span>)([concat_dense_inputs, concat_sparse_kd_embed]) <span class="comment"># B x (n2 + n1)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># dnn层，这里的Dropout参数，Dense中的参数及Dense的层数都可以自己设定</span></span><br><span class="line">    dnn_out = Dropout(<span class="number">0.5</span>)(Dense(<span class="number">1024</span>, activation=<span class="string">'relu'</span>)(dnn_input))  </span><br><span class="line">    dnn_out = Dropout(<span class="number">0.3</span>)(Dense(<span class="number">512</span>, activation=<span class="string">'relu'</span>)(dnn_out))</span><br><span class="line">    dnn_out = Dropout(<span class="number">0.1</span>)(Dense(<span class="number">256</span>, activation=<span class="string">'relu'</span>)(dnn_out))</span><br><span class="line"></span><br><span class="line">    dnn_logits = Dense(<span class="number">1</span>)(dnn_out)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dnn_logits</span><br></pre></td></tr></tbody></table></figure><p>main函数入口</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment"># 读取数据</span></span><br><span class="line">    data = pd.read_csv(<span class="string">'./data/criteo_sample.txt'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 划分dense和sparse特征</span></span><br><span class="line">    columns = data.columns.values</span><br><span class="line">    dense_features = [feat <span class="keyword">for</span> feat <span class="keyword">in</span> columns <span class="keyword">if</span> <span class="string">'I'</span> <span class="keyword">in</span> feat]</span><br><span class="line">    sparse_features = [feat <span class="keyword">for</span> feat <span class="keyword">in</span> columns <span class="keyword">if</span> <span class="string">'C'</span> <span class="keyword">in</span> feat]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 简单的数据预处理</span></span><br><span class="line">    train_data = data_process(data, dense_features, sparse_features)</span><br><span class="line">    train_data[<span class="string">'label'</span>] = data[<span class="string">'label'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将特征分组，分成linear部分和dnn部分(根据实际场景进行选择)，并将分组之后的特征做标记（使用DenseFeat, SparseFeat）</span></span><br><span class="line">    linear_feature_columns = [SparseFeat(feat, vocabulary_size=data[feat].nunique(),embedding_dim=<span class="number">4</span>)</span><br><span class="line">                            <span class="keyword">for</span> i,feat <span class="keyword">in</span> <span class="built_in">enumerate</span>(sparse_features)] + [DenseFeat(feat, <span class="number">1</span>,)</span><br><span class="line">                            <span class="keyword">for</span> feat <span class="keyword">in</span> dense_features]</span><br><span class="line"></span><br><span class="line">    dnn_feature_columns = [SparseFeat(feat, vocabulary_size=data[feat].nunique(),embedding_dim=<span class="number">4</span>)</span><br><span class="line">                            <span class="keyword">for</span> i,feat <span class="keyword">in</span> <span class="built_in">enumerate</span>(sparse_features)] + [DenseFeat(feat, <span class="number">1</span>,)</span><br><span class="line">                            <span class="keyword">for</span> feat <span class="keyword">in</span> dense_features]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建WideNDeep模型</span></span><br><span class="line">    history = WideNDeep(linear_feature_columns, dnn_feature_columns)</span><br><span class="line">    history.summary()</span><br><span class="line">    history.<span class="built_in">compile</span>(optimizer=<span class="string">"adam"</span>, </span><br><span class="line">                loss=<span class="string">"binary_crossentropy"</span>, </span><br><span class="line">                metrics=[<span class="string">"binary_crossentropy"</span>, tf.keras.metrics.AUC(name=<span class="string">'auc'</span>)])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将输入数据转化成字典的形式输入</span></span><br><span class="line">    train_model_input = {name: data[name] <span class="keyword">for</span> name <span class="keyword">in</span> dense_features + sparse_features}</span><br><span class="line">    <span class="comment"># 模型训练</span></span><br><span class="line">    history.fit(train_model_input, train_data[<span class="string">'label'</span>].values,</span><br><span class="line">            batch_size=<span class="number">64</span>, epochs=<span class="number">5</span>, validation_split=<span class="number">0.2</span>, )    </span><br></pre></td></tr></tbody></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://blog.csdn.net/anshuai_aw1/article/details/105113980">推荐系统（一）：Wide &amp; Deep 源论文整理和思考</a></li><li><a href="https://zhuanlan.zhihu.com/p/334991736">巨经典论文！详解推荐系统经典模型 Wide &amp; Deep</a></li><li><a href="https://zhuanlan.zhihu.com/p/53361519">详解 Wide &amp; Deep 结构背后的动机</a></li><li><a href="https://arxiv.org/abs/1606.07792">Wide &amp; Deep Learning for Recommender Systems</a></li><li><a href="https://zhuanlan.zhihu.com/p/142958834">见微知著，你真的搞懂 Google 的 Wide&amp;Deep 模型了吗？</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 深度推荐系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 推荐系统 </tag>
            
            <tag> DNN </tag>
            
            <tag> 特征工程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DeepCrossing理论与实践</title>
      <link href="2021/03/21/deepcrossing-li-lun-yu-shi-jian/"/>
      <url>2021/03/21/deepcrossing-li-lun-yu-shi-jian/</url>
      
        <content type="html"><![CDATA[<hr><h1 id="论文背景"><a href="#论文背景" class="headerlink" title="论文背景"></a>论文背景</h1><p>DeepCrossing是2016年由微软提出，旨在特征工程中减少人力特征组合的时间开销，通过模型自动学习特征的组合方式，解决特征组合的难题。是第一个企业以正式论文的形式分享推荐系统技术细节的模型。</p><h1 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h1><img src="https://raw.githubusercontent.com/UPTOLIMIT/PicBed/master//img/image-20210315215320120.png" alt="image-20210315215320120" style="zoom: 67%;"><p>模型的输入是一系列独立的特征，输出是用户的点击率预测值。模型一共分为4层，分别是Embedding层，Stacking层，Multiple Residual Units层，Scoring层。</p><h2 id="Embedding层"><a href="#Embedding层" class="headerlink" title="Embedding层"></a>Embedding层</h2><p>Embedding这个概念最初来源于Manifold Hypothesis（<a href="https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/">流形假设</a>）。流形假设是指<a href="https://www.zhihu.com/question/38002635">“自然的原始数据是低维的流形嵌入于(embedded in)原始数据所在的高维空间”</a>，而深度学习的任务就是把高维原始数据映射到低维流形，使得原始数据变得可分，而这个映射就叫做embedding（嵌入），比如word embedding（词嵌入）就是把单词组成的句子映射到一个表征向量。久而久之，把低维流形的表征向量叫做Embedding成为一种习惯（其实是一种误用）。在本文中，Embedding 层的作用就是将从原始独立特征中提取出来的高维稀疏特征向量转换成低维稠密的Embedding向量。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_embedding_layers</span>(<span class="params">feature_columns, input_layers_dict, is_linear</span>):</span></span><br><span class="line">    <span class="comment"># 定义一个embedding层对应的字典</span></span><br><span class="line">    embedding_layers_dict = <span class="built_in">dict</span>()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将特征中的sparse特征筛选出来</span></span><br><span class="line">    sparse_feature_columns = <span class="built_in">list</span>(<span class="built_in">filter</span>(<span class="keyword">lambda</span> x: <span class="built_in">isinstance</span>(x, SparseFeat), feature_columns)) <span class="keyword">if</span> feature_columns <span class="keyword">else</span> []</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 如果是用于线性部分的embedding层，其维度为1，否则维度就是自己定义的embedding维度</span></span><br><span class="line">    <span class="keyword">if</span> is_linear:</span><br><span class="line">        <span class="keyword">for</span> fc <span class="keyword">in</span> sparse_feature_columns:</span><br><span class="line">            embedding_layers_dict[fc.name] = Embedding(fc.vocabulary_size + <span class="number">1</span>, <span class="number">1</span>, name=<span class="string">'1d_emb_'</span> + fc.name)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">for</span> fc <span class="keyword">in</span> sparse_feature_columns:</span><br><span class="line">            embedding_layers_dict[fc.name] = Embedding(fc.vocabulary_size + <span class="number">1</span>, fc.embedding_dim, name=<span class="string">'kd_emb_'</span> + fc.name)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> embedding_layers_dict</span><br></pre></td></tr></tbody></table></figure><h2 id="Stacking层"><a href="#Stacking层" class="headerlink" title="Stacking层"></a>Stacking层</h2><p>得到了所有特征的Embedding表示之后，Stacking层接收Embedding层的输入并且把这些特征聚合起来，形成一个向量</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将所有的sparse特征embedding拼接</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">concat_embedding_list</span>(<span class="params">feature_columns, input_layer_dict, embedding_layer_dict, flatten=<span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="comment"># 将sparse特征筛选出来</span></span><br><span class="line">    sparse_feature_columns = <span class="built_in">list</span>(<span class="built_in">filter</span>(<span class="keyword">lambda</span> x: <span class="built_in">isinstance</span>(x, SparseFeat), feature_columns))</span><br><span class="line"></span><br><span class="line">    embedding_list = []</span><br><span class="line">    <span class="keyword">for</span> fc <span class="keyword">in</span> sparse_feature_columns:</span><br><span class="line">        _input = input_layer_dict[fc.name] <span class="comment"># 获取输入层 </span></span><br><span class="line">        _embed = embedding_layer_dict[fc.name] <span class="comment"># B x 1 x dim  获取对应的embedding层</span></span><br><span class="line">        embed = _embed(_input) <span class="comment"># B x dim  将input层输入到embedding层中</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 是否需要flatten, 如果embedding列表最终是直接输入到Dense层中，需要进行Flatten，否则不需要</span></span><br><span class="line">        <span class="keyword">if</span> flatten:</span><br><span class="line">            embed = Flatten()(embed)</span><br><span class="line">        </span><br><span class="line">        embedding_list.append(embed)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> embedding_list </span><br></pre></td></tr></tbody></table></figure><h2 id="Multiple-Residual-Units层"><a href="#Multiple-Residual-Units层" class="headerlink" title="Multiple Residual Units层"></a>Multiple Residual Units层</h2><p><img src="F:%5CMyBlog%5Csource%5C_posts%5Cimage-20210316193133492.png" alt="image-20210316193133492"></p><p>DeepCrossing中的Crossing就是通过多个残差单元来实现的，残差单元的结构如上图所示。与ResNet不同的是，它不包含卷积操作，也是第一次在图像识别之外应用残差单元。<br>$$<br>X^O=F(X^I,{W_0,W_1},{b_0,b_1})+X^I<br>$$<br>将$X^I$移动到左侧,可以看出$F$函数拟合的是输入和输出之间的残差。对输入进行全连接变换之后，经过<code>relu</code>激活函数送入第二个全连接层，将输出结果与原始输入进行<code>element-wise add</code>操作，再经过<code>relu</code>激活输出，实验表明，残差结构能更敏感的捕获输入输出之间的信息差。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># DNN残差块的定义</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResidualBlock</span>(<span class="params">Layer</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, units</span>):</span> <span class="comment"># units表示的是DNN隐藏层神经元数量</span></span><br><span class="line">        <span class="built_in">super</span>(ResidualBlock, self).__init__()</span><br><span class="line">        self.units = units</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span>(<span class="params">self, input_shape</span>):</span></span><br><span class="line">        out_dim = input_shape[-<span class="number">1</span>]</span><br><span class="line">        self.dnn1 = Dense(self.units, activation=<span class="string">'relu'</span>)</span><br><span class="line">        self.dnn2 = Dense(out_dim, activation=<span class="string">'relu'</span>) <span class="comment"># 保证输入的维度和输出的维度一致才能进行残差连接</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        x = inputs</span><br><span class="line">        x = self.dnn1(x)</span><br><span class="line">        x = self.dnn2(x)</span><br><span class="line">        x = Activation(<span class="string">'relu'</span>)(x + inputs) <span class="comment"># 残差操作</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><h2 id="Scoring层"><a href="#Scoring层" class="headerlink" title="Scoring层"></a>Scoring层</h2><p>Scoring层作为输出层，输出广告预测点击率，是为了拟合优化目标而存在，对于二分类问题，往往使用逻辑回归，模型通过叠加多个残差块加深网络的深度，最后将结果转换为一个概率值输出。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># block_nums表示DNN残差块的数量，通过叠加残差块的数量可以加深网络深度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_dnn_logits</span>(<span class="params">dnn_inputs, block_nums=<span class="number">3</span></span>):</span></span><br><span class="line">    dnn_out = dnn_inputs</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(block_nums):</span><br><span class="line">        dnn_out = ResidualBlock(<span class="number">64</span>)(dnn_out)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 这里使用sigmoid激活函数，将dnn的输出转化成logits</span></span><br><span class="line">        dnn_logits = Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>)(dnn_out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dnn_logits</span><br></pre></td></tr></tbody></table></figure><h2 id="代码解读"><a href="#代码解读" class="headerlink" title="代码解读"></a>代码解读</h2><p>从<code>main</code>函数开始，按照代码执行顺序，阅读代码。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line">data = pd.read_csv(<span class="string">'./data/criteo_sample.txt'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分dense和sparse特征，即密集特征和稀疏特征，观察原数据集可以发现是按列分好的</span></span><br><span class="line">columns = data.columns.values</span><br><span class="line">dense_features = [feat <span class="keyword">for</span> feat <span class="keyword">in</span> columns <span class="keyword">if</span> <span class="string">'I'</span> <span class="keyword">in</span> feat]</span><br><span class="line">sparse_features = [feat <span class="keyword">for</span> feat <span class="keyword">in</span> columns <span class="keyword">if</span> <span class="string">'C'</span> <span class="keyword">in</span> feat]</span><br></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 简单的数据预处理</span></span><br><span class="line">   train_data = data_process(data, dense_features, sparse_features)</span><br><span class="line">   train_data[<span class="string">'label'</span>] = data[<span class="string">'label'</span>]</span><br></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据预处理函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_process</span>(<span class="params">data_df, dense_features, sparse_features</span>):</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    简单处理特征，包括填充缺失值，数值处理，类别编码</span></span><br><span class="line"><span class="string">    param data_df: DataFrame格式的数据</span></span><br><span class="line"><span class="string">    param dense_features: 数值特征名称列表</span></span><br><span class="line"><span class="string">    param sparse_features: 类别特征名称列表</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    data_df[dense_features] = data_df[dense_features].fillna(<span class="number">0.0</span>)</span><br><span class="line">    <span class="comment"># 使用log函数对数据进行平滑处理</span></span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> dense_features:</span><br><span class="line">        data_df[f] = data_df[f].apply(<span class="keyword">lambda</span> x: np.log(x+<span class="number">1</span>) <span class="keyword">if</span> x &gt; -<span class="number">1</span> <span class="keyword">else</span> -<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 将缺失值填充为-1  </span></span><br><span class="line">    data_df[sparse_features] = data_df[sparse_features].fillna(<span class="string">"-1"</span>)</span><br><span class="line">    <span class="comment"># 将类别型数据one-hot编码</span></span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> sparse_features:</span><br><span class="line">        lbe = LabelEncoder()</span><br><span class="line">        data_df[f] = lbe.fit_transform(data_df[f])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> data_df[dense_features + sparse_features]</span><br></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将特征做标记</span></span><br><span class="line"></span><br><span class="line">dnn_feature_columns = [SparseFeat(feat, vocabulary_size=data[feat].nunique(),embedding_dim=<span class="number">4</span>)</span><br><span class="line">                            <span class="keyword">for</span> feat <span class="keyword">in</span> sparse_features] + [DenseFeat(feat, <span class="number">1</span>,)</span><br><span class="line">                            <span class="keyword">for</span> feat <span class="keyword">in</span> dense_features]</span><br></pre></td></tr></tbody></table></figure><p>这里用到了python的一个工厂函数<code>namedtuple()</code>可以译为<code>具名元组</code>，可以理解为元组的增强版本，为元组中的每一个元素赋予了含义，从而增强代码可读性。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SparseFeat = namedtuple(<span class="string">'SparseFeat'</span>, [<span class="string">'name'</span>, <span class="string">'vocabulary_size'</span>, <span class="string">'embedding_dim'</span>])</span><br></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建DeepCrossing模型</span></span><br><span class="line">    history = DeepCrossing(dnn_feature_columns)</span><br></pre></td></tr></tbody></table></figure><p>DeepCrossing模型代码如下：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">DeepCrossing</span>(<span class="params">dnn_feature_columns</span>):</span></span><br><span class="line">    <span class="comment"># 构建输入层，即所有特征对应的Input()层，这里使用字典的形式返回，方便后续构建模型</span></span><br><span class="line">    dense_input_dict, sparse_input_dict = build_input_layers(dnn_feature_columns)</span><br><span class="line">    <span class="comment"># 构建模型的输入层，模型的输入层不能是字典的形式，应该将字典的形式转换成列表的形式</span></span><br><span class="line">    <span class="comment"># 注意：这里实际的输入与Input()层的对应，是通过模型输入时候的字典数据的key与对应name的Input层</span></span><br><span class="line">    input_layers = <span class="built_in">list</span>(dense_input_dict.values()) + <span class="built_in">list</span>(sparse_input_dict.values())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 构建维度为k的embedding层，这里使用字典的形式返回，方便后面搭建模型</span></span><br><span class="line">    embedding_layer_dict = build_embedding_layers(dnn_feature_columns, sparse_input_dict, is_linear=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#将所有的dense特征拼接到一起，B为Batchsize</span></span><br><span class="line">    dense_dnn_list = <span class="built_in">list</span>(dense_input_dict.values())</span><br><span class="line">    dense_dnn_inputs = Concatenate(axis=<span class="number">1</span>)(dense_dnn_list) <span class="comment"># B x n (n表示数值特征的数量)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 因为需要将其与dense特征拼接到一起所以需要Flatten，不进行Flatten的Embedding层输出的维度为：Bx1xdim</span></span><br><span class="line">    sparse_dnn_list = concat_embedding_list(dnn_feature_columns, sparse_input_dict, embedding_layer_dict, flatten=<span class="literal">True</span>) </span><br><span class="line"></span><br><span class="line">    sparse_dnn_inputs = Concatenate(axis=<span class="number">1</span>)(sparse_dnn_list) <span class="comment"># B x m*dim (n表示类别特征的数量，dim表示embedding的维度)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将dense特征和Sparse特征拼接到一起</span></span><br><span class="line">    dnn_inputs = Concatenate(axis=<span class="number">1</span>)([dense_dnn_inputs, sparse_dnn_inputs]) <span class="comment"># B x (n + m*dim)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输入到dnn中，需要提前定义需要几个残差块</span></span><br><span class="line">    output_layer = get_dnn_logits(dnn_inputs, block_nums=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    model = Model(input_layers, output_layer)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>DeepCrossing的结构比较简单清晰，没有引入特殊的模型结构，只是常规的Embedding+多层神经网络。但是在DeepCrossing中没有任何人工特征工程的参与，只需要简单的特征处理，原始特征经过Embedding层输入神经网络层，自主交叉和学习，通过调整神经网络的深度进行特征之间的深度交叉，也是DeepCrossing名称的由来（大悟）。</p><p>最后附上论文<a href="https://link.zhihu.com/?target=http://www.kdd.org/kdd2016/papers/files/adf0975-shanA.pdf">链接</a></p>]]></content>
      
      
      <categories>
          
          <category> 深度推荐系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 推荐系统 </tag>
            
            <tag> DNN </tag>
            
            <tag> 特征工程 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
